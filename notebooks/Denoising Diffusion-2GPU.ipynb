{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3371fb0b-26dd-4068-948e-7b95ee1e5e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from typing import List\n",
    "import random, math, os, sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from timm.utils import ModelEmaV3\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42abc86d-f84d-4cf1-ac6b-342ac869c226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['OMP_NUM_THREADS'] = '16'\n",
    "os.environ['MKL_NUM_THREADS'] = '16'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_MULTI_GPU = NUM_GPUS > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed917e9-9dff-41f6-ab61-04d899c8d1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 * NUM_GPUS if NUM_GPUS > 0 else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "631bab9a-c38a-4311-8740-103659f8042e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_WORKERS = min(16, os.cpu_count() or 8)\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 2\n",
    "PERSISTENT_WORKERS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6059ed7-b781-4d5c-8e33-29a717bb837f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: 2\n",
      "  GPU 0: NVIDIA B200\n",
      "  GPU 1: NVIDIA B200\n",
      "Total Batch Size: 16 (per GPU: 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPUs: {NUM_GPUS}\")\n",
    "if NUM_GPUS > 0:\n",
    "    for i in range(NUM_GPUS):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(f\"Total Batch Size: {BATCH_SIZE} (per GPU: {BATCH_SIZE // max(NUM_GPUS, 1)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d2f168-8a4c-42ad-ac68-ccfe543d7c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf0e8b8-61ea-40b9-aa87-227908d658b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from text_encoders import clip_model, bert_model\n",
    "from dataloaders import caption_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10500af3-156d-44eb-b6fa-336846a21d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, time_steps:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        position = torch.arange(time_steps).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "        embeddings = torch.zeros(time_steps, embed_dim, requires_grad=False)\n",
    "        embeddings[:, 0::2] = torch.sin(position * div)\n",
    "        embeddings[:, 1::2] = torch.cos(position * div)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        embeds = self.embeddings.to(t.device)[t]\n",
    "        return embeds[:, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7818de6-c0c4-4bcc-8ea5-a1e48ecfbaff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C:int, num_groups:int, dropout_prob:float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.gnorm1 = nn.GroupNorm(num_groups=num_groups, num_channels=C)\n",
    "        self.gnorm2 = nn.GroupNorm(num_groups=num_groups, num_channels=C)\n",
    "        self.conv1 = nn.Conv2d(C, C, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, 3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=True)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        x = x + embeddings[:, :x.shape[1], :, :]\n",
    "        r = self.conv1(self.relu(self.gnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.gnorm2(r)))\n",
    "        return r + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdca158-d807-4f69-88c7-cfc0e5a1304e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, C:int, num_heads:int, dropout_prob:float):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(C, C*3)\n",
    "        self.proj2 = nn.Linear(C, C)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.proj1(x)\n",
    "        x = rearrange(x, 'b L (C H K) -> K b H L C', K=3, H=self.num_heads)\n",
    "        q, k, v = x[0], x[1], x[2]\n",
    "        x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout_prob)\n",
    "        x = rearrange(x, 'b H (h w) C -> b h w (C H)', h=h, w=w)\n",
    "        x = self.proj2(x)\n",
    "        return rearrange(x, 'b h w C -> b C h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6f2508-401d-4ee9-a789-4e1ac53ac778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnetLayer(nn.Module):\n",
    "    def __init__(self, upscale, attention, num_groups, dropout_prob, num_heads, C):\n",
    "        super().__init__()\n",
    "        self.ResBlock1 = ResBlock(C, num_groups, dropout_prob)\n",
    "        self.ResBlock2 = ResBlock(C, num_groups, dropout_prob)\n",
    "        if upscale:\n",
    "            self.conv = nn.ConvTranspose2d(C, C//2, 4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(C, C*2, 3, stride=2, padding=1)\n",
    "        if attention:\n",
    "            self.attention_layer = Attention(C, num_heads, dropout_prob)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        x = self.ResBlock1(x, embeddings)\n",
    "        if hasattr(self, 'attention_layer'):\n",
    "            x = self.attention_layer(x)\n",
    "        x = self.ResBlock2(x, embeddings)\n",
    "        return self.conv(x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18e372a3-a2ef-4751-a607-1aec7b3969d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,\n",
    "                 Channels=[64,128,256,512,512,384],\n",
    "                 Attentions=[False,True,False,False,False,True],\n",
    "                 Upscales=[False,False,False,True,True,True],\n",
    "                 num_groups=32,\n",
    "                 dropout_prob=0.1,\n",
    "                 num_heads=8,\n",
    "                 input_channels=1,\n",
    "                 output_channels=1,\n",
    "                 time_steps=1000,\n",
    "                 text_embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(Channels)\n",
    "        self.shallow_conv = nn.Conv2d(input_channels, Channels[0], 3, padding=1)\n",
    "        self.late_conv = nn.Conv2d((Channels[-1]//2)+Channels[0], (Channels[-1]//2), 3, padding=1)\n",
    "        self.output_conv = nn.Conv2d((Channels[-1]//2), output_channels, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.embeddings = SinusoidalEmbeddings(time_steps, max(Channels))\n",
    "        self.text_proj = nn.Linear(text_embed_dim, max(Channels))\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            layer = UnetLayer(\n",
    "                upscale=Upscales[i],\n",
    "                attention=Attentions[i],\n",
    "                num_groups=num_groups,\n",
    "                dropout_prob=dropout_prob,\n",
    "                C=Channels[i],\n",
    "                num_heads=num_heads\n",
    "            )\n",
    "            setattr(self, f'Layer{i+1}', layer)\n",
    "\n",
    "    def forward(self, x, t, text_emb):\n",
    "        x = self.shallow_conv(x)\n",
    "        residuals = []\n",
    "\n",
    "        text_emb = self.text_proj(text_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        time_emb = self.embeddings(x, t)\n",
    "        combined_emb = time_emb + text_emb\n",
    "\n",
    "        for i in range(self.num_layers // 2):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            x, r = layer(x, combined_emb)\n",
    "            residuals.append(r)\n",
    "\n",
    "        for i in range(self.num_layers // 2, self.num_layers):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            x = torch.concat((layer(x, combined_emb)[0], residuals[self.num_layers - i - 1]), dim=1)\n",
    "\n",
    "        return self.output_conv(self.relu(self.late_conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa7a68d-873e-4e63-bd48-bb1a5e250e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DDPM_Scheduler(nn.Module):\n",
    "    def __init__(self, num_time_steps=1000):\n",
    "        super().__init__()\n",
    "        self.beta = torch.linspace(1e-4, 0.02, num_time_steps, requires_grad=False)\n",
    "        alpha = 1 - self.beta\n",
    "        self.alpha = torch.cumprod(alpha, dim=0).requires_grad_(False)\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        return self.beta.to(device)[t], self.alpha.to(device)[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "074f0455-9b0c-43ce-bc4a-d67982de08f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b32877-9cff-47d6-8b8e-d120898bec45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/blue/eee6778/dthiyagarajan/Echoes-of-Imagination'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = os.path.dirname(os.getcwd())\n",
    "root_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "913d31fe-ff88-4801-9cc8-6e8861b50546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader = caption_dataset()\n",
    "train_dataloader,_ = dataloader.get_dataloader(partition=\"train\", batch_size=32, num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "704cd171-9b6f-4a48-954a-0f2caaf2cc09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 64, 64])\n",
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "imgs, text_emb = next(iter(train_dataloader))\n",
    "print(imgs.shape)       # [B, 3, H, W]\n",
    "print(text_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d48013be-6a4a-444e-a017-db9095043e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = DDPM_Scheduler(num_time_steps=1000)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNET(input_channels=3, output_channels=3, text_embed_dim=train_dataloader.dataset.embed_dim)\n",
    "model = nn.DataParallel(model) \n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "732d4f2c-ea08-422b-813b-f92708f90e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())     # should be 2\n",
    "print(model.device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdba39-d9f9-4f16-827d-7f9cbeb3c7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   5%|‚ñç         | 678/14952 [06:14<1:19:39,  2.99it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, text_emb in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        imgs, text_emb = imgs.to(device), text_emb.to(device)\n",
    "        b = imgs.size(0)\n",
    "\n",
    "        # sample timesteps\n",
    "        t = torch.randint(0, 1000, (b,), device=device)\n",
    "        e = torch.randn_like(imgs)\n",
    "\n",
    "        beta_t, alpha_t = scheduler(t)\n",
    "        a = alpha_t.view(b, 1, 1, 1)\n",
    "        noisy_imgs = (torch.sqrt(a) * imgs) + (torch.sqrt(1 - a) * e)\n",
    "\n",
    "        pred_noise = model(noisy_imgs, t, text_emb)\n",
    "        loss = criterion(pred_noise, e)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        del loss, pred_noise, noisy_imgs, e, t, beta_t, alpha_t, a\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d24928-f8a5-4c21-b00b-fe3fc59948a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8bfc2-b766-4d14-821b-ae9fb638b488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eoi_project",
   "language": "python",
   "name": "eoi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
