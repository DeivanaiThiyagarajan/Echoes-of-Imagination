{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e0e7fd",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n",
    "\n",
    "### What This Notebook Does:\n",
    "✓ Loads lightweight Hugging Face pretrained models (Stable Diffusion)\n",
    "✓ Integrates with your existing COCO + Flickr dataloaders\n",
    "✓ Generates high-quality images from text prompts\n",
    "✓ Works on CPU for development, optimized for GPU deployment\n",
    "✓ Provides utilities for batch processing and visualization\n",
    "\n",
    "### Key Features:\n",
    "- **Lightweight**: Uses Stable Diffusion v1.5 (~5GB)\n",
    "- **Flexible**: Adjustable inference steps, guidance scale, and other parameters\n",
    "- **Scalable**: Configuration ready for multi-GPU on HiperGator\n",
    "- **Data Integration**: Direct usage of your existing caption datasets\n",
    "\n",
    "### Next Steps for Production:\n",
    "1. **Fine-tune on your data**: Train the model on your specific dataset\n",
    "2. **Optimize hyperparameters**: Adjust guidance_scale and num_inference_steps\n",
    "3. **Evaluate quality**: Use CLIP scores or FID to measure generation quality\n",
    "4. **Deploy to HiperGator**: Use the multi-GPU configuration provided\n",
    "5. **Add LoRA**: For efficient parameter-efficient fine-tuning\n",
    "6. **Monitor performance**: Track GPU memory and inference time\n",
    "\n",
    "### Recommended Reading:\n",
    "- Hugging Face Diffusers Documentation: https://huggingface.co/docs/diffusers\n",
    "- Stable Diffusion Paper: https://arxiv.org/abs/2112.10752\n",
    "- CLIP Guidance for Image Generation: https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e580a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom prompts\n",
    "custom_prompts = [\n",
    "    \"A serene landscape with mountains and lake during sunset\",\n",
    "    \"A modern kitchen with stainless steel appliances\",\n",
    "    \"A cat sitting on a windowsill, looking outside\",\n",
    "    \"A busy street market with colorful stalls and people\"\n",
    "]\n",
    "\n",
    "print(\"Generating images from custom prompts...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "custom_results = generate_batch_images(\n",
    "    custom_prompts,\n",
    "    num_per_prompt=1,\n",
    "    num_inference_steps=inference_steps,\n",
    "    guidance_scale=7.5,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Display custom generated images\n",
    "display_images(custom_results)\n",
    "\n",
    "# Experiment with different parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experimenting with different guidance scales...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompt = \"A futuristic city with flying cars\"\n",
    "guidance_scales = [5.0, 7.5, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(guidance_scales), figsize=(5*len(guidance_scales), 5))\n",
    "fig.suptitle(f\"Effect of Guidance Scale: '{test_prompt}'\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, guidance_scale in zip(axes, guidance_scales):\n",
    "    image = generate_image(\n",
    "        test_prompt,\n",
    "        num_inference_steps=20,\n",
    "        guidance_scale=guidance_scale,\n",
    "        seed=42\n",
    "    )\n",
    "    if image is not None:\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"Guidance: {guidance_scale}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4145d",
   "metadata": {},
   "source": [
    "## 10. Advanced: Custom Prompts and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-GPU Deployment Guide for HiperGator\n",
    "==========================================\n",
    "\n",
    "To scale this code to multiple GPUs on HiperGator, follow these steps:\n",
    "\n",
    "1. DISTRIBUTED DATA PARALLEL (DDP) SETUP:\n",
    "   - Use torch.nn.parallel.DistributedDataParallel for multi-GPU inference\n",
    "   - Wrap model: model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "2. SBATCH CONFIGURATION (Example):\n",
    "   #!/bin/bash\n",
    "   #SBATCH --nodes=1\n",
    "   #SBATCH --ntasks-per-node=2\n",
    "   #SBATCH --gpus-per-node=a100:2  # Request 2 A100 GPUs\n",
    "   #SBATCH --cpus-per-task=16\n",
    "   #SBATCH --mem=64GB\n",
    "   #SBATCH --time=04:00:00\n",
    "   \n",
    "   module load python/3.11 cuda/11.8\n",
    "   source venv/bin/activate\n",
    "   \n",
    "   torchrun --nproc_per_node=2 train_distributed.py\n",
    "\n",
    "3. MEMORY OPTIMIZATION:\n",
    "   - Use mixed precision (float16) for 2x memory savings\n",
    "   - Enable gradient checkpointing if fine-tuning\n",
    "   - Use --enable-attention-slicing for single GPU, disable for multi-GPU\n",
    "\n",
    "4. PERFORMANCE TIPS:\n",
    "   - Increase batch size (BATCH_SIZE = 8-16 per GPU on A100)\n",
    "   - Use more inference steps (50+) for better quality\n",
    "   - Reduce num_workers if data loading is bottleneck\n",
    "   - Pin memory only if using GPU\n",
    "\n",
    "5. MONITORING:\n",
    "   - Use nvidia-smi to monitor GPU memory\n",
    "   - Check data loading speed with: \n",
    "     time.time() profiling around dataloader loops\n",
    "\n",
    "6. FINE-TUNING (Optional):\n",
    "   - For custom text-to-image generation, fine-tune the model\n",
    "   - Use LoRA for efficient parameter-efficient fine-tuning\n",
    "   - Requires training loop with optimizer and loss calculation\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Example configuration dictionary for multi-GPU setup\n",
    "MULTI_GPU_CONFIG = {\n",
    "    'num_gpus': NUM_GPUS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'use_ddp': NUM_GPUS > 1,\n",
    "    'mixed_precision': 'fp16' if NUM_GPUS > 1 else 'fp32',\n",
    "    'device': DEVICE,\n",
    "    'local_rank': int(os.environ.get('LOCAL_RANK', 0))\n",
    "}\n",
    "\n",
    "print(\"\\nCurrent Configuration:\")\n",
    "for key, value in MULTI_GPU_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330413c",
   "metadata": {},
   "source": [
    "## 9. Multi-GPU Deployment Configuration and Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple batches from the dataset\n",
    "def process_dataset_batch(dataloader, num_batches=2, num_images_per_caption=1):\n",
    "    \"\"\"\n",
    "    Generate images for multiple captions from the dataloader.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader\n",
    "        num_batches: Number of batches to process\n",
    "        num_images_per_caption: Number of images per caption\n",
    "    \"\"\"\n",
    "    all_generated_images = {}\n",
    "    \n",
    "    for batch_idx, (images, captions) in enumerate(dataloader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch_idx + 1}/{num_batches}\")\n",
    "        print(f\"Captions in batch: {len(captions)}\")\n",
    "        \n",
    "        # Generate images for this batch\n",
    "        batch_results = generate_batch_images(\n",
    "            list(captions),\n",
    "            num_per_prompt=num_images_per_caption,\n",
    "            num_inference_steps=inference_steps,\n",
    "            guidance_scale=7.5,\n",
    "            seed=batch_idx\n",
    "        )\n",
    "        \n",
    "        all_generated_images.update(batch_results)\n",
    "    \n",
    "    return all_generated_images\n",
    "\n",
    "\n",
    "# Process first 2 batches as a demo\n",
    "print(\"Processing batches from validation set...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_results = process_dataset_batch(val_loader, num_batches=2, num_images_per_caption=1)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(batch_results)} captions total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0239176",
   "metadata": {},
   "source": [
    "## 8. Batch Processing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated images\n",
    "display_images(generated_images)\n",
    "\n",
    "# Optional: Save images to disk\n",
    "save_images(generated_images, output_dir=\"../results/generated_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08021e",
   "metadata": {},
   "source": [
    "## 7. Visualize Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sample captions from the dataset\n",
    "num_samples = 5\n",
    "sample_captions = []\n",
    "\n",
    "# Get captions from training data\n",
    "for idx in range(min(num_samples, len(train_dataset))):\n",
    "    _, caption = train_dataset[idx]\n",
    "    sample_captions.append(caption)\n",
    "\n",
    "print(f\"Sample Captions from Dataset ({len(sample_captions)} total):\")\n",
    "for idx, caption in enumerate(sample_captions):\n",
    "    print(f\"  {idx+1}. {caption}\")\n",
    "\n",
    "# Generate images from these captions\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Generating images from dataset captions...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Reduce inference steps for faster generation on CPU\n",
    "# For better quality later on GPU, increase to 50+ steps\n",
    "inference_steps = 20 if not USE_GPU else 30\n",
    "\n",
    "generated_images = generate_batch_images(\n",
    "    sample_captions,\n",
    "    num_per_prompt=1,\n",
    "    num_inference_steps=inference_steps,\n",
    "    guidance_scale=7.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Image generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c067a",
   "metadata": {},
   "source": [
    "## 6. Generate Images from Dataset Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt, num_inference_steps=30, guidance_scale=7.5, \n",
    "                   height=512, width=512, seed=None):\n",
    "    \"\"\"\n",
    "    Generate an image from a text prompt using the pretrained model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Text description of the image to generate\n",
    "        num_inference_steps (int): Number of diffusion steps (higher = better quality, slower)\n",
    "        guidance_scale (float): Strength of guidance (higher = more faithful to prompt)\n",
    "        height (int): Image height in pixels\n",
    "        width (int): Image width in pixels\n",
    "        seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: Generated image\n",
    "    \"\"\"\n",
    "    if pipe is None:\n",
    "        print(\"Model not loaded. Please run the model loading cell first.\")\n",
    "        return None\n",
    "    \n",
    "    # Set seed if provided\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Use less memory by disabling autograd for inference\n",
    "            image = pipe(\n",
    "                prompt,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_batch_images(prompts, num_per_prompt=1, num_inference_steps=30, \n",
    "                         guidance_scale=7.5, seed=None):\n",
    "    \"\"\"\n",
    "    Generate multiple images from a batch of prompts.\n",
    "    \n",
    "    Args:\n",
    "        prompts (list): List of text prompts\n",
    "        num_per_prompt (int): Number of images to generate per prompt\n",
    "        num_inference_steps (int): Number of diffusion steps\n",
    "        guidance_scale (float): Guidance scale\n",
    "        seed (int): Base seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with prompts as keys and lists of images as values\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        print(f\"Generating images for prompt {idx+1}/{len(prompts)}: '{prompt}'\")\n",
    "        images = []\n",
    "        \n",
    "        for i in range(num_per_prompt):\n",
    "            current_seed = seed + (idx * num_per_prompt + i) if seed is not None else None\n",
    "            image = generate_image(\n",
    "                prompt,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                seed=current_seed\n",
    "            )\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "        \n",
    "        results[prompt] = images\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def display_images(image_dict, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display generated images in a grid.\n",
    "    \n",
    "    Args:\n",
    "        image_dict (dict): Dictionary with prompts and image lists\n",
    "        figsize (tuple): Figure size for matplotlib\n",
    "    \"\"\"\n",
    "    num_prompts = len(image_dict)\n",
    "    \n",
    "    for prompt, images in image_dict.items():\n",
    "        if not images:\n",
    "            continue\n",
    "        \n",
    "        num_images = len(images)\n",
    "        fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
    "        \n",
    "        if num_images == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        fig.suptitle(f\"Prompt: {prompt}\", fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for ax, img in zip(axes, images):\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def save_images(image_dict, output_dir=\"generated_images\"):\n",
    "    \"\"\"\n",
    "    Save generated images to disk.\n",
    "    \n",
    "    Args:\n",
    "        image_dict (dict): Dictionary with prompts and image lists\n",
    "        output_dir (str): Directory to save images\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for prompt_idx, (prompt, images) in enumerate(image_dict.items()):\n",
    "        for img_idx, image in enumerate(images):\n",
    "            # Create safe filename from prompt\n",
    "            safe_prompt = \"\".join(c for c in prompt if c.isalnum() or c in (' ', '_')).rstrip()\n",
    "            safe_prompt = safe_prompt[:50]  # Limit length\n",
    "            \n",
    "            filename = f\"{output_dir}/prompt_{prompt_idx:02d}_img_{img_idx:02d}_{safe_prompt}.png\"\n",
    "            image.save(filename)\n",
    "            print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"✓ Inference pipeline utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97061dbb",
   "metadata": {},
   "source": [
    "## 5. Define Inference Pipeline and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd882560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection - Options for lightweight models:\n",
    "# 1. Stable Diffusion v1.4 (compact, ~5GB)\n",
    "# 2. Stable Diffusion v2.1-base (smaller variant)\n",
    "# 3. LDM-3B (very lightweight)\n",
    "# 4. OpenJourney (community fine-tune)\n",
    "\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"  # Lightweight and widely used\n",
    "# Alternative lightweight models:\n",
    "# MODEL_ID = \"hakurei/waifu-diffusion\"  # Even lighter\n",
    "# MODEL_ID = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(\"This may take a few minutes on first run (downloads ~5GB)...\\n\")\n",
    "\n",
    "# For CPU usage, use float32; for GPU, can use float16 to save memory\n",
    "DTYPE = torch.float32 if not USE_GPU else torch.float16\n",
    "\n",
    "try:\n",
    "    # Load the full pipeline\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=DTYPE,\n",
    "        safety_checker=None,  # Disable safety checker for faster inference\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    pipe = pipe.to(DEVICE)\n",
    "    \n",
    "    # Enable memory-efficient attention for CPU/low-memory GPUs\n",
    "    pipe.enable_attention_slicing()\n",
    "    \n",
    "    if USE_GPU:\n",
    "        # Additional optimizations for GPU\n",
    "        pipe.enable_vae_slicing()\n",
    "        \n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "    print(f\"  Model dtype: {DTYPE}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure you have enough disk space for model download (~5GB)\")\n",
    "    pipe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96389257",
   "metadata": {},
   "source": [
    "## 4. Load Lightweight Pretrained Models from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e01c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path to access custom modules\n",
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))\n",
    "\n",
    "# Import custom dataloaders\n",
    "from dataloaders_text import caption_dataset, TextToImageDataset\n",
    "\n",
    "# Initialize dataloader\n",
    "dataloader_handler = caption_dataset()\n",
    "print(\"Loading datasets from COCO + Flickr...\")\n",
    "\n",
    "# Get datasets\n",
    "train_dataset, val_dataset, test_dataset = dataloader_handler.get_datasets()\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Create dataloaders with reduced batch size for CPU inference\n",
    "# For GPU training later, increase these values\n",
    "INFERENCE_BATCH_SIZE = 2 if USE_GPU else 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=INFERENCE_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0 if not USE_GPU else NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=INFERENCE_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0 if not USE_GPU else NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "# Get a sample batch to understand data shapes\n",
    "sample_images, sample_captions = next(iter(train_loader))\n",
    "print(f\"\\nSample Batch:\")\n",
    "print(f\"  Images shape: {sample_images.shape}\")\n",
    "print(f\"  Captions: {sample_captions[:2]}\")  # Show first 2 captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f676dc",
   "metadata": {},
   "source": [
    "## 3. Load Custom Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available hardware\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_GPU else 'cpu')\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Number of GPUs: {NUM_GPUS}\")\n",
    "if USE_GPU:\n",
    "    for i in range(NUM_GPUS):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Environment variables for optimization\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['OMP_NUM_THREADS'] = '16'\n",
    "\n",
    "# Batch size: scale by number of GPUs for multi-GPU training\n",
    "BATCH_SIZE = 4 * NUM_GPUS if NUM_GPUS > 0 else 4\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 4)\n",
    "PIN_MEMORY = USE_GPU\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Num Workers: {NUM_WORKERS}\")\n",
    "print(f\"  Pin Memory: {PIN_MEMORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d0257",
   "metadata": {},
   "source": [
    "## 2. Setup Device and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Transformers and Diffusers from Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import StableDiffusionPipeline, DDPMPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "# Image processing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc6a79",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1718b",
   "metadata": {},
   "source": [
    "# Text-to-Image Generation using Hugging Face Pretrained Models\n",
    "\n",
    "This notebook demonstrates lightweight text-to-image generation using pretrained models from Hugging Face. It's designed to work on CPU initially and can be scaled to multi-GPU environments like HiperGator.\n",
    "\n",
    "**Key Features:**\n",
    "- Lightweight pretrained models (DistilBERT for text encoding, compact diffusion models)\n",
    "- CPU-compatible with GPU acceleration support\n",
    "- Integration with existing COCO + Flickr dataloaders\n",
    "- Flexible inference pipeline\n",
    "- Ready for multi-GPU deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
