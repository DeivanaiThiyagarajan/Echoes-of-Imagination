{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179ce376",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0216544",
   "metadata": {},
   "source": [
    "# Fine-tune Stable Diffusion on Custom Dataset\n",
    "\n",
    "This notebook trains (fine-tunes) a Stable Diffusion model on your COCO + Flickr image-caption dataset to generate custom images specific to your domain.\n",
    "\n",
    "**Training Options:**\n",
    "- Full fine-tuning (all parameters)\n",
    "- LoRA fine-tuning (parameter-efficient, recommended for limited GPU)\n",
    "- Text encoder fine-tuning\n",
    "- UNet fine-tuning\n",
    "\n",
    "**Hardware Support:**\n",
    "- Works on CPU (slow, for debugging)\n",
    "- Single GPU (recommended minimum: 8GB VRAM)\n",
    "- Multi-GPU with DistributedDataParallel\n",
    "- HiperGator multi-node setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d692838",
   "metadata": {},
   "source": [
    "# Fine-tune Stable Diffusion on Custom Dataset\n",
    "\n",
    "This notebook trains (fine-tunes) a Stable Diffusion model on your COCO + Flickr image-caption dataset to generate custom images specific to your domain.\n",
    "\n",
    "**Training Options:**\n",
    "- Full fine-tuning (all parameters)\n",
    "- LoRA fine-tuning (parameter-efficient, recommended for limited GPU)\n",
    "- Text encoder fine-tuning\n",
    "- UNet fine-tuning\n",
    "\n",
    "**Hardware Support:**\n",
    "- Works on CPU (slow, for debugging)\n",
    "- Single GPU (recommended minimum: 8GB VRAM)\n",
    "- Multi-GPU with DistributedDataParallel\n",
    "- HiperGator multi-node setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7d456",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245ec3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aishu\\anaconda3\\envs\\eoi\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Hugging Face libraries\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, AutoTokenizer, BertTokenizer, BertModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline, LDMTextToImagePipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Image processing\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úì Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31dc81c",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hardware Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3faffe83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of GPUs: 0\n",
      "\n",
      "Training Configuration:\n",
      "  Model ID: stabilityai/sd-turbo\n",
      "  Image Size: 512x512\n",
      "  Batch Size: 2\n",
      "  Effective Batch Size (with accumulation): 8\n",
      "  Epochs: 10\n",
      "  Learning Rate: 0.0001\n",
      "  Fine-tune Text Encoder: False\n",
      "  Use LoRA: False\n",
      "  Mixed Precision: False\n"
     ]
    }
   ],
   "source": [
    "# ============ CONFIGURATION ============\n",
    "class Config:\n",
    "    # Model Configuration\n",
    "    # Choose one:\n",
    "    # - \"runwayml/stable-diffusion-v1-5\"        # 860M params, 20-40 hrs/epoch, best quality\n",
    "    # - \"stabilityai/stable-diffusion-2-base\"   # 500M params, 10-20 hrs/epoch, good balance\n",
    "    # - \"stabilityai/sd-turbo\"                  # 160M params, 2-4 hrs/epoch, RECOMMENDED for fast\n",
    "    model_id = \"stabilityai/sd-turbo\"\n",
    "    image_size = 512  # SD uses 512x512 (can reduce to 256 for faster training)\n",
    "    \n",
    "    # Training Configuration\n",
    "    num_epochs = 10\n",
    "    train_batch_size = 2  # Reduced for stability (increase if you have >8GB VRAM)\n",
    "    gradient_accumulation_steps = 4  # Simulates batch_size=8\n",
    "    learning_rate = 1e-4\n",
    "    max_grad_norm = 1.0\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # Warmup and scheduling\n",
    "    lr_warmup_steps = 500\n",
    "    num_train_timesteps = 1000\n",
    "    \n",
    "    # Fine-tuning options\n",
    "    fine_tune_text_encoder = False  # Set to True only if you have >12GB VRAM\n",
    "    use_lora = False  # Set True for parameter-efficient LoRA fine-tuning\n",
    "    lora_rank = 16  # LoRA matrix rank\n",
    "    \n",
    "    # Mixed precision\n",
    "    use_mixed_precision = torch.cuda.is_available()\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoints_dir = \"../models/checkpoints\"\n",
    "    save_interval = 2  # Save checkpoint every N epochs\n",
    "    \n",
    "    # Validation\n",
    "    num_validation_images = 4\n",
    "    validation_interval = 2\n",
    "    validation_prompts = [\n",
    "        \"a photo of a cat\",\n",
    "        \"a beautiful landscape with mountains\",\n",
    "        \"a person smiling at the camera\",\n",
    "        \"a delicious pizza on a plate\"\n",
    "    ]\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "\n",
    "# Hardware setup\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_GPU else 'cpu')\n",
    "LOCAL_RANK = int(os.environ.get('LOCAL_RANK', 0))\n",
    "WORLD_SIZE = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Number of GPUs: {NUM_GPUS}\")\n",
    "if USE_GPU:\n",
    "    for i in range(NUM_GPUS):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Adjust batch size for multi-GPU\n",
    "if NUM_GPUS > 1:\n",
    "    config.train_batch_size = max(1, config.train_batch_size // NUM_GPUS)\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Model ID: {config.model_id}\")\n",
    "print(f\"  Image Size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"  Batch Size: {config.train_batch_size}\")\n",
    "print(f\"  Effective Batch Size (with accumulation): {config.train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  Fine-tune Text Encoder: {config.fine_tune_text_encoder}\")\n",
    "print(f\"  Use LoRA: {config.use_lora}\")\n",
    "print(f\"  Mixed Precision: {config.use_mixed_precision}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config.checkpoints_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab90c959",
   "metadata": {},
   "source": [
    "## 3. Load Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732c5a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset using caption_dataset.get_dataloader()...\n",
      "  unique_image_identifier                                            caption  \\\n",
      "0          1000092795.jpg   Two young guys with shaggy hair look at their...   \n",
      "1          1000092795.jpg   Two young , White males are outside near many...   \n",
      "2          1000092795.jpg   Two men in green shirts are standing in a yard .   \n",
      "3          1000092795.jpg       A man in a blue shirt standing in a garden .   \n",
      "4          1000092795.jpg            Two friends enjoy time spent together .   \n",
      "\n",
      "                                          source_dir  source  \n",
      "0  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "1  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "2  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "3  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "4  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  unique_image_identifier                                            caption  \\\n",
      "0          1000092795.jpg   Two young guys with shaggy hair look at their...   \n",
      "1          1000092795.jpg   Two young , White males are outside near many...   \n",
      "2          1000092795.jpg   Two men in green shirts are standing in a yard .   \n",
      "3          1000092795.jpg       A man in a blue shirt standing in a garden .   \n",
      "4          1000092795.jpg            Two friends enjoy time spent together .   \n",
      "\n",
      "                                          source_dir  source  \n",
      "0  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "1  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "2  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "3  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "4  c:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagi...  Flickr  \n",
      "\n",
      "‚úì Dataloaders loaded successfully!\n",
      "  Train batches per epoch: 119609\n",
      "  Val batches per epoch: 14951\n",
      "\n",
      "Testing data format from dataloader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Image shape: torch.Size([4, 3, 64, 64])\n",
      "  Text: ('Horses in fenced area with grass and hay and adults nearby.', ' three people are dressed up as clowns in a city street .', ' A woman with a camera as many people behind her walk across the water on a walkway .', 'Two women walking on a side walk next to a street containing a car and a big white truck. ')\n"
     ]
    }
   ],
   "source": [
    "# Setup path to access custom modules\n",
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))\n",
    "\n",
    "from dataloaders_text import caption_dataset\n",
    "\n",
    "# Initialize dataloader handler\n",
    "print(\"Loading dataset using caption_dataset.get_dataloader()...\")\n",
    "dataloader_handler = caption_dataset()\n",
    "\n",
    "# Get dataloaders directly - much simpler!\n",
    "train_dataloader, train_sampler = dataloader_handler.get_dataloader(\n",
    "    partition=\"train\", \n",
    "    batch_size=config.train_batch_size,\n",
    "    distributed=False\n",
    ")\n",
    "\n",
    "val_dataloader, val_sampler = dataloader_handler.get_dataloader(\n",
    "    partition=\"val\", \n",
    "    batch_size=config.train_batch_size,\n",
    "    distributed=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataloaders loaded successfully!\")\n",
    "print(f\"  Train batches per epoch: {len(train_dataloader)}\")\n",
    "print(f\"  Val batches per epoch: {len(val_dataloader)}\")\n",
    "\n",
    "# Test a batch to see data format\n",
    "print(\"\\nTesting data format from dataloader...\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"  Image shape: {sample_batch[0].shape}\")  # Should be [B, C, H, W]\n",
    "print(f\"  Text: {sample_batch[1]}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20e8d8",
   "metadata": {},
   "source": [
    "## 4. Load Pretrained Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f38e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DIAGNOSTIC: Checking Model Download and Cache\n",
      "============================================================\n",
      "\n",
      "1. Checking internet connection...\n",
      "   ‚úì Internet connection OK\n",
      "\n",
      "2. HuggingFace cache location: C:\\Users\\Aishu\\.cache\\huggingface\\hub\n",
      "   Cache exists: True\n",
      "   Cached items: 12\n",
      "   Examples: ['.locks', 'datasets--ag_news', 'datasets--imdb']\n",
      "\n",
      "3. Checking for cached model: stabilityai/sd-turbo\n",
      "   Path: C:\\Users\\Aishu\\.cache\\huggingface\\hub\\models--stabilityai--sd-turbo\n",
      "   Cached: False\n",
      "\n",
      "‚ö†Ô∏è Model not cached. Download will occur during model loading.\n",
      "   Make sure you have:\n",
      "   - Stable internet connection\n",
      "   - ~5GB free disk space for model + cache\n",
      "   - Read/write permissions in ~/.cache/\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC: Checking Model Download and Cache\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import urllib.request\n",
    "import socket\n",
    "\n",
    "# Check internet connection\n",
    "print(\"\\n1. Checking internet connection...\")\n",
    "try:\n",
    "    urllib.request.urlopen('https://huggingface.co', timeout=5)\n",
    "    print(\"   ‚úì Internet connection OK\")\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è WARNING: No internet connection to HuggingFace Hub!\")\n",
    "    print(\"     Models may already be cached locally\")\n",
    "\n",
    "# Check HuggingFace cache\n",
    "hf_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "print(f\"\\n2. HuggingFace cache location: {hf_cache}\")\n",
    "print(f\"   Cache exists: {hf_cache.exists()}\")\n",
    "\n",
    "if hf_cache.exists():\n",
    "    cached_models = list(hf_cache.glob(\"*\"))\n",
    "    print(f\"   Cached items: {len(cached_models)}\")\n",
    "    if cached_models:\n",
    "        print(f\"   Examples: {[m.name[:50] for m in cached_models[:3]]}\")\n",
    "\n",
    "# Check if model is already cached\n",
    "model_name = config.model_id.replace(\"/\", \"--\")\n",
    "cached_model_path = hf_cache / f\"models--{model_name}\"\n",
    "print(f\"\\n3. Checking for cached model: {config.model_id}\")\n",
    "print(f\"   Path: {cached_model_path}\")\n",
    "print(f\"   Cached: {cached_model_path.exists()}\")\n",
    "\n",
    "# Show alternatives if needed\n",
    "if not cached_model_path.exists():\n",
    "    print(\"\\n‚ö†Ô∏è Model not cached. Download will occur during model loading.\")\n",
    "    print(\"   Make sure you have:\")\n",
    "    print(\"   - Stable internet connection\")\n",
    "    print(\"   - ~5GB free disk space for model + cache\")\n",
    "    print(\"   - Read/write permissions in ~/.cache/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b834d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pretrained model: stabilityai/sd-turbo\n",
      "‚úì Loading Stable Diffusion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì All components loaded successfully\n",
      "\n",
      "‚úì Model components loaded and ready\n",
      "  Text Encoder: CLIPTextModel\n",
      "  VAE: AutoencoderKL\n",
      "  UNet: UNet2DConditionModel\n",
      "  Tokenizer: CLIPTokenizer\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading pretrained model: {config.model_id}\")\n",
    "\n",
    "# All these models use standard Stable Diffusion architecture with CLIP tokenizer\n",
    "print(\"‚úì Loading Stable Diffusion model\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer (CLIP tokenizer - standard for all SD models)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(config.model_id, subfolder=\"tokenizer\")\n",
    "    \n",
    "    # Load text encoder (CLIP text model - standard for all SD models)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(config.model_id, subfolder=\"text_encoder\")\n",
    "    \n",
    "    # Load VAE (for encoding images to latents)\n",
    "    vae = AutoencoderKL.from_pretrained(config.model_id, subfolder=\"vae\")\n",
    "    \n",
    "    # Load UNet for diffusion\n",
    "    unet = UNet2DConditionModel.from_pretrained(config.model_id, subfolder=\"unet\")\n",
    "    \n",
    "    # Load noise scheduler\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(config.model_id, subfolder=\"scheduler\")\n",
    "    \n",
    "    print(\"  ‚úì All components loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚úó Error loading model: {e}\")\n",
    "    print(\"\\n  Trying fallback: using runwayml/stable-diffusion-v1-5\")\n",
    "    \n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "    \n",
    "    print(\"  ‚úì Fallback model loaded successfully\")\n",
    "\n",
    "# Move to device\n",
    "text_encoder = text_encoder.to(DEVICE)\n",
    "vae = vae.to(DEVICE)\n",
    "unet = unet.to(DEVICE)\n",
    "\n",
    "# Set to eval mode (we'll only fine-tune specific components)\n",
    "vae.eval()\n",
    "text_encoder.eval() if not config.fine_tune_text_encoder else text_encoder.train()\n",
    "unet.train()\n",
    "\n",
    "print(\"\\n‚úì Model components loaded and ready\")\n",
    "print(f\"  Text Encoder: {text_encoder.__class__.__name__}\")\n",
    "print(f\"  VAE: {vae.__class__.__name__}\")\n",
    "print(f\"  UNet: {unet.__class__.__name__}\")\n",
    "print(f\"  Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954749b",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bf292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrapping dataloader for fine-tuning...\n",
      "‚úì Dataloader wrapped successfully\n",
      "  Total batches: 119609\n"
     ]
    }
   ],
   "source": [
    "# Note: Dataloaders are already prepared above!\n",
    "# The dataloader returns: (image, caption) where caption is a string\n",
    "\n",
    "# For fine-tuning with diffusion, we need to:\n",
    "# 1. Encode images to latent space\n",
    "# 2. Tokenize captions and get text embeddings with proper shape [batch, seq_length, embedding_dim]\n",
    "\n",
    "# Create a wrapper to pre-encode images and tokenize captions\n",
    "class FineTuningWrapper:\n",
    "    def __init__(self, dataloader, vae, tokenizer, text_encoder, device, config):\n",
    "        \"\"\"\n",
    "        Wraps dataloader to pre-encode images to latents and tokenize captions.\n",
    "        \"\"\"\n",
    "        self.dataloader = dataloader\n",
    "        self.vae = vae\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Pre-encode each batch's images to latents and tokenize captions.\"\"\"\n",
    "        for batch_idx, (images, captions) in enumerate(self.dataloader):\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # Pre-encode images to latent space\n",
    "            with torch.no_grad():\n",
    "                # Resize if needed (dataloader uses 64x64, we might need 512x512)\n",
    "                if images.shape[-1] != self.config.image_size:\n",
    "                    images = F.interpolate(\n",
    "                        images, \n",
    "                        size=(self.config.image_size, self.config.image_size), \n",
    "                        mode='bilinear',\n",
    "                        align_corners=False\n",
    "                    )\n",
    "                \n",
    "                # Encode to latents\n",
    "                latents = self.vae.encode(images).latent_dist.sample()\n",
    "                latents = latents * 0.18215  # Stable Diffusion scaling\n",
    "                \n",
    "                # Tokenize captions (convert to list if necessary)\n",
    "                if isinstance(captions, torch.Tensor):\n",
    "                    # If captions are already tokens, convert to input_ids format\n",
    "                    input_ids = captions.to(self.device)\n",
    "                else:\n",
    "                    # If captions are strings, tokenize them\n",
    "                    if not isinstance(captions, (list, tuple)):\n",
    "                        captions = [captions]  # Handle single caption\n",
    "                    \n",
    "                    tokens = self.tokenizer(\n",
    "                        list(captions),\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=self.tokenizer.model_max_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    input_ids = tokens.input_ids.to(self.device)\n",
    "                \n",
    "                # Get text embeddings with proper shape [batch, seq_length, embedding_dim]\n",
    "                encoder_hidden_states = self.text_encoder(input_ids)[0]  # Shape: [batch, seq_length, 768]\n",
    "            \n",
    "            yield {\n",
    "                \"latent\": latents,\n",
    "                \"encoder_hidden_states\": encoder_hidden_states\n",
    "            }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "\n",
    "# Wrap the dataloader\n",
    "print(\"\\nWrapping dataloader for fine-tuning...\")\n",
    "wrapped_train_dataloader = FineTuningWrapper(\n",
    "    train_dataloader, \n",
    "    vae, \n",
    "    tokenizer, \n",
    "    text_encoder,\n",
    "    DEVICE,\n",
    "    config\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dataloader wrapped successfully\")\n",
    "print(f\"  Total batches: {len(wrapped_train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587ca76",
   "metadata": {},
   "source": [
    "## 6. Setup Training: Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8601ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Optimizer and scheduler configured\n",
      "  Trainable parameters: 865,910,724\n",
      "  Total training steps: 299030\n",
      "  Learning rate: 0.0001\n",
      "  Mixed precision: False\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer - only optimize trainable parameters\n",
    "trainable_params = list(unet.parameters())\n",
    "if config.fine_tune_text_encoder:\n",
    "    trainable_params += list(text_encoder.parameters())\n",
    "\n",
    "optimizer = AdamW(\n",
    "    trainable_params,\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    eps=1e-08\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / config.gradient_accumulation_steps\n",
    ")\n",
    "max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=max_train_steps\n",
    ")\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = GradScaler() if config.use_mixed_precision else None\n",
    "\n",
    "print(f\"\\n‚úì Optimizer and scheduler configured\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in trainable_params if p.requires_grad):,}\")\n",
    "print(f\"  Total training steps: {max_train_steps}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Mixed precision: {config.use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde6866",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb864fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Fine-tuning...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/119609 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 2/119609 [05:06<4856:50:48, 146.18s/it, loss=0.1102]"
     ]
    }
   ],
   "source": [
    "def train_epoch(epoch_num, wrapped_dataloader, optimizer, lr_scheduler, scaler):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    progress_bar = tqdm(total=len(wrapped_dataloader), desc=f\"Epoch {epoch_num}\")\n",
    "    losses = []\n",
    "    \n",
    "    for step, batch in enumerate(wrapped_dataloader):\n",
    "        # Get batch data\n",
    "        latents = batch[\"latent\"]  # Already on device, shape: [B, 4, H, W]\n",
    "        encoder_hidden_states = batch[\"encoder_hidden_states\"]  # Already on device, shape: [B, seq_len, 768]\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        timesteps = torch.randint(\n",
    "            0, config.num_train_timesteps, (latents.shape[0],), device=DEVICE\n",
    "        ).long()\n",
    "        \n",
    "        # Sample noise to add to latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        \n",
    "        # Add noise to latents (forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Predict noise residual with text conditioning\n",
    "        if config.use_mixed_precision:\n",
    "            with autocast():\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate loss (L2 loss between predicted and actual noise)\n",
    "                loss = F.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "        else:\n",
    "            model_pred = unet(\n",
    "                noisy_latents,\n",
    "                timesteps,\n",
    "                encoder_hidden_states\n",
    "            ).sample\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "        \n",
    "        # Backward pass with gradient accumulation\n",
    "        if config.use_mixed_precision:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        losses.append(loss.detach().item())\n",
    "        \n",
    "        # Update weights\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            if config.use_mixed_precision:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, config.max_grad_norm)\n",
    "            \n",
    "            if config.use_mixed_precision:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Fine-tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = {\"train_loss\": [], \"epoch\": []}\n",
    "\n",
    "try:\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        avg_loss = train_epoch(epoch+1, wrapped_train_dataloader, optimizer, lr_scheduler, scaler)\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"epoch\"].append(epoch+1)\n",
    "        \n",
    "        print(f\"Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % config.save_interval == 0:\n",
    "            checkpoint_path = os.path.join(\n",
    "                config.checkpoints_dir,\n",
    "                f\"checkpoint-epoch-{epoch+1}\"\n",
    "            )\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "            \n",
    "            unet.save_pretrained(os.path.join(checkpoint_path, \"unet\"))\n",
    "            if config.fine_tune_text_encoder:\n",
    "                text_encoder.save_pretrained(os.path.join(checkpoint_path, \"text_encoder\"))\n",
    "            \n",
    "            print(f\"‚úì Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c0127",
   "metadata": {},
   "source": [
    "## 8. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b026cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "if history[\"train_loss\"]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], marker='o', linewidth=2)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Average Loss\", fontsize=12)\n",
    "    plt.title(\"Training Loss Over Epochs\", fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../results/training_loss.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úì Training loss plot saved\")\n",
    "else:\n",
    "    print(\"No training history available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088267a2",
   "metadata": {},
   "source": [
    "## 9. Load Fine-tuned Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with fine-tuned model\n",
    "checkpoint_dir = os.path.join(config.checkpoints_dir, f\"checkpoint-epoch-{config.num_epochs}\")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"Loading fine-tuned model from: {checkpoint_dir}\")\n",
    "    \n",
    "    # Load fine-tuned components\n",
    "    unet_finetuned = UNet2DConditionModel.from_pretrained(\n",
    "        os.path.join(checkpoint_dir, \"unet\")\n",
    "    )\n",
    "    \n",
    "    text_encoder_finetuned = CLIPTextModel.from_pretrained(\n",
    "        os.path.join(checkpoint_dir, \"text_encoder\")\n",
    "    ) if config.fine_tune_text_encoder else CLIPTextModel.from_pretrained(\n",
    "        config.model_id, subfolder=\"text_encoder\"\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with fine-tuned components\n",
    "    pipe_finetuned = StableDiffusionPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        unet=unet_finetuned,\n",
    "        text_encoder=text_encoder_finetuned,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"‚úì Fine-tuned pipeline created successfully\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "    print(\"Using original pretrained model for inference\")\n",
    "    \n",
    "    pipe_finetuned = StableDiffusionPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd2e64",
   "metadata": {},
   "source": [
    "## 10. Test Fine-tuned Model with Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b301c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test images with different prompts\n",
    "test_prompts = [\n",
    "    \"a dog playing in the park\",\n",
    "    \"a woman reading a book in a cafe\",\n",
    "    \"a sunset over mountains\",\n",
    "    \"children building a sandcastle at the beach\",\n",
    "    \"a cat sitting on a windowsill\"\n",
    "]\n",
    "\n",
    "print(f\"Generating test images with fine-tuned model on device: {DEVICE}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = pipe_finetuned(\n",
    "            prompt,\n",
    "            num_inference_steps=50,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            generator=torch.Generator(device=DEVICE).manual_seed(42)\n",
    "        ).images\n",
    "    \n",
    "    generated_images.append(images[0])\n",
    "    print(f\"‚úì Generated 512x512 image successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Generated {len(generated_images)} test images\")\n",
    "print(\"\\nSample outputs saved (showing first 3):\")\n",
    "\n",
    "# Display first 3 images\n",
    "fig, axes = plt.subplots(1, min(3, len(generated_images)), figsize=(15, 5))\n",
    "if len(generated_images) == 1:\n",
    "    axes = [axes]\n",
    "    \n",
    "for idx, (ax, img) in enumerate(zip(axes, generated_images[:3])):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(test_prompts[idx][:30] + \"...\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.results_dir, \"test_inference.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTest images saved to: \" + os.path.join(config.results_dir, \"test_inference.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16180249",
   "metadata": {},
   "source": [
    "## 11. Multi-GPU Training and HiperGator Deployment\n",
    "\n",
    "### Running on Multi-GPU Locally:\n",
    "```bash\n",
    "# Single node, 2 GPUs\n",
    "torchrun --nproc_per_node=2 fine_tune_distributed.py\n",
    "```\n",
    "\n",
    "### HiperGator SBATCH Configuration:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --gpus-per-node=a100:2\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=96GB\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --job-name=sd-finetune\n",
    "\n",
    "module load python cuda/11.8\n",
    "source venv/bin/activate\n",
    "\n",
    "torchrun --nproc_per_node=2 --nnodes=2 \\\n",
    "         --master_addr=$MASTER_ADDR --master_port=29500 \\\n",
    "         fine_tune_distributed.py\n",
    "```\n",
    "\n",
    "### Key Considerations:\n",
    "- **Memory**: A100 GPUs (40GB) can handle batch_size=8-16\n",
    "- **Mixed Precision**: Enabled by default (reduces memory ~2x)\n",
    "- **Gradient Accumulation**: Simulates larger batches\n",
    "- **Data Loading**: Pre-encoded latents reduce I/O overhead\n",
    "- **Checkpointing**: Save every N epochs to resume if interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3df1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images using fine-tuned model\n",
    "test_prompts = [\n",
    "    \"a photo of a cat sleeping\",\n",
    "    \"a beautiful sunset over mountains\",\n",
    "    \"a person laughing at camera\",\n",
    "    \"delicious food on a plate\"\n",
    "]\n",
    "\n",
    "print(\"Generating images with fine-tuned model...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, prompt in enumerate(test_prompts):\n",
    "    print(f\"Generating: '{prompt}'\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = pipe_finetuned(\n",
    "            prompt,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            num_images_per_prompt=1\n",
    "        ).images[0]\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f\"'{prompt}'\", fontsize=10, wrap=True)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/finetuned_generation_samples.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Sample generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdaea0",
   "metadata": {},
   "source": [
    "## 12. LoRA Fine-tuning (Parameter-Efficient Alternative)\n",
    "\n",
    "LoRA (Low-Rank Adaptation) fine-tunes only small matrices added to the model, reducing:\n",
    "- **Memory**: ~80% reduction\n",
    "- **Storage**: Checkpoint size ~50MB instead of 4GB\n",
    "- **Time**: Faster convergence\n",
    "\n",
    "Uncomment the code below to use LoRA instead of full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 11. CLIP Alignment Evaluation\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLIP ALIGNMENT EVALUATION - Pretrained vs Fine-tuned\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Install clip if needed\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        import clip\n",
    "    except ImportError:\n",
    "        print(\"Installing CLIP...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai-clip\", \"-q\"])\n",
    "        import clip\n",
    "    \n",
    "    from PIL import Image as PILImage\n",
    "    import torch.nn.functional as F_nn\n",
    "    \n",
    "    # Load CLIP model\n",
    "    device_clip = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device_clip)\n",
    "    clip_model.eval()\n",
    "    \n",
    "    print(f\"‚úì CLIP model loaded on {device_clip}\")\n",
    "    \n",
    "    def calculate_clip_score(image, text, model, preprocess, device):\n",
    "        \"\"\"Calculate CLIP alignment score between image and text (0-100)\"\"\"\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            if isinstance(image, PILImage.Image):\n",
    "                image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "            else:\n",
    "                image_input = image.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Tokenize text\n",
    "            text_input = clip.tokenize([text]).to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(image_input)\n",
    "                text_features = model.encode_text(text_input)\n",
    "                \n",
    "                # Normalize\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity = (image_features @ text_features.t()).squeeze()\n",
    "                score = float(similarity.cpu().numpy()) * 100  # Scale to 0-100\n",
    "            \n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating CLIP score: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Evaluate on test prompts\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Evaluating on test prompts (Pretrained vs Fine-tuned)\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    pretrained_scores = []\n",
    "    finetuned_scores = []\n",
    "    \n",
    "    for idx, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\nüìä Prompt {idx + 1}:\")\n",
    "        print(f\"  Text: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            # Generate with PRETRAINED model\n",
    "            with torch.no_grad():\n",
    "                generated_pretrained = pipe_pretrained(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=50,\n",
    "                    guidance_scale=7.5,\n",
    "                    generator=torch.Generator(device=device).manual_seed(42)\n",
    "                ).images[0]\n",
    "            \n",
    "            # Generate with FINE-TUNED model\n",
    "            with torch.no_grad():\n",
    "                generated_finetuned = pipe(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=50,\n",
    "                    guidance_scale=7.5,\n",
    "                    generator=torch.Generator(device=device).manual_seed(42)\n",
    "                ).images[0]\n",
    "            \n",
    "            # Calculate CLIP scores\n",
    "            score_pretrained = calculate_clip_score(generated_pretrained, prompt, clip_model, clip_preprocess, device_clip)\n",
    "            score_finetuned = calculate_clip_score(generated_finetuned, prompt, clip_model, clip_preprocess, device_clip)\n",
    "            \n",
    "            if score_pretrained and score_finetuned:\n",
    "                pretrained_scores.append(score_pretrained)\n",
    "                finetuned_scores.append(score_finetuned)\n",
    "                \n",
    "                improvement = score_finetuned - score_pretrained\n",
    "                improvement_pct = (improvement / score_pretrained * 100) if score_pretrained > 0 else 0\n",
    "                \n",
    "                print(f\"  Pretrained CLIP Score: {score_pretrained:.2f}/100\")\n",
    "                print(f\"  Fine-tuned CLIP Score: {score_finetuned:.2f}/100\")\n",
    "                print(f\"  Improvement: {improvement:+.2f} ({improvement_pct:+.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary statistics\n",
    "    if pretrained_scores and finetuned_scores:\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"CLIP ALIGNMENT SUMMARY\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        avg_pretrained = np.mean(pretrained_scores)\n",
    "        avg_finetuned = np.mean(finetuned_scores)\n",
    "        avg_improvement = avg_finetuned - avg_pretrained\n",
    "        avg_improvement_pct = (avg_improvement / avg_pretrained * 100) if avg_pretrained > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìà Average CLIP Scores ({len(pretrained_scores)} samples):\")\n",
    "        print(f\"  Pretrained: {avg_pretrained:.2f}/100\")\n",
    "        print(f\"  Fine-tuned: {avg_finetuned:.2f}/100\")\n",
    "        print(f\"  Average Improvement: {avg_improvement:+.2f} ({avg_improvement_pct:+.1f}%)\")\n",
    "        \n",
    "        if avg_improvement > 0:\n",
    "            print(f\"\\n‚úÖ Fine-tuned model shows BETTER text-image alignment!\")\n",
    "        elif avg_improvement < 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  Fine-tuned model shows LOWER text-image alignment\")\n",
    "        else:\n",
    "            print(f\"\\n‚ûñ No significant difference in alignment\")\n",
    "        \n",
    "        # Visualize comparison\n",
    "        fig, axes = plt.subplots(min(2, len(test_prompts)), 2, figsize=(12, 5*min(2, len(test_prompts))))\n",
    "        if min(2, len(test_prompts)) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx, prompt in enumerate(test_prompts[:2]):\n",
    "            # Generate images\n",
    "            with torch.no_grad():\n",
    "                gen_pretrained = pipe_pretrained(\n",
    "                    prompt=prompt, num_inference_steps=50, guidance_scale=7.5,\n",
    "                    generator=torch.Generator(device=device).manual_seed(42)\n",
    "                ).images[0]\n",
    "                \n",
    "                gen_finetuned = pipe(\n",
    "                    prompt=prompt, num_inference_steps=50, guidance_scale=7.5,\n",
    "                    generator=torch.Generator(device=device).manual_seed(42)\n",
    "                ).images[0]\n",
    "            \n",
    "            # Plot\n",
    "            axes[idx, 0].imshow(gen_pretrained)\n",
    "            axes[idx, 0].set_title(f\"Pretrained\\n({pretrained_scores[idx]:.1f}/100)\", fontsize=10, color='blue')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            axes[idx, 1].imshow(gen_finetuned)\n",
    "            axes[idx, 1].set_title(f\"Fine-tuned\\n({finetuned_scores[idx]:.1f}/100)\", fontsize=10, color='green')\n",
    "            axes[idx, 1].axis('off')\n",
    "        \n",
    "        plt.suptitle('CLIP Alignment Comparison: Pretrained vs Fine-tuned', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"../results/clip_alignment_comparison.png\", dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"\\n‚úì Comparison visualization saved to: ../results/clip_alignment_comparison.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  CLIP evaluation skipped: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6909954-c0ff-4962-af9a-56fc5c522e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To use LoRA fine-tuning, install: pip install peft\n",
    "\n",
    "Then replace the model loading with:\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"to_q\", \"to_v\"],  # Target modules in attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "# Now only LoRA parameters will be trained (~1% of model)\n",
    "# This significantly reduces memory and training time\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fd6f3-87ef-4856-a837-fdbc8b309508",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "### What This Notebook Does:\n",
    "‚úÖ **Fine-tunes Stable Diffusion** on your COCO + Flickr dataset  \n",
    "‚úÖ **Trains UNet** for custom image generation  \n",
    "‚úÖ **Optionally fine-tunes text encoder** for better text understanding  \n",
    "‚úÖ **Uses pre-encoded latents** for efficient training  \n",
    "‚úÖ **Implements mixed precision** for reduced memory usage  \n",
    "‚úÖ **Saves checkpoints** for resuming training  \n",
    "‚úÖ **Generates test images** to verify quality  \n",
    "\n",
    "### Key Features:\n",
    "- **Memory Efficient**: Pre-encoding images, gradient accumulation, mixed precision\n",
    "- **Flexible**: Fine-tune UNet only or + text encoder\n",
    "- **Scalable**: Works on single GPU ‚Üí multi-GPU ‚Üí HiperGator\n",
    "- **Lightweight**: LoRA option available (~80% memory reduction)\n",
    "\n",
    "### Hyperparameters to Adjust:\n",
    "- `train_batch_size`: Increase if GPU has memory (8-16 for A100)\n",
    "- `learning_rate`: 1e-4 is good starting point\n",
    "- `num_epochs`: 10-20 typically sufficient for fine-tuning\n",
    "- `image_size`: 512x512 standard, can use 768x768 on A100\n",
    "- `num_inference_steps`: 30-50 for inference quality\n",
    "\n",
    "### Next Steps:\n",
    "1. Start with small batch size and few epochs to test\n",
    "2. Monitor loss curve for overfitting\n",
    "3. Save best checkpoint (lowest loss)\n",
    "4. Evaluate with CLIP scores or manual inspection\n",
    "5. Deploy to HiperGator for larger-scale training\n",
    "6. Consider LoRA for parameter-efficient fine-tuning\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Out of memory**: Reduce batch size or enable gradient checkpointing\n",
    "- **Slow training**: Check if data loading is bottleneck\n",
    "- **Loss not decreasing**: Check learning rate, data quality\n",
    "- **Bad quality images**: Train longer, increase inference steps\n",
    "\n",
    "Good luck! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a217352-030d-4755-b3ca-ef7861cb301e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
