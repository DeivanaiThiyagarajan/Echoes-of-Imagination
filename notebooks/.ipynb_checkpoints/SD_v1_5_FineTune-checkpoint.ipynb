{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5f33f8",
   "metadata": {},
   "source": [
    "# Stable Diffusion v1.5 Fine-Tuning Pipeline\n",
    "Complete text-to-image generation fine-tuning using SD v1.5 on COCO + Flickr30k datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46695c",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and Diffusers\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# PyTorch utilities\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Check GPU availability\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0' if USE_GPU else 'cpu')\n",
    "NUM_GPUS = torch.cuda.device_count() if USE_GPU else 0\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {USE_GPU}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Number of GPUs: {NUM_GPUS}\")\n",
    "if USE_GPU:\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7699e",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "**Model**: Stable Diffusion v1.5 (860M parameters)\n",
    "- Training time: 20-40 hours per epoch (on single GPU)\n",
    "- Memory requirement: 8GB+ VRAM\n",
    "- Image resolution: 512x512 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Model configuration\n",
    "    model_id: str = \"runwayml/stable-diffusion-v1-5\"\n",
    "    image_size: int = 512\n",
    "    center_crop: bool = True\n",
    "    random_flip: bool = True\n",
    "    \n",
    "    # Training configuration\n",
    "    train_batch_size: int = 1  # Reduced for SD v1.5 (860M params)\n",
    "    eval_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 4  # Effective batch size = 4\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "    num_epochs: int = 10\n",
    "    warmup_steps: int = 500\n",
    "    max_grad_norm: float = 1.0\n",
    "    use_mixed_precision: bool = True\n",
    "    \n",
    "    # Fine-tuning strategy\n",
    "    fine_tune_text_encoder: bool = False  # Keep frozen to save memory\n",
    "    fine_tune_unet: bool = True\n",
    "    fine_tune_vae: bool = False  # Keep frozen\n",
    "    \n",
    "    # Data configuration\n",
    "    num_workers: int = 4\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Paths\n",
    "    checkpoints_dir: str = \"./models/checkpoints\"\n",
    "    results_dir: str = \"./results\"\n",
    "    log_dir: str = \"./logs\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_id}\")\n",
    "print(f\"  Batch size: {config.train_batch_size} (effective: {config.train_batch_size * config.gradient_accumulation_steps})\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Fine-tune UNet: {config.fine_tune_unet}\")\n",
    "print(f\"  Fine-tune Text Encoder: {config.fine_tune_text_encoder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f64200",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to path for dataloaders\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from dataloaders_text import caption_dataset\n",
    "\n",
    "# Initialize dataset\n",
    "dataloader_handler = caption_dataset()\n",
    "\n",
    "# Get train and validation dataloaders\n",
    "train_dataloader = dataloader_handler.get_dataloader(\n",
    "    split='train',\n",
    "    batch_size=config.train_batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = dataloader_handler.get_dataloader(\n",
    "    split='val',\n",
    "    batch_size=config.eval_batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train dataloader length: {len(train_dataloader)}\")\n",
    "print(f\"Val dataloader length: {len(val_dataloader)}\")\n",
    "\n",
    "# Get sample batch\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"\\nSample batch:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} - {value.dtype}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"  {key}: list of {len(value)} items\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d809d95",
   "metadata": {},
   "source": [
    "## 4. Load Pretrained Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading Stable Diffusion v1.5 from: {config.model_id}\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        config.model_id,\n",
    "        subfolder=\"tokenizer\"\n",
    "    )\n",
    "    print(\"✓ Tokenizer loaded\")\n",
    "    \n",
    "    # Load text encoder\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        config.model_id,\n",
    "        subfolder=\"text_encoder\",\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32\n",
    "    ).to(DEVICE)\n",
    "    print(\"✓ Text encoder loaded\")\n",
    "    \n",
    "    # Load VAE\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        config.model_id,\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32\n",
    "    ).to(DEVICE)\n",
    "    print(\"✓ VAE loaded\")\n",
    "    \n",
    "    # Load UNet\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        config.model_id,\n",
    "        subfolder=\"unet\",\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32\n",
    "    ).to(DEVICE)\n",
    "    print(\"✓ UNet loaded\")\n",
    "    \n",
    "    # Load scheduler\n",
    "    scheduler = DDPMScheduler.from_pretrained(\n",
    "        config.model_id,\n",
    "        subfolder=\"scheduler\"\n",
    "    )\n",
    "    print(\"✓ Scheduler loaded\")\n",
    "    \n",
    "    print(f\"\\n✓ All components loaded successfully!\")\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    print(f\"  Text Encoder: {sum(p.numel() for p in text_encoder.parameters()) / 1e6:.1f}M parameters\")\n",
    "    print(f\"  VAE: {sum(p.numel() for p in vae.parameters()) / 1e6:.1f}M parameters\")\n",
    "    print(f\"  UNet: {sum(p.numel() for p in unet.parameters()) / 1e6:.1f}M parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model: {e}\")\n",
    "    print(f\"Attempting to clear cache and retry...\")\n",
    "    import shutil\n",
    "    hf_cache = Path.home() / '.cache' / 'huggingface' / 'hub'\n",
    "    if hf_cache.exists():\n",
    "        shutil.rmtree(hf_cache)\n",
    "        print(\"Cache cleared. Please re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0901a8",
   "metadata": {},
   "source": [
    "## 5. Fine-Tuning Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningWrapper(torch.nn.Module):\n",
    "    \"\"\"Wrapper class for fine-tuning Stable Diffusion v1.5\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        unet,\n",
    "        text_encoder,\n",
    "        vae,\n",
    "        tokenizer,\n",
    "        scheduler,\n",
    "        fine_tune_text_encoder: bool = False,\n",
    "        device: str = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vae = vae\n",
    "        self.tokenizer = tokenizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.fine_tune_text_encoder = fine_tune_text_encoder\n",
    "        \n",
    "        # Freeze components we're not fine-tuning\n",
    "        if not fine_tune_text_encoder:\n",
    "            self.text_encoder.requires_grad_(False)\n",
    "        self.vae.requires_grad_(False)\n",
    "        self.scheduler.requires_grad_(False)\n",
    "        \n",
    "    def encode_text(self, captions):\n",
    "        \"\"\"Encode text captions to embeddings\"\"\"\n",
    "        tokens = self.tokenizer(\n",
    "            captions,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        token_ids = tokens['input_ids'].to(self.device)\n",
    "        attention_mask = tokens['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(\n",
    "                token_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )[0]\n",
    "        return text_embeddings\n",
    "    \n",
    "    def encode_images(self, images):\n",
    "        \"\"\"Encode images to latent space using VAE\"\"\"\n",
    "        images = images.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            latents = self.vae.encode(images).latent_dist.sample()\n",
    "            latents = latents * 0.18215  # VAE scaling factor\n",
    "        return latents\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        images,\n",
    "        captions,\n",
    "        timesteps=None\n",
    "    ):\n",
    "        \"\"\"Forward pass for training\"\"\"\n",
    "        # Encode images\n",
    "        latents = self.encode_images(images)\n",
    "        batch_size = latents.shape[0]\n",
    "        \n",
    "        # Encode text\n",
    "        text_embeddings = self.encode_text(captions)\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        if timesteps is None:\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                self.scheduler.config.num_train_timesteps,\n",
    "                (batch_size,),\n",
    "                device=latents.device\n",
    "            ).long()\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "        \n",
    "        # Add noise to latents (forward diffusion process)\n",
    "        noisy_latents = self.scheduler.add_noise(\n",
    "            latents,\n",
    "            noise,\n",
    "            timesteps\n",
    "        )\n",
    "        \n",
    "        # Predict noise with UNet\n",
    "        model_pred = self.unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            text_embeddings\n",
    "        ).sample\n",
    "        \n",
    "        # Compute loss (MSE between predicted and actual noise)\n",
    "        loss = torch.nn.functional.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Create wrapper instance\n",
    "wrapper = FineTuningWrapper(\n",
    "    unet=unet,\n",
    "    text_encoder=text_encoder,\n",
    "    vae=vae,\n",
    "    tokenizer=tokenizer,\n",
    "    scheduler=scheduler,\n",
    "    fine_tune_text_encoder=config.fine_tune_text_encoder,\n",
    "    device=str(DEVICE)\n",
    ")\n",
    "\n",
    "print(\"✓ Fine-tuning wrapper created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aca3a1",
   "metadata": {},
   "source": [
    "## 6. Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trainable parameters\n",
    "trainable_params = list(unet.parameters())\n",
    "if config.fine_tune_text_encoder:\n",
    "    trainable_params.extend(text_encoder.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params) / 1e6:.1f}M\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = AdamW(\n",
    "    trainable_params,\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "total_steps = len(train_dataloader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "scheduler_lr = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {config.warmup_steps}\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config.use_mixed_precision else None\n",
    "print(f\"Mixed precision: {config.use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbd9a1",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "global_step = 0\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Steps per epoch: {len(train_dataloader)}\")\n",
    "print(f\"Total steps: {len(train_dataloader) * config.num_epochs}\")\n",
    "print(f\"Device: {DEVICE}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Training phase\n",
    "    unet.train()\n",
    "    if config.fine_tune_text_encoder:\n",
    "        text_encoder.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training\", ncols=80)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            images = batch['images'].to(DEVICE)\n",
    "            captions = batch['captions']\n",
    "            \n",
    "            with autocast(enabled=config.use_mixed_precision and USE_GPU):\n",
    "                loss = wrapper(images, captions)\n",
    "            \n",
    "            # Backward pass with gradient accumulation\n",
    "            if config.use_mixed_precision and USE_GPU:\n",
    "                scaler.scale(loss / config.gradient_accumulation_steps).backward()\n",
    "            else:\n",
    "                (loss / config.gradient_accumulation_steps).backward()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
    "                if config.use_mixed_precision and USE_GPU:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(trainable_params, config.max_grad_norm)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(trainable_params, config.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                scheduler_lr.step()\n",
    "                global_step += 1\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item():.4f})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    print(f\"\\nEpoch {epoch + 1} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(config.checkpoints_dir, f\"checkpoint-epoch-{epoch + 1}\")\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    unet.save_pretrained(os.path.join(checkpoint_path, \"unet\"))\n",
    "    if config.fine_tune_text_encoder:\n",
    "        text_encoder.save_pretrained(os.path.join(checkpoint_path, \"text_encoder\"))\n",
    "    print(f\"✓ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Final loss: {train_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c6b95",
   "metadata": {},
   "source": [
    "## 8. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65499775",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.results_dir, 'training_loss.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss plot saved to: {os.path.join(config.results_dir, 'training_loss.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd2324",
   "metadata": {},
   "source": [
    "## 9. Load Fine-Tuned Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81368cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with fine-tuned model\n",
    "checkpoint_dir = os.path.join(config.checkpoints_dir, f\"checkpoint-epoch-{config.num_epochs}\")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"Loading fine-tuned model from: {checkpoint_dir}\")\n",
    "    \n",
    "    # Load fine-tuned UNet\n",
    "    unet_finetuned = UNet2DConditionModel.from_pretrained(\n",
    "        os.path.join(checkpoint_dir, \"unet\")\n",
    "    )\n",
    "    \n",
    "    # Load fine-tuned text encoder if available\n",
    "    text_encoder_finetuned = CLIPTextModel.from_pretrained(\n",
    "        os.path.join(checkpoint_dir, \"text_encoder\")\n",
    "    ) if os.path.exists(os.path.join(checkpoint_dir, \"text_encoder\")) else CLIPTextModel.from_pretrained(\n",
    "        config.model_id, subfolder=\"text_encoder\"\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with fine-tuned components\n",
    "    pipe_finetuned = StableDiffusionPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        unet=unet_finetuned,\n",
    "        text_encoder=text_encoder_finetuned,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"✓ Fine-tuned pipeline created successfully\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "    print(\"Using original pretrained model for inference\")\n",
    "    \n",
    "    pipe_finetuned = StableDiffusionPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"✓ Original pipeline loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd8b13",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e187be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test images with different prompts\n",
    "test_prompts = [\n",
    "    \"a dog playing in the park\",\n",
    "    \"a woman reading a book in a cafe\",\n",
    "    \"a sunset over mountains\",\n",
    "    \"children building a sandcastle at the beach\",\n",
    "    \"a cat sitting on a windowsill\"\n",
    "]\n",
    "\n",
    "print(f\"Generating test images with fine-tuned model on device: {DEVICE}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = pipe_finetuned(\n",
    "            prompt,\n",
    "            num_inference_steps=50,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            generator=torch.Generator(device=DEVICE).manual_seed(42)\n",
    "        ).images\n",
    "    \n",
    "    generated_images.append(images[0])\n",
    "    print(f\"✓ Generated 512x512 image successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Generated {len(generated_images)} test images\")\n",
    "print(\"\\nSample outputs saved (showing first 3):\")\n",
    "\n",
    "# Display first 3 images\n",
    "fig, axes = plt.subplots(1, min(3, len(generated_images)), figsize=(15, 5))\n",
    "if len(generated_images) == 1:\n",
    "    axes = [axes]\n",
    "    \n",
    "for idx, (ax, img) in enumerate(zip(axes, generated_images[:3])):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(test_prompts[idx][:30] + \"...\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.results_dir, \"test_inference.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTest images saved to: \" + os.path.join(config.results_dir, \"test_inference.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0823d6",
   "metadata": {},
   "source": [
    "## 11. Save Model Configuration and Results\n",
    "Save the training configuration and results for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc653e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "config_dict = {\n",
    "    'model_id': config.model_id,\n",
    "    'image_size': config.image_size,\n",
    "    'batch_size': config.train_batch_size,\n",
    "    'effective_batch_size': config.train_batch_size * config.gradient_accumulation_steps,\n",
    "    'learning_rate': config.learning_rate,\n",
    "    'epochs': config.num_epochs,\n",
    "    'warmup_steps': config.warmup_steps,\n",
    "    'fine_tune_text_encoder': config.fine_tune_text_encoder,\n",
    "    'fine_tune_unet': config.fine_tune_unet,\n",
    "    'use_mixed_precision': config.use_mixed_precision,\n",
    "    'device': str(DEVICE),\n",
    "    'final_loss': float(train_losses[-1]) if train_losses else None,\n",
    "}\n",
    "\n",
    "with open(os.path.join(config.results_dir, 'training_config.json'), 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(\"✓ Configuration saved to: \" + os.path.join(config.results_dir, 'training_config.json'))\n",
    "print(\"\\nTraining Summary:\")\n",
    "for key, value in config_dict.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97b346",
   "metadata": {},
   "source": [
    "## 12. Clean Up and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d60aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {config.model_id}\")\n",
    "print(f\"Training epochs: {config.num_epochs}\")\n",
    "print(f\"Total samples processed: {len(train_dataloader) * config.train_batch_size * config.num_epochs}\")\n",
    "print(f\"Initial loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Loss reduction: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%\")\n",
    "print(f\"\\nCheckpoints saved in: {config.checkpoints_dir}\")\n",
    "print(f\"Results saved in: {config.results_dir}\")\n",
    "print(f\"Logs saved in: {config.log_dir}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0236cf6",
   "metadata": {},
   "source": [
    "## 13. References and Documentation\n",
    "- **Stable Diffusion v1.5**: [Runaway ML](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
    "- **Paper**: [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)\n",
    "- **Datasets**: [COCO 2014/2017](https://cocodataset.org/), [Flickr30K](https://shannon.cs.illinois.edu/DenotationGraph/)\n",
    "- **Frameworks**: PyTorch 2.0+, Hugging Face Transformers & Diffusers\n",
    "\n",
    "### Key Parameters Explained\n",
    "- **Batch Size**: Reduced to 1 for Stable Diffusion v1.5 due to 860M parameters\n",
    "- **Gradient Accumulation**: 4 steps to achieve effective batch size of 4 while saving memory\n",
    "- **Learning Rate**: 1e-4 following original SD paper recommendations\n",
    "- **Epochs**: 10 for reasonable training time (~20-40 hours on single GPU)\n",
    "\n",
    "### Troubleshooting\n",
    "- **Out of Memory**: Reduce `train_batch_size` to 1 or disable `use_mixed_precision`\n",
    "- **Slow Training**: Use multiple GPUs with DistributedDataParallel\n",
    "- **Model Download Failed**: Check internet connection and HuggingFace cache folder\n",
    "- **Low Inference Quality**: Increase `num_inference_steps` to 50-100 (slower but better quality)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
