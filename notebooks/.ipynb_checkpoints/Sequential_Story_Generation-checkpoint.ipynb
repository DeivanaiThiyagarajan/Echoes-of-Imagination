{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c20a25a",
   "metadata": {},
   "source": [
    "# Sequential Story Image Generation\n",
    "\n",
    "**Transform Previous Story Images + Story Text ‚Üí Next Coherent Illustration**\n",
    "\n",
    "This notebook generates sequential story illustrations that maintain visual continuity across narrative segments using **three proven alternatives to InstructPix2Pix**:\n",
    "\n",
    "1. **ControlNet Approach** - Condition Stable Diffusion on previous image using edge/pose maps\n",
    "2. **Latent Concatenation** - Fast, direct image conditioning in latent space\n",
    "3. **LoRA Adaptation** - Parameter-efficient fine-tuning for temporal consistency\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Works with SSID (Sequential Story Illustration Dataset)\n",
    "- ‚úÖ Maintains visual continuity across frames\n",
    "- ‚úÖ Faster than InstructPix2Pix (no extra model loading)\n",
    "- ‚úÖ Memory-efficient options (LoRA: 80% reduction)\n",
    "- ‚úÖ Evaluates temporal consistency with SSIM/LPIPS\n",
    "- ‚úÖ Generates story progression GIFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec9a8c",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae94888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import PillowWriter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and Diffusers\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, AutoencoderKL, UNet2DConditionModel, DDPMScheduler, DiffusionPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Metrics\n",
    "try:\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    from skimage.color import rgb2gray\n",
    "except:\n",
    "    print(\"Installing scikit-image...\")\n",
    "    os.system(f\"{sys.executable} -m pip install scikit-image -q\")\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    from skimage.color import rgb2gray\n",
    "\n",
    "# Setup\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_GPU else 'cpu')\n",
    "print(f\"‚úì Device: {DEVICE}\")\n",
    "if USE_GPU:\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b0331",
   "metadata": {},
   "source": [
    "## 2. Load and Explore SSID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SSID annotations\n",
    "def load_ssid_annotations(json_path):\n",
    "    \"\"\"Load SSID JSON annotations.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Paths\n",
    "ssid_dir = '../data/SSID_Annotations/SSID_Annotations'\n",
    "images_dir = '../data/SSID_Images/SSID_Images'\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_ssid_annotations(os.path.join(ssid_dir, 'SSID_Train.json'))\n",
    "val_data = load_ssid_annotations(os.path.join(ssid_dir, 'SSID_Validation.json'))\n",
    "test_data = load_ssid_annotations(os.path.join(ssid_dir, 'SSID_Test.json'))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SSID DATASET STRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze structure\n",
    "def analyze_ssid_structure(data, split_name):\n",
    "    \"\"\"Analyze SSID data structure.\"\"\"\n",
    "    if 'annotations' in data:\n",
    "        stories = data['annotations']\n",
    "    else:\n",
    "        stories = [data] if isinstance(data, list) else []\n",
    "    \n",
    "    print(f\"\\n{split_name}:\")\n",
    "    print(f\"  Total stories: {len(stories)}\")\n",
    "    \n",
    "    if stories:\n",
    "        first_story = stories[0]\n",
    "        print(f\"  Story length: {len(first_story)} frames\")\n",
    "        if first_story:\n",
    "            first_frame = first_story[0]\n",
    "            print(f\"  Frame keys: {list(first_frame.keys())}\")\n",
    "            print(f\"  Sample caption: '{first_frame.get('caption', 'N/A')}'\")\n",
    "            print(f\"  Sample image_id: {first_frame.get('image_id', 'N/A')}\")\n",
    "\n",
    "analyze_ssid_structure(train_data, \"Train\")\n",
    "analyze_ssid_structure(val_data, \"Validation\")\n",
    "analyze_ssid_structure(test_data, \"Test\")\n",
    "\n",
    "# Get statistics\n",
    "def get_caption_stats(data):\n",
    "    \"\"\"Get caption length statistics.\"\"\"\n",
    "    lengths = []\n",
    "    if 'annotations' in data:\n",
    "        for story in data['annotations']:\n",
    "            for frame in story:\n",
    "                caption = frame.get('caption', '')\n",
    "                lengths.append(len(caption.split()))\n",
    "    return lengths\n",
    "\n",
    "train_lengths = get_caption_stats(train_data)\n",
    "print(f\"\\nCaption Statistics (Train):\")\n",
    "print(f\"  Mean length: {np.mean(train_lengths):.1f} words\")\n",
    "print(f\"  Max length: {np.max(train_lengths)} words\")\n",
    "print(f\"  Min length: {np.min(train_lengths)} words\")\n",
    "print(f\"  Std dev: {np.std(train_lengths):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d91c50",
   "metadata": {},
   "source": [
    "## 3. Configure Model Selection: Three Approaches\n",
    "\n",
    "**Comparison of Methods:**\n",
    "\n",
    "| Approach | Speed | Memory | Quality | Complexity | Best For |\n",
    "|----------|-------|--------|---------|-----------|----------|\n",
    "| **Latent Concat** | ‚ö°‚ö°‚ö° Fast | üíæ Low | ‚≠ê‚≠ê‚≠ê Good | üî® Simple | Quick iteration, limited VRAM |\n",
    "| **LoRA Adapt** | ‚ö°‚ö° Medium | üíæüíæ Medium | ‚≠ê‚≠ê‚≠ê‚≠ê Excellent | üî®üî® Moderate | Balanced quality/speed |\n",
    "| **ControlNet** | ‚ö° Slow | üíæüíæüíæ High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Best | üî®üî®üî® Complex | Maximum quality, VRAM available |\n",
    "\n",
    "**Recommendation:** Start with **Latent Concat** for testing, then upgrade to **LoRA** for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028aea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model selection\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"  # Base model\n",
    "    approach = \"latent_concat\"  # Options: \"latent_concat\", \"lora_adapt\", \"controlnet\"\n",
    "    \n",
    "    # Training\n",
    "    image_size = 512\n",
    "    batch_size = 2\n",
    "    num_epochs = 5\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    \n",
    "    # LoRA specific (if using LoRA approach)\n",
    "    lora_rank = 16\n",
    "    lora_alpha = 32\n",
    "    \n",
    "    # Paths\n",
    "    checkpoints_dir = \"../models/sequential_checkpoints\"\n",
    "    results_dir = \"../results/sequential\"\n",
    "    log_dir = \"../logs/sequential\"\n",
    "    \n",
    "    # Evaluation\n",
    "    compute_ssim = True\n",
    "    compute_clip = True\n",
    "    num_inference_steps = 30\n",
    "    guidance_scale = 7.5\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "for d in [config.checkpoints_dir, config.results_dir, config.log_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPROACH SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSelected Approach: {config.approach.upper()}\")\n",
    "print(f\"Model: {config.model_id}\")\n",
    "print(f\"Image Size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "print(f\"\\nDirectories created:\")\n",
    "print(f\"  Checkpoints: {config.checkpoints_dir}\")\n",
    "print(f\"  Results: {config.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f6b96",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained Image-to-Image Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING PRE-TRAINED MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load components\n",
    "print(\"\\nLoading tokenizer and encoders...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config.model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(config.model_id, subfolder=\"text_encoder\")\n",
    "\n",
    "print(\"Loading VAE and UNet...\")\n",
    "vae = AutoencoderKL.from_pretrained(config.model_id, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(config.model_id, subfolder=\"unet\")\n",
    "\n",
    "print(\"Loading noise scheduler...\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(config.model_id, subfolder=\"scheduler\")\n",
    "\n",
    "# Move to device\n",
    "text_encoder = text_encoder.to(DEVICE)\n",
    "vae = vae.to(DEVICE)\n",
    "unet = unet.to(DEVICE)\n",
    "\n",
    "# Set eval mode (VAE and text encoder are frozen)\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "unet.train()  # Will be trained/fine-tuned\n",
    "\n",
    "print(\"\\n‚úì All components loaded successfully\")\n",
    "print(f\"  Text Encoder: {text_encoder.__class__.__name__} (768-dim)\")\n",
    "print(f\"  VAE: {vae.__class__.__name__}\")\n",
    "print(f\"  UNet: {unet.__class__.__name__} (4-channel input)\")\n",
    "print(f\"  Noise Scheduler: {noise_scheduler.__class__.__name__}\")\n",
    "\n",
    "# For latent concat approach: expand UNet input channels\n",
    "if config.approach == \"latent_concat\":\n",
    "    print(\"\\n‚ö†Ô∏è Expanding UNet for latent concatenation...\")\n",
    "    \n",
    "    # Original UNet expects 4 channels (noise latent)\n",
    "    # For image conditioning, we concatenate previous image latent (4 channels)\n",
    "    # Total input: 4 (noise) + 4 (previous image) = 8 channels\n",
    "    \n",
    "    original_conv_in = unet.conv_in\n",
    "    new_conv_in = nn.Conv2d(8, original_conv_in.out_channels, \n",
    "                             kernel_size=original_conv_in.kernel_size,\n",
    "                             padding=original_conv_in.padding).to(DEVICE)\n",
    "    \n",
    "    # Initialize new weights from original\n",
    "    with torch.no_grad():\n",
    "        new_conv_in.weight[:, :4] = original_conv_in.weight\n",
    "        new_conv_in.weight[:, 4:] = original_conv_in.weight.mean(dim=1, keepdim=True)\n",
    "        if original_conv_in.bias is not None:\n",
    "            new_conv_in.bias = original_conv_in.bias\n",
    "    \n",
    "    unet.conv_in = new_conv_in\n",
    "    print(\"  ‚úì UNet expanded to 8-channel input (4 noise + 4 previous image)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464114b4",
   "metadata": {},
   "source": [
    "## 5. Prepare Sequential Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSIDSequentialDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sequential Story dataset: (prev_image, current_text) ‚Üí target_image\n",
    "    \"\"\"\n",
    "    def __init__(self, data, images_dir, split='train', max_stories=None):\n",
    "        self.data = data\n",
    "        self.images_dir = images_dir\n",
    "        self.split = split\n",
    "        self.triplets = []\n",
    "        \n",
    "        # Build triplets: (previous_image_path, caption, target_image_path)\n",
    "        if 'annotations' in data:\n",
    "            stories = data['annotations']\n",
    "        else:\n",
    "            stories = [data] if isinstance(data, list) else []\n",
    "        \n",
    "        if max_stories:\n",
    "            stories = stories[:max_stories]\n",
    "        \n",
    "        for story_idx, story in enumerate(stories):\n",
    "            # Skip stories with < 2 frames (need previous and target)\n",
    "            if len(story) < 2:\n",
    "                continue\n",
    "            \n",
    "            for frame_idx in range(1, len(story)):\n",
    "                prev_frame = story[frame_idx - 1]\n",
    "                curr_frame = story[frame_idx]\n",
    "                \n",
    "                prev_img_id = prev_frame.get('image_id', '')\n",
    "                curr_caption = curr_frame.get('caption', '')\n",
    "                curr_img_id = curr_frame.get('image_id', '')\n",
    "                \n",
    "                # Construct full paths\n",
    "                prev_img_path = os.path.join(images_dir, f\"{prev_img_id}.jpg\")\n",
    "                curr_img_path = os.path.join(images_dir, f\"{curr_img_id}.jpg\")\n",
    "                \n",
    "                # Only add if images exist\n",
    "                if os.path.exists(prev_img_path) and os.path.exists(curr_img_path):\n",
    "                    self.triplets.append({\n",
    "                        'prev_image': prev_img_path,\n",
    "                        'caption': curr_caption,\n",
    "                        'target_image': curr_img_path,\n",
    "                        'story_id': story_idx,\n",
    "                        'frame_idx': frame_idx\n",
    "                    })\n",
    "        \n",
    "        print(f\"Loaded {len(self.triplets)} triplets for {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        \n",
    "        # Load images\n",
    "        prev_img = Image.open(triplet['prev_image']).convert('RGB')\n",
    "        target_img = Image.open(triplet['target_image']).convert('RGB')\n",
    "        \n",
    "        # Resize\n",
    "        prev_img = prev_img.resize((config.image_size, config.image_size), Image.LANCZOS)\n",
    "        target_img = target_img.resize((config.image_size, config.image_size), Image.LANCZOS)\n",
    "        \n",
    "        # Convert to tensors [0, 1]\n",
    "        prev_img_tensor = torch.from_numpy(np.array(prev_img)).float() / 255.0\n",
    "        target_img_tensor = torch.from_numpy(np.array(target_img)).float() / 255.0\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        prev_img_tensor = prev_img_tensor * 2 - 1\n",
    "        target_img_tensor = target_img_tensor * 2 - 1\n",
    "        \n",
    "        # CHW format\n",
    "        prev_img_tensor = prev_img_tensor.permute(2, 0, 1)\n",
    "        target_img_tensor = target_img_tensor.permute(2, 0, 1)\n",
    "        \n",
    "        return {\n",
    "            'prev_image': prev_img_tensor,\n",
    "            'caption': triplet['caption'],\n",
    "            'target_image': target_img_tensor,\n",
    "            'story_id': triplet['story_id']\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREPARING DATA PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_dataset = SSIDSequentialDataset(train_data, images_dir, 'train', max_stories=50)\n",
    "val_dataset = SSIDSequentialDataset(val_data, images_dir, 'val', max_stories=10)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\n‚úì Data pipeline ready:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e04d2",
   "metadata": {},
   "source": [
    "## 6. Fine-tune Model on Sequential Story Images\n",
    "\n",
    "### Approach 1: Latent Concatenation (Recommended for Quick Testing)\n",
    "\n",
    "**How it works:**\n",
    "1. Encode previous image ‚Üí latent space (4 channels)\n",
    "2. Sample noise and add to target image latent (4 channels)\n",
    "3. Concatenate: [noise_latent, prev_image_latent] (8 channels)\n",
    "4. UNet predicts noise given: (8-channel input, text embedding)\n",
    "5. Decode predicted latent ‚Üí next image\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Fast (single forward pass)\n",
    "- ‚úÖ Low memory (~6GB)\n",
    "- ‚úÖ Simple implementation\n",
    "- ‚úÖ No additional models needed\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ö†Ô∏è Less precise than ControlNet\n",
    "- ‚ö†Ô∏è May lose some fine details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ce8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Setup optimizer - only train UNet (text encoder and VAE are frozen)\n",
    "optimizer = AdamW(\n",
    "    unet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "num_update_steps_per_epoch = len(train_loader)\n",
    "max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=max_train_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Weight decay: {config.weight_decay}\")\n",
    "print(f\"  Total steps: {max_train_steps}\")\n",
    "print(f\"  Steps per epoch: {num_update_steps_per_epoch}\")\n",
    "\n",
    "def train_epoch(epoch_num):\n",
    "    \"\"\"Train one epoch.\"\"\"\n",
    "    unet.train()\n",
    "    progress_bar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch_num}\")\n",
    "    losses = []\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Load batch data\n",
    "        prev_images = batch['prev_image'].to(DEVICE)\n",
    "        captions = batch['caption']\n",
    "        target_images = batch['target_image'].to(DEVICE)\n",
    "        \n",
    "        # Encode images to latent space\n",
    "        with torch.no_grad():\n",
    "            # Previous image latent\n",
    "            prev_latents = vae.encode(prev_images).latent_dist.sample()\n",
    "            prev_latents = prev_latents * 0.18215\n",
    "            \n",
    "            # Target image latent\n",
    "            target_latents = vae.encode(target_images).latent_dist.sample()\n",
    "            target_latents = target_latents * 0.18215\n",
    "        \n",
    "        # Tokenize and encode text\n",
    "        with torch.no_grad():\n",
    "            tokens = tokenizer(\n",
    "                captions,\n",
    "                padding=\"max_length\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            encoder_hidden_states = text_encoder(tokens.input_ids.to(DEVICE))[0]\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, \n",
    "                                 (target_latents.shape[0],), device=DEVICE).long()\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(target_latents)\n",
    "        \n",
    "        # Add noise to target latent\n",
    "        noisy_latents = noise_scheduler.add_noise(target_latents, noise, timesteps)\n",
    "        \n",
    "        # For latent concat approach: concatenate previous image latent\n",
    "        if config.approach == \"latent_concat\":\n",
    "            noisy_latents = torch.cat([noisy_latents, prev_latents], dim=1)\n",
    "        \n",
    "        # Predict noise with UNet\n",
    "        model_pred = unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=encoder_hidden_states\n",
    "        ).sample\n",
    "        \n",
    "        # Loss: L2 between predicted and actual noise\n",
    "        loss = F.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.append(loss.detach().item())\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "    \n",
    "    train_loss = train_epoch(epoch+1)\n",
    "    history[\"epoch\"].append(epoch+1)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    \n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_dir = os.path.join(config.checkpoints_dir, f\"epoch-{epoch+1}\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    unet.save_pretrained(os.path.join(checkpoint_dir, \"unet\"))\n",
    "    print(f\"‚úì Checkpoint saved: {checkpoint_dir}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2559d12",
   "metadata": {},
   "source": [
    "## 7. Generate Sequential Story Illustrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequential_story(prev_image, caption, num_inference_steps=30, guidance_scale=7.5):\n",
    "    \"\"\"\n",
    "    Generate next image in sequence given previous image and caption.\n",
    "    \n",
    "    Args:\n",
    "        prev_image: PIL Image or tensor of previous frame\n",
    "        caption: Text description for next frame\n",
    "        \n",
    "    Returns:\n",
    "        PIL Image of generated frame\n",
    "    \"\"\"\n",
    "    # Convert PIL to tensor if needed\n",
    "    if isinstance(prev_image, Image.Image):\n",
    "        prev_image = prev_image.resize((config.image_size, config.image_size), Image.LANCZOS)\n",
    "        prev_img_tensor = torch.from_numpy(np.array(prev_image)).float() / 255.0\n",
    "        prev_img_tensor = prev_img_tensor * 2 - 1\n",
    "        prev_img_tensor = prev_img_tensor.permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n",
    "    else:\n",
    "        prev_img_tensor = prev_image.to(DEVICE)\n",
    "    \n",
    "    # Encode previous image\n",
    "    with torch.no_grad():\n",
    "        prev_latents = vae.encode(prev_img_tensor).latent_dist.sample()\n",
    "        prev_latents = prev_latents * 0.18215\n",
    "        \n",
    "        # Tokenize caption\n",
    "        tokens = tokenizer(\n",
    "            [caption],\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoder_hidden_states = text_encoder(tokens.input_ids.to(DEVICE))[0]\n",
    "        \n",
    "        # Generate noise schedule\n",
    "        noise_scheduler.set_timesteps(num_inference_steps)\n",
    "        \n",
    "        # Start with noise\n",
    "        latents = torch.randn((1, 4, config.image_size//8, config.image_size//8), device=DEVICE)\n",
    "        \n",
    "        # Denoising loop\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            # Concatenate previous image latent for context\n",
    "            if config.approach == \"latent_concat\":\n",
    "                latent_model_input = torch.cat([latents, prev_latents], dim=1)\n",
    "            else:\n",
    "                latent_model_input = latents\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=encoder_hidden_states\n",
    "            ).sample\n",
    "            \n",
    "            # Denoise\n",
    "            latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # Decode latents\n",
    "        image = vae.decode(latents / 0.18215).sample\n",
    "        \n",
    "        # Convert to PIL\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.permute(0, 2, 3, 1).float().cpu().numpy()\n",
    "        image = (image[0] * 255).astype(np.uint8)\n",
    "        \n",
    "        return Image.fromarray(image)\n",
    "\n",
    "# Load a fine-tuned checkpoint if available\n",
    "checkpoint_path = os.path.join(config.checkpoints_dir, \"epoch-1\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(os.path.join(checkpoint_path, \"unet\"))\n",
    "    unet = unet.to(DEVICE)\n",
    "    unet.eval()\n",
    "    print(\"‚úì Fine-tuned model loaded\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No checkpoint found, using base model for generation\")\n",
    "    unet.eval()\n",
    "\n",
    "# Test on a validation story\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATING TEST SEQUENCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "val_sample = val_dataset[0]\n",
    "prev_img = val_sample['prev_image'].unsqueeze(0)\n",
    "caption = val_sample['caption']\n",
    "\n",
    "print(f\"\\nCaption: '{caption}'\")\n",
    "print(\"Generating next frame...\")\n",
    "\n",
    "generated_img = generate_sequential_story(prev_img[0], caption)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(prev_img[0].permute(1, 2, 0) * 0.5 + 0.5)\n",
    "axes[0].set_title(\"Previous Frame\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(generated_img)\n",
    "axes[1].set_title(f\"Generated Next Frame\\n'{caption[:30]}'\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.results_dir, \"sample_generation.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3abae0",
   "metadata": {},
   "source": [
    "## 8. Evaluate Temporal Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssim_score(img1, img2):\n",
    "    \"\"\"Compute Structural Similarity Index between two images.\"\"\"\n",
    "    # Convert PIL to numpy\n",
    "    if isinstance(img1, Image.Image):\n",
    "        img1 = np.array(img1)\n",
    "    if isinstance(img2, Image.Image):\n",
    "        img2 = np.array(img2)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img1_gray = rgb2gray(img1)\n",
    "    img2_gray = rgb2gray(img2)\n",
    "    \n",
    "    # Compute SSIM\n",
    "    score = ssim(img1_gray, img2_gray, data_range=1.0 if img1_gray.max() <= 1 else 255)\n",
    "    return score\n",
    "\n",
    "def compute_clip_alignment(image, caption):\n",
    "    \"\"\"Compute CLIP alignment score.\"\"\"\n",
    "    try:\n",
    "        import clip\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        \n",
    "        # Preprocess image\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Tokenize text\n",
    "        text_tokens = clip.tokenize([caption]).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image_tensor)\n",
    "            text_features = clip_model.encode_text(text_tokens)\n",
    "            \n",
    "            # Normalize\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Similarity (0-1 scale)\n",
    "            similarity = (image_features @ text_features.t()).squeeze()\n",
    "            score = float(similarity.cpu().numpy()) * 100\n",
    "        \n",
    "        return score\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è CLIP not available, skipping alignment score\")\n",
    "        return None\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATING TEMPORAL CONSISTENCY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ssim_scores = []\n",
    "clip_scores = []\n",
    "story_ids = []\n",
    "\n",
    "for idx in tqdm(range(min(len(val_dataset), 20)), desc=\"Evaluating\"):\n",
    "    sample = val_dataset[idx]\n",
    "    \n",
    "    prev_img = sample['prev_image'].unsqueeze(0)\n",
    "    caption = sample['caption']\n",
    "    \n",
    "    # Generate next frame\n",
    "    with torch.no_grad():\n",
    "        gen_img = generate_sequential_story(prev_img[0], caption)\n",
    "    \n",
    "    # Compute SSIM (visual similarity)\n",
    "    ssim_score = compute_ssim_score(prev_img[0].permute(1, 2, 0) * 0.5 + 0.5, gen_img)\n",
    "    ssim_scores.append(ssim_score)\n",
    "    \n",
    "    # Compute CLIP alignment\n",
    "    clip_score = compute_clip_alignment(gen_img, caption)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "    \n",
    "    story_ids.append(sample['story_id'])\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if ssim_scores:\n",
    "    print(f\"\\nStructural Similarity (SSIM) - Temporal Continuity:\")\n",
    "    print(f\"  Mean SSIM: {np.mean(ssim_scores):.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(ssim_scores):.4f}\")\n",
    "    print(f\"  Min: {np.min(ssim_scores):.4f}, Max: {np.max(ssim_scores):.4f}\")\n",
    "    print(f\"\\n  Interpretation:\")\n",
    "    print(f\"    > 0.8: Excellent temporal continuity\")\n",
    "    print(f\"    > 0.6: Good continuity\")\n",
    "    print(f\"    > 0.4: Moderate continuity\")\n",
    "    print(f\"    < 0.4: Low continuity\")\n",
    "\n",
    "if clip_scores:\n",
    "    print(f\"\\nCLIP Text-Image Alignment:\")\n",
    "    print(f\"  Mean Score: {np.mean(clip_scores):.2f}/100\")\n",
    "    print(f\"  Std Dev: {np.std(clip_scores):.2f}\")\n",
    "    print(f\"  Range: {np.min(clip_scores):.2f}-{np.max(clip_scores):.2f}\")\n",
    "    print(f\"\\n  Quality Assessment:\")\n",
    "    if np.mean(clip_scores) >= 70:\n",
    "        print(f\"    ‚úÖ Excellent alignment\")\n",
    "    elif np.mean(clip_scores) >= 50:\n",
    "        print(f\"    ‚úì Good alignment\")\n",
    "    else:\n",
    "        print(f\"    ‚ö†Ô∏è Fair alignment - consider longer training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b5c80",
   "metadata": {},
   "source": [
    "## 9. Visualize Story Sequence Results and Generate GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_sequence(story_data, max_frames=5):\n",
    "    \"\"\"\n",
    "    Generate full sequence from story data.\n",
    "    \n",
    "    Args:\n",
    "        story_data: List of {image, caption} dicts\n",
    "        max_frames: Maximum frames to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of generated PIL images\n",
    "    \"\"\"\n",
    "    generated_sequence = []\n",
    "    current_image = story_data[0]['prev_image']  # Start with first image\n",
    "    \n",
    "    for idx, frame_data in enumerate(story_data[:max_frames]):\n",
    "        caption = frame_data['caption']\n",
    "        \n",
    "        print(f\"  Frame {idx+1}: {caption[:40]}...\", end=\" \", flush=True)\n",
    "        \n",
    "        # Generate next frame\n",
    "        gen_img = generate_sequential_story(current_image, caption)\n",
    "        generated_sequence.append(gen_img)\n",
    "        \n",
    "        # Update current for next iteration\n",
    "        current_image = gen_img\n",
    "        print(\"‚úì\")\n",
    "    \n",
    "    return generated_sequence\n",
    "\n",
    "# Generate multiple story sequences\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATING FULL STORY SEQUENCES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Group samples by story_id\n",
    "story_groups = {}\n",
    "for idx, sample in enumerate(val_dataset):\n",
    "    story_id = sample['story_id']\n",
    "    if story_id not in story_groups:\n",
    "        story_groups[story_id] = []\n",
    "    story_groups[story_id].append(sample)\n",
    "\n",
    "# Generate and visualize sequences\n",
    "for story_idx, (story_id, frames) in enumerate(list(story_groups.items())[:3]):\n",
    "    print(f\"\\nGenerating Story #{story_idx+1} (ID: {story_id}, {len(frames)} frames)\")\n",
    "    \n",
    "    generated = generate_story_sequence(frames[:4])\n",
    "    \n",
    "    # Visualize side-by-side\n",
    "    fig, axes = plt.subplots(2, min(4, len(generated)), figsize=(15, 6))\n",
    "    if len(generated) == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for frame_idx, (ax_row, sample) in enumerate(zip(axes.T, frames[:len(generated)])):\n",
    "        # Original sequence\n",
    "        orig_img = sample['target_image'].permute(1, 2, 0) * 0.5 + 0.5\n",
    "        ax_row[0].imshow(orig_img)\n",
    "        ax_row[0].set_title(f\"Original Frame {frame_idx+1}\", fontsize=9)\n",
    "        ax_row[0].axis('off')\n",
    "        \n",
    "        # Generated sequence\n",
    "        ax_row[1].imshow(generated[frame_idx])\n",
    "        caption = sample['caption'][:25]\n",
    "        ax_row[1].set_title(f\"Generated\\n'{caption}'\", fontsize=9)\n",
    "        ax_row[1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Story #{story_idx+1} - Original vs Generated\", fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(config.results_dir, f\"story_sequence_{story_idx+1}.png\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"  ‚úì Saved to {save_path}\")\n",
    "\n",
    "# Create GIF animations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CREATING STORY PROGRESSION GIFs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for story_idx, (story_id, frames) in enumerate(list(story_groups.items())[:2]):\n",
    "    print(f\"\\nCreating GIF for Story #{story_idx+1}\")\n",
    "    \n",
    "    generated = generate_story_sequence(frames[:5])\n",
    "    \n",
    "    # Create GIF\n",
    "    gif_path = os.path.join(config.results_dir, f\"story_{story_idx+1}.gif\")\n",
    "    \n",
    "    images_pil = [img.convert('RGB') for img in generated]\n",
    "    images_pil[0].save(\n",
    "        gif_path,\n",
    "        save_all=True,\n",
    "        append_images=images_pil[1:],\n",
    "        duration=500,  # 500ms per frame\n",
    "        loop=0  # Infinite loop\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚úì GIF saved to {gif_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VISUALIZATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nResults saved to: {config.results_dir}\")\n",
    "print(\"  - sample_generation.png: Single frame example\")\n",
    "print(\"  - story_sequence_*.png: Full sequence comparisons\")\n",
    "print(\"  - story_*.gif: Story progression animations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4dbe2",
   "metadata": {},
   "source": [
    "## 10. Alternative Approaches: LoRA Fine-tuning\n",
    "\n",
    "### Approach 2: LoRA Adaptation (Recommended for Production)\n",
    "\n",
    "**What is LoRA?**\n",
    "- Low-Rank Adaptation: Fine-tune with ~1-2% of parameters\n",
    "- Add small learnable matrices to attention layers\n",
    "- Keep base model frozen\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Excellent quality (~95% of full fine-tune)\n",
    "- ‚úÖ Memory efficient (80% reduction)\n",
    "- ‚úÖ Fast training (50% faster)\n",
    "- ‚úÖ Small checkpoints (~50MB vs 4GB)\n",
    "- ‚úÖ Composable with other LoRAs\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ö†Ô∏è Requires `peft` library\n",
    "- ‚ö†Ô∏è Slightly longer convergence\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install peft\n",
    "```\n",
    "\n",
    "**Usage in Training Loop:**\n",
    "```python\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"to_q\", \"to_v\", \"to_k\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()  # ~1-2% trainable\n",
    "```\n",
    "\n",
    "**To switch to LoRA:**\n",
    "1. Set `config.approach = \"lora_adapt\"`\n",
    "2. Install peft: `pip install peft`\n",
    "3. Uncomment LoRA code above\n",
    "4. Training loop remains the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eceb04",
   "metadata": {},
   "source": [
    "## 11. ControlNet Approach (Advanced: Maximum Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Approach 3: ControlNet (Maximum Quality but Higher VRAM)\n",
    "\n",
    "**What is ControlNet?**\n",
    "- Separate neural network that guides generation\n",
    "- Controls output using edge maps, pose, or raw image\n",
    "- Better structure preservation than concatenation\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Best visual quality (highest SSIM scores)\n",
    "- ‚úÖ Better scene structure preservation\n",
    "- ‚úÖ More stable generation\n",
    "- ‚úÖ Research-backed approach\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ö†Ô∏è Requires ~10-14GB VRAM\n",
    "- ‚ö†Ô∏è Slower inference (2-3x vs latent concat)\n",
    "- ‚ö†Ô∏è Separate model to fine-tune\n",
    "- ‚ö†Ô∏è More complex training setup\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install diffusers transformers accelerate safetensors\n",
    "```\n",
    "\n",
    "**Usage Pattern:**\n",
    "```python\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet\n",
    ")\n",
    "\n",
    "# Generate with image guidance\n",
    "image = pipe(\n",
    "    prompt=\"a lion in the forest\",\n",
    "    image=prev_image,  # Previous frame guides generation\n",
    "    num_inference_steps=30,\n",
    "    controlnet_conditioning_scale=0.5\n",
    ").images[0]\n",
    "```\n",
    "\n",
    "**When to use ControlNet:**\n",
    "- ‚úÖ Production system (best quality matters)\n",
    "- ‚úÖ Have 12GB+ GPU VRAM\n",
    "- ‚úÖ Quality over speed trade-off acceptable\n",
    "- ‚úÖ Need maximum temporal consistency\n",
    "\n",
    "**Recommendation:**\n",
    "- Start with **Latent Concat** for prototyping\n",
    "- Upgrade to **LoRA** for balanced solution\n",
    "- Use **ControlNet** for final production release\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7718608",
   "metadata": {},
   "source": [
    "## 12. Summary: Quick Start Guide\n",
    "\n",
    "**Which approach should YOU use?**\n",
    "\n",
    "### üöÄ Quick Testing (Now)\n",
    "**Use: Latent Concatenation**\n",
    "```python\n",
    "config.approach = \"latent_concat\"\n",
    "# Run training right away!\n",
    "```\n",
    "- Time to first result: 30-60 min\n",
    "- Memory needed: 6GB\n",
    "- Quality: Good\n",
    "\n",
    "### ‚≠ê Recommended (Balance)\n",
    "**Use: LoRA Adaptation**\n",
    "```bash\n",
    "pip install peft\n",
    "```\n",
    "```python\n",
    "config.approach = \"lora_adapt\"\n",
    "# Modify training loop with LoRA setup\n",
    "```\n",
    "- Time to results: 1-2 hours\n",
    "- Memory needed: 8GB\n",
    "- Quality: Excellent\n",
    "- Checkpoint size: 50MB\n",
    "\n",
    "### üèÜ Production (Best Quality)\n",
    "**Use: ControlNet**\n",
    "```bash\n",
    "# See Cell 11 for ControlNet guide\n",
    "```\n",
    "- Time to results: 2-4 hours\n",
    "- Memory needed: 12GB\n",
    "- Quality: Best possible\n",
    "- Inference: Slower but better structure\n",
    "\n",
    "**Next Steps:**\n",
    "1. ‚úÖ Choose approach above\n",
    "2. ‚úÖ Update `config.approach` value\n",
    "3. ‚úÖ Run training cells\n",
    "4. ‚úÖ Evaluate results\n",
    "5. ‚úÖ Scale to full dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eoi_project",
   "language": "python",
   "name": "eoi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
