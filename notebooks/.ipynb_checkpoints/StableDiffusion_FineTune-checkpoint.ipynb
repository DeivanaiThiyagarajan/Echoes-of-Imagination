{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179ce376",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0216544",
   "metadata": {},
   "source": [
    "# Fine-tune Stable Diffusion on Custom Dataset\n",
    "\n",
    "This notebook trains (fine-tunes) a Stable Diffusion model on your COCO + Flickr image-caption dataset to generate custom images specific to your domain.\n",
    "\n",
    "**Training Options:**\n",
    "- Full fine-tuning (all parameters)\n",
    "- LoRA fine-tuning (parameter-efficient, recommended for limited GPU)\n",
    "- Text encoder fine-tuning\n",
    "- UNet fine-tuning\n",
    "\n",
    "**Hardware Support:**\n",
    "- Works on CPU (slow, for debugging)\n",
    "- Single GPU (recommended minimum: 8GB VRAM)\n",
    "- Multi-GPU with DistributedDataParallel\n",
    "- HiperGator multi-node setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d692838",
   "metadata": {},
   "source": [
    "# Fine-tune Stable Diffusion on Custom Dataset\n",
    "\n",
    "This notebook trains (fine-tunes) a Stable Diffusion model on your COCO + Flickr image-caption dataset to generate custom images specific to your domain.\n",
    "\n",
    "**Training Options:**\n",
    "- Full fine-tuning (all parameters)\n",
    "- LoRA fine-tuning (parameter-efficient, recommended for limited GPU)\n",
    "- Text encoder fine-tuning\n",
    "- UNet fine-tuning\n",
    "\n",
    "**Hardware Support:**\n",
    "- Works on CPU (slow, for debugging)\n",
    "- Single GPU (recommended minimum: 8GB VRAM)\n",
    "- Multi-GPU with DistributedDataParallel\n",
    "- HiperGator multi-node setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7d456",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245ec3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Hugging Face libraries\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, AutoTokenizer, BertTokenizer, BertModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline, LDMTextToImagePipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Image processing\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"âœ“ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31dc81c",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hardware Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3faffe83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA B200\n",
      "\n",
      "Training Configuration:\n",
      "  Model ID: runwayml/stable-diffusion-v1-5\n",
      "  Image Size: 128x128\n",
      "  Batch Size: 8\n",
      "  Epochs: 10\n",
      "  Learning Rate: 0.0001\n",
      "  Fine-tune Text Encoder: True\n",
      "  Use LoRA: False\n",
      "  Mixed Precision: True\n"
     ]
    }
   ],
   "source": [
    "# ============ CONFIGURATION ============\n",
    "class Config:\n",
    "    # Model Configuration\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    image_size = 128  # Resolution for fine-tuning\n",
    "    \n",
    "    # Training Configuration\n",
    "    num_epochs = 10\n",
    "    train_batch_size = 8  # Adjust based on GPU memory: 2-4 for 8GB, 8-16 for 24GB\n",
    "    gradient_accumulation_steps = 2  # Simulates larger batches\n",
    "    learning_rate = 1e-4\n",
    "    max_grad_norm = 1.0\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # Warmup and scheduling\n",
    "    lr_warmup_steps = 500\n",
    "    num_train_timesteps = 1000\n",
    "    \n",
    "    # Fine-tuning options\n",
    "    fine_tune_text_encoder = True  # Fine-tune text encoder alongside UNet\n",
    "    use_lora = False  # Set True for parameter-efficient LoRA fine-tuning\n",
    "    lora_rank = 16  # LoRA matrix rank\n",
    "    \n",
    "    # Mixed precision\n",
    "    use_mixed_precision = torch.cuda.is_available()\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoints_dir = \"../models/checkpoints\"\n",
    "    save_interval = 2  # Save checkpoint every N epochs\n",
    "    \n",
    "    # Validation\n",
    "    num_validation_images = 4\n",
    "    validation_interval = 2\n",
    "    validation_prompts = [\n",
    "        \"a photo of a cat\",\n",
    "        \"a beautiful landscape with mountains\",\n",
    "        \"a person smiling at the camera\",\n",
    "        \"a delicious pizza on a plate\"\n",
    "    ]\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "\n",
    "# Hardware setup\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_GPU else 'cpu')\n",
    "LOCAL_RANK = int(os.environ.get('LOCAL_RANK', 0))\n",
    "WORLD_SIZE = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Number of GPUs: {NUM_GPUS}\")\n",
    "if USE_GPU:\n",
    "    for i in range(NUM_GPUS):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Adjust batch size for multi-GPU\n",
    "if NUM_GPUS > 1:\n",
    "    config.train_batch_size = max(1, config.train_batch_size // NUM_GPUS)\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Model ID: {config.model_id}\")\n",
    "print(f\"  Image Size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"  Batch Size: {config.train_batch_size}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  Fine-tune Text Encoder: {config.fine_tune_text_encoder}\")\n",
    "print(f\"  Use LoRA: {config.use_lora}\")\n",
    "print(f\"  Mixed Precision: {config.use_mixed_precision}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config.checkpoints_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab90c959",
   "metadata": {},
   "source": [
    "## 3. Load Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "732c5a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset using caption_dataset.get_dataloader()...\n",
      "  unique_image_identifier                                            caption  \\\n",
      "0          1000092795.jpg   Two young guys with shaggy hair look at their...   \n",
      "1          1000092795.jpg   Two young , White males are outside near many...   \n",
      "2          1000092795.jpg   Two men in green shirts are standing in a yard .   \n",
      "3          1000092795.jpg       A man in a blue shirt standing in a garden .   \n",
      "4          1000092795.jpg            Two friends enjoy time spent together .   \n",
      "\n",
      "                                          source_dir  source  \n",
      "0  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "1  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "2  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "3  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "4  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "  unique_image_identifier                                            caption  \\\n",
      "0          1000092795.jpg   Two young guys with shaggy hair look at their...   \n",
      "1          1000092795.jpg   Two young , White males are outside near many...   \n",
      "2          1000092795.jpg   Two men in green shirts are standing in a yard .   \n",
      "3          1000092795.jpg       A man in a blue shirt standing in a garden .   \n",
      "4          1000092795.jpg            Two friends enjoy time spent together .   \n",
      "\n",
      "                                          source_dir  source  \n",
      "0  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "1  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "2  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "3  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "4  /blue/eee6778/share/dthiyagarajan/extracted_fi...  Flickr  \n",
      "\n",
      "âœ“ Dataloaders loaded successfully!\n",
      "  Train batches per epoch: 59805\n",
      "  Val batches per epoch: 7476\n",
      "\n",
      "Testing data format from dataloader...\n",
      "  Image shape: torch.Size([8, 3, 64, 64])\n",
      "  Text: ['A group of people hold up Frisbees for the camera.', 'A kitchen with a counter, sink and pantry.', 'A man surfing on a white surf board.', 'two people walking on a street talking on a cell phone', ' A man is standing on a green pasture with a cane watching something afar .', 'Hot dogs lined up in a pan on the table. ', 'An egg McMuffin in placed on a small plate.', 'A jet airliner raises its wheels after takeoff.  ']\n"
     ]
    }
   ],
   "source": [
    "# Setup path to access custom modules\n",
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))\n",
    "\n",
    "from dataloaders_text import caption_dataset\n",
    "\n",
    "# Initialize dataloader handler\n",
    "print(\"Loading dataset using caption_dataset.get_dataloader()...\")\n",
    "dataloader_handler = caption_dataset()\n",
    "\n",
    "# Get dataloaders directly - much simpler!\n",
    "train_dataloader, train_sampler = dataloader_handler.get_dataloader(\n",
    "    partition=\"train\", \n",
    "    batch_size=config.train_batch_size,\n",
    "    distributed=False\n",
    ")\n",
    "\n",
    "val_dataloader, val_sampler = dataloader_handler.get_dataloader(\n",
    "    partition=\"val\", \n",
    "    batch_size=config.train_batch_size,\n",
    "    distributed=False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Dataloaders loaded successfully!\")\n",
    "print(f\"  Train batches per epoch: {len(train_dataloader)}\")\n",
    "print(f\"  Val batches per epoch: {len(val_dataloader)}\")\n",
    "\n",
    "# Test a batch to see data format\n",
    "print(\"\\nTesting data format from dataloader...\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"  Image shape: {sample_batch[0].shape}\")  # Should be [B, C, H, W]\n",
    "print(f\"  Text: {sample_batch[1]}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20e8d8",
   "metadata": {},
   "source": [
    "## 4. Load Pretrained Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b834d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pretrained model: runwayml/stable-diffusion-v1-5\n",
      "âœ“ Model components loaded\n",
      "  Text Encoder: CLIPTextModel\n",
      "  VAE: AutoencoderKL\n",
      "  UNet: UNet2DConditionModel\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading pretrained model: {config.model_id}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config.model_id, subfolder=\"tokenizer\")\n",
    "\n",
    "# Load text encoder\n",
    "text_encoder = CLIPTextModel.from_pretrained(config.model_id, subfolder=\"text_encoder\")\n",
    "\n",
    "# Load VAE (for encoding images to latents)\n",
    "vae = AutoencoderKL.from_pretrained(config.model_id, subfolder=\"vae\")\n",
    "\n",
    "# Load UNet for diffusion\n",
    "unet = UNet2DConditionModel.from_pretrained(config.model_id, subfolder=\"unet\")\n",
    "\n",
    "# Load noise scheduler\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(config.model_id, subfolder=\"scheduler\")\n",
    "\n",
    "# Move to device\n",
    "text_encoder = text_encoder.to(DEVICE)\n",
    "vae = vae.to(DEVICE)\n",
    "unet = unet.to(DEVICE)\n",
    "#noise_scheduler = noise_scheduler.to(DEVICE)\n",
    "\n",
    "# Set to eval mode (we'll only fine-tune specific components)\n",
    "vae.eval()\n",
    "text_encoder.eval() if not config.fine_tune_text_encoder else text_encoder.train()\n",
    "unet.train()\n",
    "\n",
    "print(\"âœ“ Model components loaded\")\n",
    "print(f\"  Text Encoder: {text_encoder.__class__.__name__}\")\n",
    "print(f\"  VAE: {vae.__class__.__name__}\")\n",
    "print(f\"  UNet: {unet.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954749b",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73bf292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrapping dataloader for fine-tuning...\n",
      "âœ“ Dataloader wrapped successfully\n",
      "  Total batches: 59805\n"
     ]
    }
   ],
   "source": [
    "# Note: Dataloaders are already prepared above!\n",
    "# The dataloader returns: (image, caption) where caption is a string\n",
    "\n",
    "# For fine-tuning with diffusion, we need to:\n",
    "# 1. Encode images to latent space\n",
    "# 2. Tokenize captions and get text embeddings with proper shape [batch, seq_length, embedding_dim]\n",
    "\n",
    "# Create a wrapper to pre-encode images and tokenize captions\n",
    "class FineTuningWrapper:\n",
    "    def __init__(self, dataloader, vae, tokenizer, text_encoder, device, config):\n",
    "        \"\"\"\n",
    "        Wraps dataloader to pre-encode images to latents and tokenize captions.\n",
    "        \"\"\"\n",
    "        self.dataloader = dataloader\n",
    "        self.vae = vae\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Pre-encode each batch's images to latents and tokenize captions.\"\"\"\n",
    "        for batch_idx, (images, captions) in enumerate(self.dataloader):\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # Pre-encode images to latent space\n",
    "            with torch.no_grad():\n",
    "                # Resize if needed (dataloader uses 64x64, we might need 512x512)\n",
    "                if images.shape[-1] != self.config.image_size:\n",
    "                    images = F.interpolate(\n",
    "                        images, \n",
    "                        size=(self.config.image_size, self.config.image_size), \n",
    "                        mode='bilinear',\n",
    "                        align_corners=False\n",
    "                    )\n",
    "                \n",
    "                # Encode to latents\n",
    "                latents = self.vae.encode(images).latent_dist.sample()\n",
    "                latents = latents * 0.18215  # Stable Diffusion scaling\n",
    "                \n",
    "                # Tokenize captions (convert to list if necessary)\n",
    "                if isinstance(captions, torch.Tensor):\n",
    "                    # If captions are already tokens, convert to input_ids format\n",
    "                    input_ids = captions.to(self.device)\n",
    "                else:\n",
    "                    # If captions are strings, tokenize them\n",
    "                    if not isinstance(captions, (list, tuple)):\n",
    "                        captions = [captions]  # Handle single caption\n",
    "                    \n",
    "                    tokens = self.tokenizer(\n",
    "                        list(captions),\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=self.tokenizer.model_max_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    input_ids = tokens.input_ids.to(self.device)\n",
    "                \n",
    "                # Get text embeddings with proper shape [batch, seq_length, embedding_dim]\n",
    "                encoder_hidden_states = self.text_encoder(input_ids)[0]  # Shape: [batch, seq_length, 768]\n",
    "            \n",
    "            yield {\n",
    "                \"latent\": latents,\n",
    "                \"encoder_hidden_states\": encoder_hidden_states\n",
    "            }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "\n",
    "# Wrap the dataloader\n",
    "print(\"\\nWrapping dataloader for fine-tuning...\")\n",
    "wrapped_train_dataloader = FineTuningWrapper(\n",
    "    train_dataloader, \n",
    "    vae, \n",
    "    tokenizer, \n",
    "    text_encoder,\n",
    "    DEVICE,\n",
    "    config\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Dataloader wrapped successfully\")\n",
    "print(f\"  Total batches: {len(wrapped_train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587ca76",
   "metadata": {},
   "source": [
    "## 6. Setup Training: Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8601ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Optimizer and scheduler configured\n",
      "  Trainable parameters: 982,581,444\n",
      "  Total training steps: 299030\n",
      "  Learning rate: 0.0001\n",
      "  Mixed precision: True\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer - only optimize trainable parameters\n",
    "trainable_params = list(unet.parameters())\n",
    "if config.fine_tune_text_encoder:\n",
    "    trainable_params += list(text_encoder.parameters())\n",
    "\n",
    "optimizer = AdamW(\n",
    "    trainable_params,\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    eps=1e-08\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / config.gradient_accumulation_steps\n",
    ")\n",
    "max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=max_train_steps\n",
    ")\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = GradScaler() if config.use_mixed_precision else None\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer and scheduler configured\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in trainable_params if p.requires_grad):,}\")\n",
    "print(f\"  Total training steps: {max_train_steps}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Mixed precision: {config.use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde6866",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb864fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Fine-tuning...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/59805 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def train_epoch(epoch_num, wrapped_dataloader, optimizer, lr_scheduler, scaler):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    progress_bar = tqdm(total=len(wrapped_dataloader), desc=f\"Epoch {epoch_num}\")\n",
    "    losses = []\n",
    "    \n",
    "    for step, batch in enumerate(wrapped_dataloader):\n",
    "        # Get batch data\n",
    "        latents = batch[\"latent\"]  # Already on device, shape: [B, 4, H, W]\n",
    "        encoder_hidden_states = batch[\"encoder_hidden_states\"]  # Already on device, shape: [B, seq_len, 768]\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        timesteps = torch.randint(\n",
    "            0, config.num_train_timesteps, (latents.shape[0],), device=DEVICE\n",
    "        ).long()\n",
    "        \n",
    "        # Sample noise to add to latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        \n",
    "        # Add noise to latents (forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Predict noise residual with text conditioning\n",
    "        if config.use_mixed_precision:\n",
    "            with autocast():\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate loss (L2 loss between predicted and actual noise)\n",
    "                loss = F.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "        else:\n",
    "            model_pred = unet(\n",
    "                noisy_latents,\n",
    "                timesteps,\n",
    "                encoder_hidden_states\n",
    "            ).sample\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "        \n",
    "        # Backward pass with gradient accumulation\n",
    "        if config.use_mixed_precision:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        losses.append(loss.detach().item())\n",
    "        \n",
    "        # Update weights\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            if config.use_mixed_precision:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, config.max_grad_norm)\n",
    "            \n",
    "            if config.use_mixed_precision:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Fine-tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = {\"train_loss\": [], \"epoch\": []}\n",
    "\n",
    "try:\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        avg_loss = train_epoch(epoch+1, wrapped_train_dataloader, optimizer, lr_scheduler, scaler)\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"epoch\"].append(epoch+1)\n",
    "        \n",
    "        print(f\"Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % config.save_interval == 0:\n",
    "            checkpoint_path = os.path.join(\n",
    "                config.checkpoints_dir,\n",
    "                f\"checkpoint-epoch-{epoch+1}\"\n",
    "            )\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "            \n",
    "            unet.save_pretrained(os.path.join(checkpoint_path, \"unet\"))\n",
    "            if config.fine_tune_text_encoder:\n",
    "                text_encoder.save_pretrained(os.path.join(checkpoint_path, \"text_encoder\"))\n",
    "            \n",
    "            print(f\"âœ“ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c0127",
   "metadata": {},
   "source": [
    "## 8. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b026cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "if history[\"train_loss\"]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], marker='o', linewidth=2)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Average Loss\", fontsize=12)\n",
    "    plt.title(\"Training Loss Over Epochs\", fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../results/training_loss.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Training loss plot saved\")\n",
    "else:\n",
    "    print(\"No training history available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088267a2",
   "metadata": {},
   "source": [
    "## 9. Load Fine-tuned Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with fine-tuned model\n",
    "checkpoint_dir = os.path.join(config.checkpoints_dir, f\"checkpoint-epoch-{config.num_epochs}\")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"Loading fine-tuned model from: {checkpoint_dir}\")\n",
    "    \n",
    "    # Load fine-tuned components\n",
    "    unet_finetuned = UNet2DConditionModel.from_pretrained(\n",
    "        os.path.join(checkpoint_dir, \"unet\")\n",
    "    )\n",
    "    \n",
    "    text_encoder_finetuned = CLIPTextModel.from_pretrained(\n",
    "        os.path.join(checkpoint_dir, \"text_encoder\")\n",
    "    ) if config.fine_tune_text_encoder else CLIPTextModel.from_pretrained(\n",
    "        config.model_id, subfolder=\"text_encoder\"\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with fine-tuned components\n",
    "    pipe_finetuned = StableDiffusionPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        unet=unet_finetuned,\n",
    "        text_encoder=text_encoder_finetuned,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"âœ“ Fine-tuned pipeline created successfully\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "    print(\"Using original pretrained model for inference\")\n",
    "    pipe_finetuned = StableDiffusionPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd2e64",
   "metadata": {},
   "source": [
    "## 10. Test Fine-tuned Model with Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b301c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images using fine-tuned model\n",
    "test_prompts = [\n",
    "    \"a photo of a cat sleeping\",\n",
    "    \"a beautiful sunset over mountains\",\n",
    "    \"a person laughing at camera\",\n",
    "    \"delicious food on a plate\"\n",
    "]\n",
    "\n",
    "print(\"Generating images with fine-tuned model...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, prompt in enumerate(test_prompts):\n",
    "    print(f\"Generating: '{prompt}'\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = pipe_finetuned(\n",
    "            prompt,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            num_images_per_prompt=1\n",
    "        ).images[0]\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f\"'{prompt}'\", fontsize=10, wrap=True)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/finetuned_generation_samples.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Sample generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16180249",
   "metadata": {},
   "source": [
    "## 11. Multi-GPU Training and HiperGator Deployment\n",
    "\n",
    "### Running on Multi-GPU Locally:\n",
    "```bash\n",
    "# Single node, 2 GPUs\n",
    "torchrun --nproc_per_node=2 fine_tune_distributed.py\n",
    "```\n",
    "\n",
    "### HiperGator SBATCH Configuration:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --gpus-per-node=a100:2\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=96GB\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --job-name=sd-finetune\n",
    "\n",
    "module load python cuda/11.8\n",
    "source venv/bin/activate\n",
    "\n",
    "torchrun --nproc_per_node=2 --nnodes=2 \\\n",
    "         --master_addr=$MASTER_ADDR --master_port=29500 \\\n",
    "         fine_tune_distributed.py\n",
    "```\n",
    "\n",
    "### Key Considerations:\n",
    "- **Memory**: A100 GPUs (40GB) can handle batch_size=8-16\n",
    "- **Mixed Precision**: Enabled by default (reduces memory ~2x)\n",
    "- **Gradient Accumulation**: Simulates larger batches\n",
    "- **Data Loading**: Pre-encoded latents reduce I/O overhead\n",
    "- **Checkpointing**: Save every N epochs to resume if interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3df1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images using fine-tuned model\n",
    "test_prompts = [\n",
    "    \"a photo of a cat sleeping\",\n",
    "    \"a beautiful sunset over mountains\",\n",
    "    \"a person laughing at camera\",\n",
    "    \"delicious food on a plate\"\n",
    "]\n",
    "\n",
    "print(\"Generating images with fine-tuned model...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, prompt in enumerate(test_prompts):\n",
    "    print(f\"Generating: '{prompt}'\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = pipe_finetuned(\n",
    "            prompt,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            num_images_per_prompt=1\n",
    "        ).images[0]\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f\"'{prompt}'\", fontsize=10, wrap=True)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/finetuned_generation_samples.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Sample generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdaea0",
   "metadata": {},
   "source": [
    "## 12. LoRA Fine-tuning (Parameter-Efficient Alternative)\n",
    "\n",
    "LoRA (Low-Rank Adaptation) fine-tunes only small matrices added to the model, reducing:\n",
    "- **Memory**: ~80% reduction\n",
    "- **Storage**: Checkpoint size ~50MB instead of 4GB\n",
    "- **Time**: Faster convergence\n",
    "\n",
    "Uncomment the code below to use LoRA instead of full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6909954-c0ff-4962-af9a-56fc5c522e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To use LoRA fine-tuning, install: pip install peft\n",
    "\n",
    "Then replace the model loading with:\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"to_q\", \"to_v\"],  # Target modules in attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "# Now only LoRA parameters will be trained (~1% of model)\n",
    "# This significantly reduces memory and training time\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fd6f3-87ef-4856-a837-fdbc8b309508",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "### What This Notebook Does:\n",
    "âœ… **Fine-tunes Stable Diffusion** on your COCO + Flickr dataset  \n",
    "âœ… **Trains UNet** for custom image generation  \n",
    "âœ… **Optionally fine-tunes text encoder** for better text understanding  \n",
    "âœ… **Uses pre-encoded latents** for efficient training  \n",
    "âœ… **Implements mixed precision** for reduced memory usage  \n",
    "âœ… **Saves checkpoints** for resuming training  \n",
    "âœ… **Generates test images** to verify quality  \n",
    "\n",
    "### Key Features:\n",
    "- **Memory Efficient**: Pre-encoding images, gradient accumulation, mixed precision\n",
    "- **Flexible**: Fine-tune UNet only or + text encoder\n",
    "- **Scalable**: Works on single GPU â†’ multi-GPU â†’ HiperGator\n",
    "- **Lightweight**: LoRA option available (~80% memory reduction)\n",
    "\n",
    "### Hyperparameters to Adjust:\n",
    "- `train_batch_size`: Increase if GPU has memory (8-16 for A100)\n",
    "- `learning_rate`: 1e-4 is good starting point\n",
    "- `num_epochs`: 10-20 typically sufficient for fine-tuning\n",
    "- `image_size`: 512x512 standard, can use 768x768 on A100\n",
    "- `num_inference_steps`: 30-50 for inference quality\n",
    "\n",
    "### Next Steps:\n",
    "1. Start with small batch size and few epochs to test\n",
    "2. Monitor loss curve for overfitting\n",
    "3. Save best checkpoint (lowest loss)\n",
    "4. Evaluate with CLIP scores or manual inspection\n",
    "5. Deploy to HiperGator for larger-scale training\n",
    "6. Consider LoRA for parameter-efficient fine-tuning\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Out of memory**: Reduce batch size or enable gradient checkpointing\n",
    "- **Slow training**: Check if data loading is bottleneck\n",
    "- **Loss not decreasing**: Check learning rate, data quality\n",
    "- **Bad quality images**: Train longer, increase inference steps\n",
    "\n",
    "Good luck! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a217352-030d-4755-b3ca-ef7861cb301e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eoi_project",
   "language": "python",
   "name": "eoi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
