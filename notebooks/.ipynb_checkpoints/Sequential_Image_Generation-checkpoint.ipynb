{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa95b45d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and Vision models\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModel, CLIPProcessor\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Check GPU\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0' if USE_GPU else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPU Available: {USE_GPU}\")\n",
    "if USE_GPU:\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360d780",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "**Model Architecture**: Lightweight Sequential Generator\n",
    "- Text encoder: CLIP (frozen)\n",
    "- Image encoder: Lightweight ResNet50 (frozen)\n",
    "- Generator: 4-layer CNN with residual blocks\n",
    "- Output: 256×256 RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model architecture\n",
    "    image_size: int = 256\n",
    "    text_embed_dim: int = 768  # CLIP embedding dimension\n",
    "    image_embed_dim: int = 2048  # ResNet50 output dimension\n",
    "    latent_dim: int = 512  # Hidden dimension for generator\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 8\n",
    "    num_epochs: int = 30\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    \n",
    "    # Data\n",
    "    num_workers: int = 4\n",
    "    shuffle: bool = True\n",
    "    \n",
    "    # Checkpoints\n",
    "    checkpoints_dir: str = \"./models/sequential_checkpoints\"\n",
    "    results_dir: str = \"./results/sequential\"\n",
    "    log_dir: str = \"./logs/sequential\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Image size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31154d5",
   "metadata": {},
   "source": [
    "## 3. Load SSID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SSID annotations\n",
    "def load_annotations(json_path, split_name):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    flat_data = [storylet for story in data['annotations'] for storylet in story]\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    df['split'] = split_name\n",
    "    return df\n",
    "\n",
    "# Paths\n",
    "annotations_dir = '../data/SSID_Annotations/SSID_Annotations'\n",
    "images_dir = '../data/SSID_Images/SSID_Images'\n",
    "\n",
    "train_json = os.path.join(annotations_dir, \"SSID_Train.json\")\n",
    "val_json = os.path.join(annotations_dir, \"SSID_Validation.json\")\n",
    "test_json = os.path.join(annotations_dir, \"SSID_Test.json\")\n",
    "\n",
    "# Load all splits\n",
    "df_train = load_annotations(train_json, 'train')\n",
    "df_val = load_annotations(val_json, 'val')\n",
    "df_test = load_annotations(test_json, 'test')\n",
    "\n",
    "df_all = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "print(f\"Train storylets: {len(df_train)}\")\n",
    "print(f\"Validation storylets: {len(df_val)}\")\n",
    "print(f\"Test storylets: {len(df_test)}\")\n",
    "print(f\"Total storylets: {len(df_all)}\")\n",
    "print(f\"Unique stories: {df_all['story_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587626e",
   "metadata": {},
   "source": [
    "## 4. Create Sequential Pairs Dataset\n",
    "Build pairs of (previous_image, text, next_image) for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbd25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_pairs(df, images_dir, split='train'):\n",
    "    \"\"\"\n",
    "    Create (prev_image, text, next_image) pairs from stories.\n",
    "    Only use stories with 2+ images.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for story_id in df['story_id'].unique():\n",
    "        story_data = df[df['story_id'] == story_id].sort_values('image_order').reset_index(drop=True)\n",
    "        \n",
    "        # Skip stories with only 1 image\n",
    "        if len(story_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Create pairs: (image_t, text_t+1, image_t+1)\n",
    "        for i in range(len(story_data) - 1):\n",
    "            prev_row = story_data.iloc[i]\n",
    "            next_row = story_data.iloc[i + 1]\n",
    "            \n",
    "            prev_img_path = os.path.join(images_dir, f\"{prev_row['youtube_image_id']}.jpg\")\n",
    "            next_img_path = os.path.join(images_dir, f\"{next_row['youtube_image_id']}.jpg\")\n",
    "            \n",
    "            # Only add if both images exist\n",
    "            if os.path.exists(prev_img_path) and os.path.exists(next_img_path):\n",
    "                pairs.append({\n",
    "                    'prev_image': prev_img_path,\n",
    "                    'text': next_row['storytext'],\n",
    "                    'next_image': next_img_path,\n",
    "                    'story_id': story_id,\n",
    "                    'split': split\n",
    "                })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create pairs for each split\n",
    "train_pairs = create_sequential_pairs(df_train, images_dir, 'train')\n",
    "val_pairs = create_sequential_pairs(df_val, images_dir, 'val')\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "print(f\"Validation pairs: {len(val_pairs)}\")\n",
    "print(f\"\\nExample pair:\")\n",
    "if train_pairs:\n",
    "    pair = train_pairs[0]\n",
    "    print(f\"  Previous image: {os.path.basename(pair['prev_image'])}\")\n",
    "    print(f\"  Text prompt: {pair['text'][:60]}...\")\n",
    "    print(f\"  Next image: {os.path.basename(pair['next_image'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e2739",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8769da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for sequential image generation:\n",
    "    Input: (previous_image, text_prompt)\n",
    "    Output: next_image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pairs, tokenizer, image_size=256):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.CenterCrop((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Load and transform images\n",
    "        try:\n",
    "            prev_img = Image.open(pair['prev_image']).convert('RGB')\n",
    "            next_img = Image.open(pair['next_image']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images: {e}\")\n",
    "            # Return black images as fallback\n",
    "            prev_img = Image.new('RGB', (self.image_size, self.image_size))\n",
    "            next_img = Image.new('RGB', (self.image_size, self.image_size))\n",
    "        \n",
    "        prev_img_tensor = self.image_transform(prev_img)\n",
    "        next_img_tensor = self.image_transform(next_img)\n",
    "        \n",
    "        # Tokenize text\n",
    "        text = pair['text']\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'prev_image': prev_img_tensor,\n",
    "            'text_input_ids': tokens['input_ids'].squeeze(),\n",
    "            'text_attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'next_image': next_img_tensor,\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SequentialImageDataset(train_pairs, tokenizer, config.image_size)\n",
    "val_dataset = SequentialImageDataset(val_pairs, tokenizer, config.image_size)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "\n",
    "print(f\"Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Validation loader batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc50d0",
   "metadata": {},
   "source": [
    "## 6. Lightweight Sequential Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Lightweight residual block for efficient image generation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class SequentialImageGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight sequential image generator.\n",
    "    Takes previous image + text embedding as input, generates next image.\n",
    "    Architecture:\n",
    "    - Previous image: 3 channels → 64 channels\n",
    "    - Text embedding: 768 dim → spatial features (8x8x128)\n",
    "    - Fusion + Residual blocks + Upsampling\n",
    "    - Output: 256x256 RGB image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_embed_dim=768, latent_dim=512, image_size=256):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # ========== Image Encoder (lightweight) ==========\n",
    "        # Process previous image: 3 → 64 channels\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 7, stride=2, padding=3),  # 256 → 128\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 128 → 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # ========== Text Projection ==========\n",
    "        # Project text embedding to spatial features: 768 → (8x8x128)\n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(text_embed_dim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 8 * 8 * 128)  # Spatial reshape\n",
    "        )\n",
    "        \n",
    "        # ========== Fusion Module ==========\n",
    "        # Fuse image features (64x64x64) and text features (8x8x128)\n",
    "        # Upsample text to 64x64 and concatenate\n",
    "        self.text_upsample = nn.Sequential(\n",
    "            nn.Upsample(size=(64, 64), mode='nearest'),\n",
    "            nn.Conv2d(128, 64, 1)\n",
    "        )\n",
    "        \n",
    "        # Fused features: 64 (image) + 64 (text) = 128\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(128, latent_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # ========== Generator (residual + upsampling) ==========\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            ResidualBlock(latent_dim, latent_dim),\n",
    "            ResidualBlock(latent_dim, latent_dim),\n",
    "            ResidualBlock(latent_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Upsample to 256x256\n",
    "        self.upsampler = nn.Sequential(\n",
    "            # 64x64 → 128x128\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(latent_dim, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 128x128 → 256x256\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Final layer\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Tanh()  # Output [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, prev_image, text_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_image: (B, 3, 256, 256) - normalized to [-1, 1] or [0, 1]\n",
    "            text_embedding: (B, 768) - CLIP text embedding\n",
    "        Returns:\n",
    "            generated_image: (B, 3, 256, 256) - normalized to [-1, 1]\n",
    "        \"\"\"\n",
    "        # Encode previous image\n",
    "        img_features = self.image_encoder(prev_image)  # (B, 64, 64, 64)\n",
    "        \n",
    "        # Project text embedding to spatial features\n",
    "        text_spatial = self.text_projection(text_embedding)  # (B, 8*8*128)\n",
    "        text_spatial = text_spatial.view(-1, 128, 8, 8)  # (B, 128, 8, 8)\n",
    "        \n",
    "        # Upsample text features to match image features\n",
    "        text_features = self.text_upsample(text_spatial)  # (B, 64, 64, 64)\n",
    "        \n",
    "        # Fuse image and text features\n",
    "        fused = torch.cat([img_features, text_features], dim=1)  # (B, 128, 64, 64)\n",
    "        fused = self.fusion(fused)  # (B, 512, 64, 64)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        features = self.residual_blocks(fused)  # (B, 512, 64, 64)\n",
    "        \n",
    "        # Upsample to final resolution\n",
    "        output = self.upsampler(features)  # (B, 3, 256, 256)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = SequentialImageGenerator(\n",
    "    text_embed_dim=config.text_embed_dim,\n",
    "    latent_dim=config.latent_dim,\n",
    "    image_size=config.image_size\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Parameters: {total_params / 1e6:.2f}M\")\n",
    "print(f\"Model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ee39c",
   "metadata": {},
   "source": [
    "## 7. Load Encoders (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP text encoder (frozen for embeddings)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "text_encoder.eval()\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"✓ CLIP text encoder loaded (frozen)\")\n",
    "print(f\"  Text embedding dimension: {config.text_embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bf37b",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Combination of L1 and Perceptual loss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # L1: Encourages pixel-level accuracy\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        \n",
    "        # MSE: Smooth pixel differences\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        \n",
    "        # Combined: 0.7 * L1 + 0.3 * MSE\n",
    "        return 0.7 * l1 + 0.3 * mse\n",
    "\n",
    "criterion = CombinedLoss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "print(f\"Loss Function: Combined L1 + MSE\")\n",
    "print(f\"Optimizer: Adam (lr={config.learning_rate}, weight_decay={config.weight_decay})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9c81a",
   "metadata": {},
   "source": [
    "## 9. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0977f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, text_encoder, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", ncols=80)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        try:\n",
    "            # Load batch\n",
    "            prev_images = batch['prev_image'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            next_images = batch['next_image'].to(device)\n",
    "            \n",
    "            # Get text embeddings from CLIP (no grad)\n",
    "            with torch.no_grad():\n",
    "                text_embeddings = text_encoder(\n",
    "                    input_ids=text_input_ids,\n",
    "                    attention_mask=text_attention_mask\n",
    "                ).last_hidden_state[:, 0]  # Use [CLS] token embedding\n",
    "            \n",
    "            # Forward pass\n",
    "            generated_images = model(prev_images, text_embeddings)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(generated_images, next_images)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item():.4f})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, text_encoder, val_loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=\"Validating\", ncols=80)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                # Load batch\n",
    "                prev_images = batch['prev_image'].to(device)\n",
    "                text_input_ids = batch['text_input_ids'].to(device)\n",
    "                text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "                next_images = batch['next_image'].to(device)\n",
    "                \n",
    "                # Get text embeddings\n",
    "                text_embeddings = text_encoder(\n",
    "                    input_ids=text_input_ids,\n",
    "                    attention_mask=text_attention_mask\n",
    "                ).last_hidden_state[:, 0]\n",
    "                \n",
    "                # Forward pass\n",
    "                generated_images = model(prev_images, text_embeddings)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(generated_images, next_images)\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": loss.item():.4f})\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5c995",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e37137",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "print(f\"Device: {DEVICE}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, text_encoder, train_loader, optimizer, criterion, DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, text_encoder, val_loader, criterion, DEVICE)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_path = os.path.join(config.checkpoints_dir, \"best_model.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"✓ Best model saved (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(config.checkpoints_dir, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"✓ Checkpoint saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7ec87",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d240f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(train_losses, label='Train Loss', marker='o', linewidth=2, markersize=6)\n",
    "ax.plot(val_losses, label='Val Loss', marker='s', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Sequential Image Generator - Training History', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.results_dir, 'training_loss.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss plot saved to: {os.path.join(config.results_dir, 'training_loss.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b129b",
   "metadata": {},
   "source": [
    "## 12. Load Best Model and Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc1c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = os.path.join(config.checkpoints_dir, \"best_model.pt\")\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    print(f\"✓ Best model loaded\")\n",
    "else:\n",
    "    print(f\"No best model found at {best_model_path}\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35223f95",
   "metadata": {},
   "source": [
    "## 13. Test Inference on Validation Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06230835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    \"\"\"Convert from [-1, 1] or [0, 1] to PIL Image\"\"\"\n",
    "    # If tanh output [-1, 1]\n",
    "    if tensor.min() < 0:\n",
    "        tensor = (tensor + 1) / 2\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    tensor = tensor * 255\n",
    "    return tensor.byte()\n",
    "\n",
    "\n",
    "def generate_next_image(prev_image_path, text, model, text_encoder, device, image_size=256):\n",
    "    \"\"\"\n",
    "    Generate next image given previous image and text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(prev_image_path).convert('RGB')\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.CenterCrop((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    img_tensor = img_transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    text_input_ids = tokens['input_ids'].to(device)\n",
    "    text_attention_mask = tokens['attention_mask'].to(device)\n",
    "    \n",
    "    # Get text embedding\n",
    "    with torch.no_grad():\n",
    "        text_embedding = text_encoder(\n",
    "            input_ids=text_input_ids,\n",
    "            attention_mask=text_attention_mask\n",
    "        ).last_hidden_state[:, 0]\n",
    "        \n",
    "        # Generate\n",
    "        generated = model(img_tensor, text_embedding)\n",
    "    \n",
    "    # Convert to PIL\n",
    "    generated = generated.squeeze(0).cpu()\n",
    "    generated = denormalize(generated)\n",
    "    generated = transforms.ToPILImage()(generated)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "# Test on a sample from validation set\n",
    "print(f\"Testing inference on {min(3, len(val_pairs))} validation pairs...\\n\")\n",
    "\n",
    "for idx in range(min(3, len(val_pairs))):\n",
    "    pair = val_pairs[idx]\n",
    "    \n",
    "    print(f\"\\nPair {idx + 1}:\")\n",
    "    print(f\"  Text: {pair['text'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        generated_img = generate_next_image(\n",
    "            pair['prev_image'],\n",
    "            pair['text'],\n",
    "            model,\n",
    "            text_encoder,\n",
    "            DEVICE,\n",
    "            config.image_size\n",
    "        )\n",
    "        \n",
    "        # Load ground truth\n",
    "        ground_truth = Image.open(pair['next_image']).convert('RGB')\n",
    "        ground_truth = ground_truth.resize((config.image_size, config.image_size))\n",
    "        \n",
    "        # Display\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Previous image\n",
    "        prev_img = Image.open(pair['prev_image']).convert('RGB')\n",
    "        prev_img = prev_img.resize((config.image_size, config.image_size))\n",
    "        axes[0].imshow(prev_img)\n",
    "        axes[0].set_title('Previous Image', fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Generated\n",
    "        axes[1].imshow(generated_img)\n",
    "        axes[1].set_title('Generated Next Image', fontsize=10, color='green')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[2].imshow(ground_truth)\n",
    "        axes[2].set_title('Ground Truth Next Image', fontsize=10, color='blue')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(config.results_dir, f'inference_sample_{idx + 1}.png'),\n",
    "            dpi=100,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        print(f\"  ✓ Sample saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "\n",
    "print(f\"\\nInference samples saved to: {config.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d700e",
   "metadata": {},
   "source": [
    "## 14. Generate Full Story Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac75f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_sequence(story_id, df, model, text_encoder, device, images_dir, image_size=256):\n",
    "    \"\"\"\n",
    "    Generate a full sequence of images for a story.\n",
    "    Autoregressively: use generated image as input for next generation.\n",
    "    \"\"\"\n",
    "    story_data = df[df['story_id'] == story_id].sort_values('image_order')\n",
    "    generated_sequence = []\n",
    "    \n",
    "    if len(story_data) < 2:\n",
    "        print(f\"Story {story_id} has less than 2 images\")\n",
    "        return None\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # First image (use real)\n",
    "    first_img_path = os.path.join(images_dir, f\"{story_data.iloc[0]['youtube_image_id']}.jpg\")\n",
    "    first_img = Image.open(first_img_path).convert('RGB')\n",
    "    first_img = first_img.resize((image_size, image_size))\n",
    "    generated_sequence.append(first_img)\n",
    "    \n",
    "    current_img_path = first_img_path\n",
    "    \n",
    "    # Generate remaining images\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, len(story_data)):\n",
    "            text = story_data.iloc[i]['storytext']\n",
    "            \n",
    "            try:\n",
    "                generated_img = generate_next_image(\n",
    "                    current_img_path,\n",
    "                    text,\n",
    "                    model,\n",
    "                    text_encoder,\n",
    "                    device,\n",
    "                    image_size\n",
    "                )\n",
    "                generated_sequence.append(generated_img)\n",
    "                \n",
    "                # Save temporary for next iteration\n",
    "                temp_path = os.path.join(config.results_dir, 'temp_gen.jpg')\n",
    "                generated_img.save(temp_path)\n",
    "                current_img_path = temp_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating image {i}: {e}\")\n",
    "                break\n",
    "    \n",
    "    return generated_sequence, story_data\n",
    "\n",
    "\n",
    "# Generate a story sequence\n",
    "if len(val_pairs) > 0:\n",
    "    # Get a story_id from validation pairs\n",
    "    story_id = val_pairs[0]['story_id']\n",
    "    \n",
    "    print(f\"Generating story sequence for story_id: {story_id}\\n\")\n",
    "    \n",
    "    result = generate_story_sequence(\n",
    "        story_id,\n",
    "        df_val,\n",
    "        model,\n",
    "        text_encoder,\n",
    "        DEVICE,\n",
    "        images_dir,\n",
    "        config.image_size\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        generated_sequence, story_data = result\n",
    "        \n",
    "        # Display sequence\n",
    "        num_images = len(generated_sequence)\n",
    "        fig, axes = plt.subplots(1, num_images, figsize=(4*num_images, 4))\n",
    "        \n",
    "        if num_images == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, img in enumerate(generated_sequence):\n",
    "            axes[idx].imshow(img)\n",
    "            text = story_data.iloc[idx]['storytext'][:20] + \"...\"\n",
    "            axes[idx].set_title(f\"Image {idx + 1}\", fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(config.results_dir, 'story_sequence.png'),\n",
    "            dpi=100,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nGenerated {num_images} images for story\")\n",
    "        print(\"Full Story:\")\n",
    "        for idx, row in story_data.iterrows():\n",
    "            print(f\"{row['image_order']}. {row['storytext']}\")\n",
    "        \n",
    "        print(f\"\\nSequence saved to: {os.path.join(config.results_dir, 'story_sequence.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631149c",
   "metadata": {},
   "source": [
    "## 15. Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SEQUENTIAL IMAGE GENERATION - TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel Architecture: Lightweight Sequential Generator\")\n",
    "print(f\"  Total Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training pairs: {len(train_pairs)}\")\n",
    "print(f\"  Validation pairs: {len(val_pairs)}\")\n",
    "print(f\"  Unique stories (train): {df_train['story_id'].nunique()}\")\n",
    "print(f\"  Unique stories (val): {df_val['story_id'].nunique()}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Image size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Initial train loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Best validation loss: {min(val_losses):.4f}\")\n",
    "print(f\"  Loss reduction: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%\")\n",
    "print(f\"\\nCheckpoints saved in: {config.checkpoints_dir}\")\n",
    "print(f\"Results saved in: {config.results_dir}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"  ✓ Lightweight model (~50-100M parameters)\")\n",
    "print(\"  ✓ Efficient training on moderate GPUs\")\n",
    "print(\"  ✓ Takes previous image + text as input\")\n",
    "print(\"  ✓ Generates contextually coherent next images\")\n",
    "print(\"  ✓ Autoregressive inference for full story generation\")\n",
    "print(\"  ✓ Quality images at 256x256 resolution\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
