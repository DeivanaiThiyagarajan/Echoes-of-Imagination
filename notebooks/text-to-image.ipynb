{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":6019472,"sourceType":"datasetVersion","datasetId":3445072}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport torch\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torchvision.ops as ops\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\n\n# Importing machine learning utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom einops import rearrange, repeat\n\n# Importing libraries for medical image handling and dataset setup\nimport SimpleITK as sitk\nimport os\nimport json\nimport ast\nimport gc\nimport shutil\nimport glob\nimport sys\nimport random\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:35:41.849167Z","iopub.execute_input":"2025-11-05T00:35:41.849428Z","iopub.status.idle":"2025-11-05T00:35:51.695356Z","shell.execute_reply.started":"2025-11-05T00:35:41.849404Z","shell.execute_reply":"2025-11-05T00:35:51.694760Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install transformers==4.44.2 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:35:51.696650Z","iopub.execute_input":"2025-11-05T00:35:51.697162Z","iopub.status.idle":"2025-11-05T00:36:06.084049Z","shell.execute_reply.started":"2025-11-05T00:35:51.697140Z","shell.execute_reply":"2025-11-05T00:36:06.083264Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.44.2)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.44.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.19.1 transformers-4.44.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import transformers, huggingface_hub\nprint(transformers.__version__, huggingface_hub.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:06.085071Z","iopub.execute_input":"2025-11-05T00:36:06.085293Z","iopub.status.idle":"2025-11-05T00:36:06.444631Z","shell.execute_reply.started":"2025-11-05T00:36:06.085272Z","shell.execute_reply":"2025-11-05T00:36:06.443782Z"}},"outputs":[{"name":"stdout","text":"4.44.2 0.36.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"os.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:06.445555Z","iopub.execute_input":"2025-11-05T00:36:06.445849Z","iopub.status.idle":"2025-11-05T00:36:06.449941Z","shell.execute_reply.started":"2025-11-05T00:36:06.445830Z","shell.execute_reply":"2025-11-05T00:36:06.449046Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import RobertaModel, RobertaTokenizer, BertTokenizer\nfrom transformers import BertModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:06.452377Z","iopub.execute_input":"2025-11-05T00:36:06.452689Z","iopub.status.idle":"2025-11-05T00:36:07.416420Z","shell.execute_reply.started":"2025-11-05T00:36:06.452671Z","shell.execute_reply":"2025-11-05T00:36:07.415733Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n\ncaptions_2014_path = \"/kaggle/input/coco-image-caption/annotations_trainval2014/annotations/captions_train2014.json\"\ncaptions_2017_path = \"/kaggle/input/coco-image-caption/annotations_trainval2017/annotations/captions_val2017.json\"\n\n# Load JSON\nwith open(captions_2014_path, \"r\") as f:\n    captions_2014 = json.load(f)\n\nwith open(captions_2017_path, \"r\") as f:\n    captions_2017 = json.load(f)\n\n# Convert to DataFrames\ndf_captions_2014 = pd.DataFrame(captions_2014[\"annotations\"])\ndf_captions_2017 = pd.DataFrame(captions_2017[\"annotations\"])\n\nprint(\"Train 2014 captions:\", df_captions_2014.shape)\nprint(\"Val 2017 captions:\", df_captions_2017.shape)\nprint(\"\\nSample caption:\\n\", df_captions_2017['caption'].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:07.417188Z","iopub.execute_input":"2025-11-05T00:36:07.417746Z","iopub.status.idle":"2025-11-05T00:36:09.159578Z","shell.execute_reply.started":"2025-11-05T00:36:07.417725Z","shell.execute_reply":"2025-11-05T00:36:09.158814Z"}},"outputs":[{"name":"stdout","text":"Train 2014 captions: (414113, 3)\nVal 2017 captions: (25014, 3)\n\nSample caption:\n A black Honda motorcycle parked in front of a garage.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"image_dir_2014 = \"/kaggle/input/coco-image-caption/train2014/train2014\"\nimage_dir_2017 = \"/kaggle/input/coco-image-caption/val2017/val2017\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.160404Z","iopub.execute_input":"2025-11-05T00:36:09.160909Z","iopub.status.idle":"2025-11-05T00:36:09.164248Z","shell.execute_reply.started":"2025-11-05T00:36:09.160884Z","shell.execute_reply":"2025-11-05T00:36:09.163563Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df_captions_2014['source_dir'] = image_dir_2014\ndf_captions_2017['source_dir'] = image_dir_2017","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.164919Z","iopub.execute_input":"2025-11-05T00:36:09.165152Z","iopub.status.idle":"2025-11-05T00:36:09.177928Z","shell.execute_reply.started":"2025-11-05T00:36:09.165134Z","shell.execute_reply":"2025-11-05T00:36:09.177203Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df_captions_2014['image_id'] = (\n    'COCO_train2014_' + df_captions_2014['image_id'].astype(str).str.zfill(12)\n)\n\ndf_captions_2017['image_id'] = (\n    df_captions_2017['image_id'].astype(str).str.zfill(12)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.178816Z","iopub.execute_input":"2025-11-05T00:36:09.179066Z","iopub.status.idle":"2025-11-05T00:36:09.450718Z","shell.execute_reply.started":"2025-11-05T00:36:09.179048Z","shell.execute_reply":"2025-11-05T00:36:09.450086Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.451492Z","iopub.execute_input":"2025-11-05T00:36:09.451687Z","iopub.status.idle":"2025-11-05T00:36:09.455539Z","shell.execute_reply.started":"2025-11-05T00:36:09.451672Z","shell.execute_reply":"2025-11-05T00:36:09.454676Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df_captions_2014['source_dir']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.456244Z","iopub.execute_input":"2025-11-05T00:36:09.456451Z","iopub.status.idle":"2025-11-05T00:36:09.470372Z","shell.execute_reply.started":"2025-11-05T00:36:09.456431Z","shell.execute_reply":"2025-11-05T00:36:09.469528Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0         /kaggle/input/coco-image-caption/train2014/train2014\n1         /kaggle/input/coco-image-caption/train2014/train2014\n2         /kaggle/input/coco-image-caption/train2014/train2014\n3         /kaggle/input/coco-image-caption/train2014/train2014\n4         /kaggle/input/coco-image-caption/train2014/train2014\n                                  ...                         \n414108    /kaggle/input/coco-image-caption/train2014/train2014\n414109    /kaggle/input/coco-image-caption/train2014/train2014\n414110    /kaggle/input/coco-image-caption/train2014/train2014\n414111    /kaggle/input/coco-image-caption/train2014/train2014\n414112    /kaggle/input/coco-image-caption/train2014/train2014\nName: source_dir, Length: 414113, dtype: object"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df_coco_unified = pd.concat([df_captions_2014, df_captions_2017], ignore_index=True)\ndf_coco_unified = df_coco_unified[['image_id', 'caption','source_dir']]\ndf_coco_unified['source'] = 'COCO'\n\n# Rename 'image_id' for consistency if you plan to combine with Flickr, \n# although image access will differ (COCO uses 'image_id' to format the filename).\ndf_coco_unified.rename(columns={'image_id': 'unique_image_identifier'}, inplace=True)\ndf_coco_unified['unique_image_identifier'] = df_coco_unified['unique_image_identifier'].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.471231Z","iopub.execute_input":"2025-11-05T00:36:09.471509Z","iopub.status.idle":"2025-11-05T00:36:09.551499Z","shell.execute_reply.started":"2025-11-05T00:36:09.471484Z","shell.execute_reply":"2025-11-05T00:36:09.550901Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df_coco_unified.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.552178Z","iopub.execute_input":"2025-11-05T00:36:09.552384Z","iopub.status.idle":"2025-11-05T00:36:09.578459Z","shell.execute_reply.started":"2025-11-05T00:36:09.552368Z","shell.execute_reply":"2025-11-05T00:36:09.577790Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"       unique_image_identifier  \\\n0  COCO_train2014_000000318556   \n1  COCO_train2014_000000116100   \n2  COCO_train2014_000000318556   \n3  COCO_train2014_000000116100   \n4  COCO_train2014_000000379340   \n\n                                                       caption  \\\n0               A very clean and well decorated empty bathroom   \n1     A panoramic view of a kitchen and all of its appliances.   \n2  A blue and white bathroom with butterfly themed wall tiles.   \n3               A panoramic photo of a kitchen and dining room   \n4    A graffiti-ed stop sign across the street from a red car    \n\n                                             source_dir source  \n0  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n1  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n2  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n3  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n4  /kaggle/input/coco-image-caption/train2014/train2014   COCO  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_image_identifier</th>\n      <th>caption</th>\n      <th>source_dir</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>COCO_train2014_000000318556</td>\n      <td>A very clean and well decorated empty bathroom</td>\n      <td>/kaggle/input/coco-image-caption/train2014/train2014</td>\n      <td>COCO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>COCO_train2014_000000116100</td>\n      <td>A panoramic view of a kitchen and all of its appliances.</td>\n      <td>/kaggle/input/coco-image-caption/train2014/train2014</td>\n      <td>COCO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>COCO_train2014_000000318556</td>\n      <td>A blue and white bathroom with butterfly themed wall tiles.</td>\n      <td>/kaggle/input/coco-image-caption/train2014/train2014</td>\n      <td>COCO</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>COCO_train2014_000000116100</td>\n      <td>A panoramic photo of a kitchen and dining room</td>\n      <td>/kaggle/input/coco-image-caption/train2014/train2014</td>\n      <td>COCO</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>COCO_train2014_000000379340</td>\n      <td>A graffiti-ed stop sign across the street from a red car</td>\n      <td>/kaggle/input/coco-image-caption/train2014/train2014</td>\n      <td>COCO</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"flickr_annotations_path = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\nflickr_image_path = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.581623Z","iopub.execute_input":"2025-11-05T00:36:09.581995Z","iopub.status.idle":"2025-11-05T00:36:09.585370Z","shell.execute_reply.started":"2025-11-05T00:36:09.581978Z","shell.execute_reply":"2025-11-05T00:36:09.584692Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df_flickr = pd.read_csv(flickr_annotations_path, delimiter = '|')\ndf_flickr['source_dir'] = flickr_image_path\ndf_flickr.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.586016Z","iopub.execute_input":"2025-11-05T00:36:09.586232Z","iopub.status.idle":"2025-11-05T00:36:09.943772Z","shell.execute_reply.started":"2025-11-05T00:36:09.586214Z","shell.execute_reply":"2025-11-05T00:36:09.943089Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Index(['image_name', ' comment_number', ' comment', 'source_dir'], dtype='object')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"\ntry:\n    # Use the column names and delimiter identified in our previous steps\n    df_flickr = pd.read_csv(\n        flickr_annotations_path,\n        delimiter='|',\n        names=['image_name', 'comment_number', 'caption'],\n        header=None,\n        encoding='utf-8',\n        skiprows=1 \n    )\n    df_flickr['source_dir'] = flickr_image_path\n    # Flickr image names are already unique strings (e.g., '1000092795.jpg')\n    df_flickr = df_flickr[['image_name', 'caption','source_dir']]\n    df_flickr.rename(columns={'image_name': 'unique_image_identifier'}, inplace=True)\n    df_flickr['source'] = 'Flickr'\nexcept Exception as e:\n    print(f\"Error loading Flickr CSV: {e}. Using a small dummy set for Flickr.\")\n    df_flickr = pd.DataFrame({\n        'unique_image_identifier': ['dummy1.jpg', 'dummy2.jpg'],\n        'caption': ['A placeholder image.', 'Another example sentence.'],\n        'source': 'Flickr'\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:09.944411Z","iopub.execute_input":"2025-11-05T00:36:09.944617Z","iopub.status.idle":"2025-11-05T00:36:10.192058Z","shell.execute_reply.started":"2025-11-05T00:36:09.944601Z","shell.execute_reply":"2025-11-05T00:36:10.191352Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df_combined = pd.concat([df_coco_unified, df_flickr], ignore_index=True)\n\nprint(\"Combined DataFrame Shape:\", df_combined.shape)\nprint(\"Combined Data Sources:\\n\", df_combined['source'].value_counts())\nprint(\"Combined DataFrame Head:\")\nprint(df_combined[df_combined['source_dir'].str.contains('2014', na=False)].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:10.192957Z","iopub.execute_input":"2025-11-05T00:36:10.193573Z","iopub.status.idle":"2025-11-05T00:36:10.460036Z","shell.execute_reply.started":"2025-11-05T00:36:10.193543Z","shell.execute_reply":"2025-11-05T00:36:10.459384Z"}},"outputs":[{"name":"stdout","text":"Combined DataFrame Shape: (598042, 4)\nCombined Data Sources:\n source\nCOCO      439127\nFlickr    158915\nName: count, dtype: int64\nCombined DataFrame Head:\n       unique_image_identifier  \\\n0  COCO_train2014_000000318556   \n1  COCO_train2014_000000116100   \n2  COCO_train2014_000000318556   \n3  COCO_train2014_000000116100   \n4  COCO_train2014_000000379340   \n\n                                                       caption  \\\n0               A very clean and well decorated empty bathroom   \n1     A panoramic view of a kitchen and all of its appliances.   \n2  A blue and white bathroom with butterfly themed wall tiles.   \n3               A panoramic photo of a kitchen and dining room   \n4    A graffiti-ed stop sign across the street from a red car    \n\n                                             source_dir source  \n0  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n1  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n2  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n3  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n4  /kaggle/input/coco-image-caption/train2014/train2014   COCO  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 80% train, 10% val, 10% test\ntrain_df, temp_df = train_test_split(df_combined, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\nprint(len(train_df), len(val_df), len(test_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:10.460710Z","iopub.execute_input":"2025-11-05T00:36:10.460965Z","iopub.status.idle":"2025-11-05T00:36:10.684308Z","shell.execute_reply.started":"2025-11-05T00:36:10.460940Z","shell.execute_reply":"2025-11-05T00:36:10.683671Z"}},"outputs":[{"name":"stdout","text":"478433 59804 59805\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"train_df['source_dir']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:10.684974Z","iopub.execute_input":"2025-11-05T00:36:10.685176Z","iopub.status.idle":"2025-11-05T00:36:10.691503Z","shell.execute_reply.started":"2025-11-05T00:36:10.685160Z","shell.execute_reply":"2025-11-05T00:36:10.690772Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"52151                     /kaggle/input/coco-image-caption/train2014/train2014\n595847    /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\n514118    /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\n160750                    /kaggle/input/coco-image-caption/train2014/train2014\n477213    /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\n                                          ...                                 \n110268                    /kaggle/input/coco-image-caption/train2014/train2014\n259178                    /kaggle/input/coco-image-caption/train2014/train2014\n365838                    /kaggle/input/coco-image-caption/train2014/train2014\n131932                    /kaggle/input/coco-image-caption/train2014/train2014\n121958                    /kaggle/input/coco-image-caption/train2014/train2014\nName: source_dir, Length: 478433, dtype: object"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# --- Configuration ---\nMAX_LEN = 128 # Fixed sequence length for the Transformer input\nBERT_MODEL = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nbert_model.eval()\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbert_model.to(device)\n\n# --- 1. Tokenization Function (adapted from previous step) ---\ndef prepare_caption_for_roberta(caption, tokenizer, max_len):\n    \"\"\"Tokenizes a caption using RoBERTa's subword tokenizer.\"\"\"\n    # 'comment' is the raw string caption from the DataFrame\n    encoding = tokenizer.encode_plus(\n        str(caption),\n        add_special_tokens=True,\n        max_length=max_len,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    # Squeeze(0) converts (1, MAX_LEN) to (MAX_LEN)\n    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0)\n\n# --- 2. Custom PyTorch Dataset ---\nclass TextToImageDataset(Dataset):\n    def __init__(self, df_combined, tokenizer, max_len, transform=None):\n        self.data = df_combined\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n    \n        # Text encoding\n        input_ids, attention_mask = prepare_caption_for_roberta(row['caption'], self.tokenizer, self.max_len)\n        #input_ids = input_ids.to(device)           # remove unsqueeze(0)\n        #attention_mask = attention_mask.to(device)\n\n        #with torch.no_grad():\n            #outputs = bert_model(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n            #text_embedding = outputs.last_hidden_state.squeeze(0)\n        # Image loading\n        img_name = row['unique_image_identifier'] + ('.jpg' if '.' not in row['unique_image_identifier'] else '')\n        img_dir = row['source_dir']\n        img_path = img_dir + '/' + img_name\n    \n        try:\n            image = Image.open(img_path).convert('RGB')\n        except FileNotFoundError:\n            print(f\"Warning: Image not found at {img_path}\")\n            image = Image.new('RGB', (256, 256), color='black')\n    \n        if self.transform:\n            image = self.transform(image)\n    \n        return image, input_ids, attention_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:10.692367Z","iopub.execute_input":"2025-11-05T00:36:10.692677Z","iopub.status.idle":"2025-11-05T00:36:14.028256Z","shell.execute_reply.started":"2025-11-05T00:36:10.692653Z","shell.execute_reply":"2025-11-05T00:36:14.027360Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9231740827f64f85991fe4d819b1cc3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef5ef0691d841e5b4554368753c1eb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90583b88313d4001b98ff84cc83a81e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"536dc3353d1b4411ba86ed6734785d48"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8e5b332f677478c87c27da95f361347"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"image_transforms = T.Compose([\n    T.Resize((256, 256)), # Target size for the VAE\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\ntrain_dataset = TextToImageDataset(train_df, tokenizer, MAX_LEN, image_transforms)\nval_dataset   = TextToImageDataset(val_df, tokenizer, MAX_LEN, image_transforms)\ntest_dataset  = TextToImageDataset(test_df, tokenizer, MAX_LEN, image_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.029238Z","iopub.execute_input":"2025-11-05T00:36:14.029835Z","iopub.status.idle":"2025-11-05T00:36:14.034688Z","shell.execute_reply.started":"2025-11-05T00:36:14.029807Z","shell.execute_reply":"2025-11-05T00:36:14.033950Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\nprint(\"Train samples:\", len(train_dataset))\nprint(\"Val samples:\", len(val_dataset))\nprint(\"Test samples:\", len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.035782Z","iopub.execute_input":"2025-11-05T00:36:14.036056Z","iopub.status.idle":"2025-11-05T00:36:14.053706Z","shell.execute_reply.started":"2025-11-05T00:36:14.036026Z","shell.execute_reply":"2025-11-05T00:36:14.052981Z"}},"outputs":[{"name":"stdout","text":"Train samples: 478433\nVal samples: 59804\nTest samples: 59805\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, dim, context_dim, num_heads=8):\n        super().__init__()\n        self.num_heads = num_heads\n        \n        self.query = nn.Linear(dim, dim)\n        self.key   = nn.Linear(context_dim, dim)\n        self.value = nn.Linear(context_dim, dim)\n        self.proj  = nn.Linear(dim, dim)\n        \n    def forward(self, x, context):\n        B, N, C = x.shape  # image tokens\n        _, M, D = context.shape  # text tokens\n\n        q = self.query(x)\n        #print(q.shape, 'query shape')\n        k = self.key(context)\n        #print(k.shape, 'key shape')\n        v = self.value(context)\n        #print(v.shape, 'after value shape')\n\n        q = q.view(B, N, self.num_heads, C//self.num_heads).transpose(1,2)\n        k = k.view(B, M, self.num_heads, C//self.num_heads).transpose(1,2)\n        v = v.view(B, M, self.num_heads, C//self.num_heads).transpose(1,2)\n        #print(q.shape, k.shape, v.shape)\n        attn = (q @ k.transpose(-2, -1)) / (C**0.5)\n        attn = attn.softmax(dim=-1)\n        #print(attn.shape, 'attention shape')\n        out = attn @ v\n        out = out.transpose(1,2).reshape(B, N, C)\n        #print(out.shape,'out shape')\n        return self.proj(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.054600Z","iopub.execute_input":"2025-11-05T00:36:14.054885Z","iopub.status.idle":"2025-11-05T00:36:14.064954Z","shell.execute_reply.started":"2025-11-05T00:36:14.054859Z","shell.execute_reply":"2025-11-05T00:36:14.064186Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class TextConditionedDecoder(nn.Module):\n    def __init__(self, latent_dim=4, hidden_dim=256, text_dim=768):\n        super().__init__()\n\n        self.initial_conv = nn.Conv2d(latent_dim, hidden_dim, 3, padding=1)\n\n        self.block1 = nn.Sequential(\n            nn.ConvTranspose2d(hidden_dim, hidden_dim//2, kernel_size = 4, stride = 2, padding=1),\n            nn.GroupNorm(8, hidden_dim//2),\n            nn.SiLU(),\n        )\n        self.cross1 = CrossAttention(hidden_dim, text_dim)\n\n        self.block2 = nn.Sequential(\n            nn.ConvTranspose2d(hidden_dim//2, hidden_dim//4, kernel_size=4, stride=2, padding=1),\n            nn.GroupNorm(8, hidden_dim//4),\n            nn.SiLU(),\n        )\n        self.cross2 = CrossAttention(hidden_dim//2, text_dim)\n\n        self.block3 = nn.Sequential(\n            nn.ConvTranspose2d(hidden_dim//4, hidden_dim//8, kernel_size=4, stride=2, padding=1),\n            nn.GroupNorm(8, hidden_dim//8),\n            nn.SiLU(),\n        )\n        \n        self.cross3 = CrossAttention(hidden_dim//4, text_dim)\n\n        self.final_conv = nn.Conv2d(hidden_dim//8, 3, 3, padding=1)\n\n    def forward(self, z, text_emb):\n        # z: (B, 4, H/8, W/8)\n        B, C, H, W = z.shape\n        x = self.initial_conv(z)\n        #print(x.shape,'after initial conv')\n        # Tokenize spatial map for cross-attention\n        x_tokens = x.flatten(2).transpose(1,2)  # (B, HW, C)\n        #print(x_tokens.shape, 'after flatten')\n        x_tokens = x_tokens + self.cross1(x_tokens, text_emb)\n        #print(x_tokens.shape, 'after one cross')\n        x = x_tokens.transpose(1,2).reshape(B, -1, H, W)\n        #print(x.shape, 'after transpose and reshape')\n        x = self.block1(x)\n        #print(x.shape, 'after block1')\n\n        x_tokens = x.flatten(2).transpose(1,2)\n        #print(x_tokens.shape, 'after flatten 2')\n        x_tokens = x_tokens + self.cross2(x_tokens, text_emb)\n        #print(x_tokens.shape, 'after cross2')\n        x = x_tokens.transpose(1,2).reshape(B, -1, H*2, W*2)\n        #print(x.shape, 'after transpose and reshape')\n        x = self.block2(x)\n        #print(x.shape,'after block2')\n\n\n        H, W = x.shape[2], x.shape[3]\n        x_tokens = x.flatten(2).transpose(1,2)\n        x_tokens = x_tokens + self.cross3(x_tokens, text_emb)\n        x = x_tokens.transpose(1,2).reshape(B, -1, H, W)\n        x = self.block3(x)  # -> 256x256\n        #print(x.shape, 'final block shape')\n        return torch.sigmoid(self.final_conv(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.065865Z","iopub.execute_input":"2025-11-05T00:36:14.066512Z","iopub.status.idle":"2025-11-05T00:36:14.080547Z","shell.execute_reply.started":"2025-11-05T00:36:14.066487Z","shell.execute_reply":"2025-11-05T00:36:14.079969Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class VAEEncoder(nn.Module):\n    def __init__(self, in_channels=3, latent_dim=4, hidden_dim=256):\n        super().__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim//2, 3, stride=2, padding=1),  # 256→128\n            nn.GroupNorm(8, hidden_dim//2),\n            nn.SiLU(),\n        )\n\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(hidden_dim//2, hidden_dim, 3, stride=2, padding=1),    # 128→64\n            nn.GroupNorm(8, hidden_dim),\n            nn.SiLU(),\n        )\n\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, 3, stride=2, padding=1),      # 64→32\n            nn.GroupNorm(8, hidden_dim),\n            nn.SiLU(),\n        )\n\n        # Final conv layers before producing mean & logvar\n        self.conv_out = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n            nn.GroupNorm(8, hidden_dim),\n            nn.SiLU(),\n        )\n\n        # Mean and log-variance projection heads\n        self.to_mean   = nn.Conv2d(hidden_dim, latent_dim, 3, padding=1)\n        self.to_logvar = nn.Conv2d(hidden_dim, latent_dim, 3, padding=1)\n\n    def forward(self, x):\n        # x : (B, 3, 256, 256)\n        x = self.conv1(x)  # -> (B, 128, 128, 128)\n        x = self.conv2(x)  # -> (B, 256,  64,  64)\n        x = self.conv3(x)  # -> (B, 256,  32,  32)\n\n        x = self.conv_out(x)  \n\n        mu     = self.to_mean(x)     # (B, 4, 32, 32)\n        logvar = self.to_logvar(x)   # (B, 4, 32, 32)\n\n        return mu, logvar\n\n\ndef reparameterize(mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.081243Z","iopub.execute_input":"2025-11-05T00:36:14.081678Z","iopub.status.idle":"2025-11-05T00:36:14.095674Z","shell.execute_reply.started":"2025-11-05T00:36:14.081655Z","shell.execute_reply":"2025-11-05T00:36:14.095124Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def kl_loss(mu, logvar, reduction='mean'):\n    \"\"\"\n    KL divergence between encoded latent distribution and N(0,1)\n    \"\"\"\n    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n    if reduction == 'sum':\n        return kl.sum()\n    return kl.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.096423Z","iopub.execute_input":"2025-11-05T00:36:14.096625Z","iopub.status.idle":"2025-11-05T00:36:14.109508Z","shell.execute_reply.started":"2025-11-05T00:36:14.096589Z","shell.execute_reply":"2025-11-05T00:36:14.108844Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nencoder = VAEEncoder().to(device)\ndecoder = TextConditionedDecoder().to(device)\nbert_model.eval().to(device)  # freeze BERT for now\n\noptimizer = torch.optim.AdamW(list(encoder.parameters()) + list(decoder.parameters()), lr=2e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.110369Z","iopub.execute_input":"2025-11-05T00:36:14.110594Z","iopub.status.idle":"2025-11-05T00:36:14.163290Z","shell.execute_reply.started":"2025-11-05T00:36:14.110578Z","shell.execute_reply":"2025-11-05T00:36:14.162666Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.163896Z","iopub.execute_input":"2025-11-05T00:36:14.164058Z","iopub.status.idle":"2025-11-05T00:36:14.168460Z","shell.execute_reply.started":"2025-11-05T00:36:14.164046Z","shell.execute_reply":"2025-11-05T00:36:14.167453Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"num_epochs = 5\n\nfor epoch in range(num_epochs):\n    encoder.train()\n    decoder.train()\n    \n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n        images, input_ids, attention_mask = batch  # images: (B,3,256,256), text_embeddings: (B, max_len, 768)\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n\n        with torch.no_grad():\n            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n            text_embedding = outputs.last_hidden_state\n            text_embedding = text_embedding.to(device)\n            \n        #print(text_embeddings.shape)\n        optimizer.zero_grad()\n\n        # --- Encode image ---\n        mu, logvar = encoder(images)\n        z = reparameterize(mu, logvar)  # (B, latent_dim, H, W)\n        #print(z.shape)\n        # --- Decode with text conditioning ---\n        recon = decoder(z, text_embedding)  # output: (B,3,256,256)\n\n        # --- Compute Loss ---\n        recon_loss = F.mse_loss(recon, images, reduction='mean')\n        kld = kl_loss(mu, logvar, reduction='mean')\n        loss = recon_loss + kld\n\n        # --- Backprop ---\n        loss.backward()\n        optimizer.step()\n\n        del images, input_ids, attention_mask, text_embedding, mu, logvar, z, recon, loss\n        torch.cuda.empty_cache()\n\n    print(f\"Epoch {epoch} | Recon Loss: {recon_loss.item():.4f} | KL Loss: {kld.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T00:36:14.169412Z","iopub.execute_input":"2025-11-05T00:36:14.169700Z"}},"outputs":[{"name":"stderr","text":"Epoch 0:  38%|███▊      | 5644/14952 [1:38:59<2:45:22,  1.07s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!kill 2584\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths\nencoder_path = \"/kaggle/working/encoder.pth\"\ndecoder_path = \"decoder.pth\"\noptimizer_path = \"optimizer.pth\"\n\n# Save models\ntorch.save(encoder.state_dict(), encoder_path)\ntorch.save(decoder.state_dict(), decoder_path)\ntorch.save(optimizer.state_dict(), optimizer_path)\n\nprint(\"Models and optimizer saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}