{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc9a080",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78acee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu128\n",
      "Device: cuda:0\n",
      "GPU Available: True\n",
      "GPU Memory: 191.51 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and Diffusers\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import (\n",
    "    StableDiffusionInstructPix2PixPipeline,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    DDPMScheduler,\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from torchvision import transforms\n",
    "\n",
    "# Check GPU\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0' if USE_GPU else 'cpu')\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPU Available: {USE_GPU}\")\n",
    "if USE_GPU:\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392f4bf",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b2cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: timbrooks/instruct-pix2pix\n",
      "  Image size: 512x512\n",
      "  Batch size: 2 (effective: 4)\n",
      "  Learning rate: 5e-05\n",
      "  Epochs: 20\n",
      "  Fine-tune UNet: True\n",
      "  Fine-tune Text Encoder: False\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model selection\n",
    "    model_id: str = \"timbrooks/instruct-pix2pix\"  # Pretrained InstructPix2Pix\n",
    "    \n",
    "    # Image configuration\n",
    "    image_size: int = 512\n",
    "    \n",
    "    # Training configuration\n",
    "    batch_size: int = 2  # Small batch for 8GB+ GPU\n",
    "    gradient_accumulation_steps: int = 1  # No accumulation - process full batch\n",
    "    num_epochs: int = 50  # More epochs with early stopping\n",
    "    learning_rate: float = 1e-5  # MUCH lower - fine-tuning not training from scratch\n",
    "    weight_decay: float = 0.0  # Disable weight decay for diffusion models\n",
    "    warmup_steps: int = 100  # Shorter warmup\n",
    "    max_grad_norm: float = 1.0\n",
    "    use_mixed_precision: bool = False  # Disable - causes training instability\n",
    "    \n",
    "    # Fine-tuning strategy\n",
    "    fine_tune_unet: bool = True  # Fine-tune UNet\n",
    "    fine_tune_text_encoder: bool = False  # Keep text encoder frozen\n",
    "    fine_tune_vae: bool = False  # Keep VAE frozen\n",
    "    \n",
    "    # Data\n",
    "    num_workers: int = 4\n",
    "    \n",
    "    # Paths\n",
    "    checkpoints_dir: str = \"./models/instructpix2pix_checkpoints\"\n",
    "    results_dir: str = \"./results/instructpix2pix\"\n",
    "    log_dir: str = \"./logs/instructpix2pix\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_id}\")\n",
    "print(f\"  Image size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"  Batch size: {config.batch_size} (effective: {config.batch_size * config.gradient_accumulation_steps})\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Fine-tune UNet: {config.fine_tune_unet}\")\n",
    "print(f\"  Fine-tune Text Encoder: {config.fine_tune_text_encoder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3157d",
   "metadata": {},
   "source": [
    "## 3. Load SSID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0597c4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train storylets: 62500\n",
      "Validation storylets: 3480\n",
      "Unique stories (train): 12500\n",
      "Unique stories (val): 696\n"
     ]
    }
   ],
   "source": [
    "# Load SSID annotations\n",
    "def load_annotations(json_path, split_name):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    flat_data = [storylet for story in data['annotations'] for storylet in story]\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    df['split'] = split_name\n",
    "    return df\n",
    "\n",
    "# Paths\n",
    "annotations_dir = '../data/SSID_Annotations/SSID_Annotations'\n",
    "images_dir = '../data/SSID_Images/SSID_Images'\n",
    "\n",
    "train_json = os.path.join(annotations_dir, \"SSID_Train.json\")\n",
    "val_json = os.path.join(annotations_dir, \"SSID_Validation.json\")\n",
    "\n",
    "# Load splits\n",
    "df_train = load_annotations(train_json, 'train')\n",
    "df_val = load_annotations(val_json, 'val')\n",
    "\n",
    "print(f\"Train storylets: {len(df_train)}\")\n",
    "print(f\"Validation storylets: {len(df_val)}\")\n",
    "print(f\"Unique stories (train): {df_train['story_id'].nunique()}\")\n",
    "print(f\"Unique stories (val): {df_val['story_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba1b8a",
   "metadata": {},
   "source": [
    "## 4. Create Training Pairs (Previous Image + Text → Next Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e998f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs: 50000 (missing: 0)\n",
      "Validation pairs: 2784 (missing: 0)\n",
      "\n",
      "Images directory: ../data/SSID_Images/SSID_Images\n",
      "Images directory exists: True\n",
      "Number of files: 17367\n",
      "Sample files: ['265.jpg', '10883.jpg', '10195.jpg', '3475.jpg', '25.jpg']\n",
      "\n",
      "Sample storylet data:\n",
      "  youtube_image_id  story_id  image_order\n",
      "0             2001      5887            1\n",
      "1             2002      5887            2\n",
      "2             2003      5887            3\n",
      "3             2004      5887            4\n",
      "4             2005      5887            5\n",
      "\n",
      "Example pair:\n",
      "  Input image: 2001.jpg\n",
      "  Edit prompt: He is telling me about his car....\n",
      "  Output image: 2002.jpg\n"
     ]
    }
   ],
   "source": [
    "def create_training_pairs(df, images_dir, split='train'):\n",
    "    \"\"\"\n",
    "    Create (input_image, edit_prompt, output_image) triplets.\n",
    "    input_image: previous frame in story\n",
    "    edit_prompt: text description of what changes to next frame\n",
    "    output_image: actual next frame\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    missing_count = 0\n",
    "    \n",
    "    for story_id in df['story_id'].unique():\n",
    "        story_data = df[df['story_id'] == story_id].sort_values('image_order').reset_index(drop=True)\n",
    "        \n",
    "        # Need at least 2 images per story\n",
    "        if len(story_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Create pairs: (image_t, text_t+1, image_t+1)\n",
    "        for i in range(len(story_data) - 1):\n",
    "            prev_row = story_data.iloc[i]\n",
    "            next_row = story_data.iloc[i + 1]\n",
    "            \n",
    "            # Try multiple image path patterns\n",
    "            image_id_prev = str(prev_row['youtube_image_id']).strip()\n",
    "            image_id_next = str(next_row['youtube_image_id']).strip()\n",
    "            \n",
    "            # Try different path formats\n",
    "            possible_prev_paths = [\n",
    "                os.path.join(images_dir, f\"{image_id_prev}.jpg\"),\n",
    "                os.path.join(images_dir, image_id_prev),\n",
    "                os.path.join(images_dir, f\"{image_id_prev}.png\"),\n",
    "            ]\n",
    "            possible_next_paths = [\n",
    "                os.path.join(images_dir, f\"{image_id_next}.jpg\"),\n",
    "                os.path.join(images_dir, image_id_next),\n",
    "                os.path.join(images_dir, f\"{image_id_next}.png\"),\n",
    "            ]\n",
    "            \n",
    "            prev_img_path = None\n",
    "            next_img_path = None\n",
    "            \n",
    "            for path in possible_prev_paths:\n",
    "                if os.path.exists(path):\n",
    "                    prev_img_path = path\n",
    "                    break\n",
    "            \n",
    "            for path in possible_next_paths:\n",
    "                if os.path.exists(path):\n",
    "                    next_img_path = path\n",
    "                    break\n",
    "            \n",
    "            if prev_img_path and next_img_path:\n",
    "                pairs.append({\n",
    "                    'input_image': prev_img_path,  # Previous image\n",
    "                    'edit_prompt': next_row['storytext'],  # Text describing next scene\n",
    "                    'output_image': next_img_path,  # Target next image\n",
    "                    'story_id': story_id,\n",
    "                    'split': split\n",
    "                })\n",
    "            else:\n",
    "                missing_count += 1\n",
    "    \n",
    "    return pairs, missing_count\n",
    "\n",
    "# Create pairs\n",
    "train_pairs, train_missing = create_training_pairs(df_train, images_dir, 'train')\n",
    "val_pairs, val_missing = create_training_pairs(df_val, images_dir, 'val')\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)} (missing: {train_missing})\")\n",
    "print(f\"Validation pairs: {len(val_pairs)} (missing: {val_missing})\")\n",
    "\n",
    "# Debug: List what's in the images directory\n",
    "print(f\"\\nImages directory: {images_dir}\")\n",
    "print(f\"Images directory exists: {os.path.exists(images_dir)}\")\n",
    "if os.path.exists(images_dir):\n",
    "    img_files = os.listdir(images_dir)\n",
    "    print(f\"Number of files: {len(img_files)}\")\n",
    "    if img_files:\n",
    "        print(f\"Sample files: {img_files[:5]}\")\n",
    "\n",
    "# Debug: Show sample storylet data\n",
    "if len(df_train) > 0:\n",
    "    print(f\"\\nSample storylet data:\")\n",
    "    print(df_train[['youtube_image_id', 'story_id', 'image_order']].head())\n",
    "\n",
    "if train_pairs:\n",
    "    print(f\"\\nExample pair:\")\n",
    "    pair = train_pairs[0]\n",
    "    print(f\"  Input image: {os.path.basename(pair['input_image'])}\")\n",
    "    print(f\"  Edit prompt: {pair['edit_prompt'][:60]}...\")\n",
    "    print(f\"  Output image: {os.path.basename(pair['output_image'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30bfd7",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99cf88f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 50000\n",
      "Validation dataset size: 2784\n",
      "Train loader batches: 25000\n",
      "Validation loader batches: 1392\n"
     ]
    }
   ],
   "source": [
    "class InstructPix2PixDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for InstructPix2Pix fine-tuning.\n",
    "    Input: (input_image, edit_prompt)\n",
    "    Target: output_image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pairs, tokenizer, image_size=512):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.CenterCrop((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Load and transform images\n",
    "        try:\n",
    "            input_img = Image.open(pair['input_image']).convert('RGB')\n",
    "            output_img = Image.open(pair['output_image']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images: {e}\")\n",
    "            # Return blank images as fallback\n",
    "            input_img = Image.new('RGB', (self.image_size, self.image_size), color='gray')\n",
    "            output_img = Image.new('RGB', (self.image_size, self.image_size), color='gray')\n",
    "        \n",
    "        input_tensor = self.image_transform(input_img)\n",
    "        output_tensor = self.image_transform(output_img)\n",
    "        \n",
    "        # Tokenize edit prompt\n",
    "        prompt = pair['edit_prompt']\n",
    "        tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_image': input_tensor,\n",
    "            'prompt_input_ids': tokens['input_ids'].squeeze(),\n",
    "            'prompt_attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'output_image': output_tensor,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InstructPix2PixDataset(train_pairs, tokenizer, config.image_size)\n",
    "val_dataset = InstructPix2PixDataset(val_pairs, tokenizer, config.image_size)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "\n",
    "print(f\"Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Validation loader batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4c25e",
   "metadata": {},
   "source": [
    "## 6. Load Pretrained InstructPix2Pix Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be7bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained InstructPix2Pix model...\n",
      "Model: timbrooks/instruct-pix2pix\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab5cf50c4334d40a697071d184e356b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model components loaded successfully!\n",
      "\n",
      "Model Architecture:\n",
      "  Text Encoder: 123.1M parameters\n",
      "  VAE: 83.7M parameters\n",
      "  UNet: 859.5M parameters\n",
      "  Total: 1066.2M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading pretrained InstructPix2Pix model...\")\n",
    "print(f\"Model: {config.model_id}\\n\")\n",
    "\n",
    "try:\n",
    "    # Load the full pipeline in float32 for training\n",
    "    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=False\n",
    "    )\n",
    "    \n",
    "    # Extract individual components\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    vae = pipe.vae\n",
    "    unet = pipe.unet\n",
    "    scheduler = pipe.scheduler\n",
    "    \n",
    "    # Move to device FIRST\n",
    "    text_encoder = text_encoder.to(DEVICE)\n",
    "    vae = vae.to(DEVICE)\n",
    "    unet = unet.to(DEVICE)\n",
    "    \n",
    "    # THEN convert to float32 explicitly (handles both params and buffers)\n",
    "    text_encoder = text_encoder.float()\n",
    "    vae = vae.float()\n",
    "    unet = unet.float()\n",
    "    \n",
    "    print(\"✓ Model components loaded successfully!\")\n",
    "    print(f\"\\nModel Architecture:\")\n",
    "    print(f\"  Text Encoder: {sum(p.numel() for p in text_encoder.parameters()) / 1e6:.1f}M parameters (dtype: {next(text_encoder.parameters()).dtype})\")\n",
    "    print(f\"  VAE: {sum(p.numel() for p in vae.parameters()) / 1e6:.1f}M parameters (dtype: {next(vae.parameters()).dtype})\")\n",
    "    print(f\"  UNet: {sum(p.numel() for p in unet.parameters()) / 1e6:.1f}M parameters (dtype: {next(unet.parameters()).dtype})\")\n",
    "    print(f\"  Total: {(sum(p.numel() for p in text_encoder.parameters()) + sum(p.numel() for p in vae.parameters()) + sum(p.numel() for p in unet.parameters())) / 1e6:.1f}M parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model: {e}\")\n",
    "    print(f\"Make sure you have internet connection to download the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb9b0c",
   "metadata": {},
   "source": [
    "## 7. Setup for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b686e887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoder frozen\n",
      "VAE frozen\n",
      "UNet unfrozen for fine-tuning\n",
      "\n",
      "Trainable parameters: 859.5M\n",
      "\n",
      "Optimizer: AdamW\n",
      "Learning rate: 5e-05\n",
      "Total training steps: 250000\n",
      "Mixed precision: True\n"
     ]
    }
   ],
   "source": [
    "# Freeze components we're not fine-tuning\n",
    "if not config.fine_tune_text_encoder:\n",
    "    text_encoder.requires_grad_(False)\n",
    "    print(\"Text encoder frozen\")\n",
    "\n",
    "if not config.fine_tune_vae:\n",
    "    vae.requires_grad_(False)\n",
    "    print(\"VAE frozen\")\n",
    "\n",
    "# Only fine-tune UNet\n",
    "if config.fine_tune_unet:\n",
    "    unet.requires_grad_(True)\n",
    "    print(\"UNet unfrozen for fine-tuning\")\n",
    "else:\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "print(f\"\\nTrainable parameters: {trainable_params / 1e6:.1f}M\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = AdamW(\n",
    "    unet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_update_steps_per_epoch = len(train_loader) // config.gradient_accumulation_steps\n",
    "max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=max_train_steps\n",
    ")\n",
    "\n",
    "# Gradient scaler for mixed precision\n",
    "scaler = GradScaler() if config.use_mixed_precision and USE_GPU else None\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Total training steps: {max_train_steps}\")\n",
    "print(f\"Mixed precision: {config.use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4165f",
   "metadata": {},
   "source": [
    "## 8. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab1f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined\n"
     ]
    }
   ],
   "source": [
    "def encode_text_embedding(tokenizer, text_encoder, prompt, device, dtype):\n",
    "    \"\"\"Encode text prompt to embedding\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        prompt,\n",
    "        padding='max_length',\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embedding = text_encoder(\n",
    "            input_ids=tokens['input_ids'].to(device),\n",
    "            attention_mask=tokens['attention_mask'].to(device)\n",
    "        )[0]\n",
    "    \n",
    "    return text_embedding.to(dtype)\n",
    "\n",
    "\n",
    "def train_epoch(epoch, unet, vae, text_encoder, tokenizer, train_loader, optimizer, lr_scheduler, criterion, device, scaler, dtype):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    unet.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", ncols=80)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Load batch to device with correct dtype\n",
    "            input_images = batch['input_image'].to(device, dtype=dtype)\n",
    "            output_images = batch['output_image'].to(device, dtype=dtype)\n",
    "            prompts = batch['prompt']\n",
    "            \n",
    "            # Encode images to latent space\n",
    "            with torch.no_grad():\n",
    "                input_latents = vae.encode(input_images).latent_dist.sample() * 0.18215\n",
    "                output_latents = vae.encode(output_images).latent_dist.sample() * 0.18215\n",
    "                input_latents = input_latents.to(dtype)\n",
    "                output_latents = output_latents.to(dtype)\n",
    "            \n",
    "            # Encode prompts\n",
    "            prompt_embeds_list = []\n",
    "            for prompt in prompts:\n",
    "                prompt_embed = encode_text_embedding(tokenizer, text_encoder, prompt, device, dtype)\n",
    "                prompt_embeds_list.append(prompt_embed)\n",
    "            prompt_embeds = torch.cat(prompt_embeds_list, dim=0)\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                scheduler.config.num_train_timesteps,\n",
    "                (input_latents.shape[0],),\n",
    "                device=device\n",
    "            ).long()\n",
    "            \n",
    "            # Sample noise\n",
    "            noise = torch.randn_like(output_latents)\n",
    "            \n",
    "            # Add noise to output latents (forward process)\n",
    "            noisy_latents = scheduler.add_noise(output_latents, noise, timesteps)\n",
    "            \n",
    "            # Concatenate input and noisy latents\n",
    "            latent_model_input = torch.cat([input_latents, noisy_latents], dim=1)\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = unet(\n",
    "                latent_model_input,\n",
    "                timesteps,\n",
    "                encoder_hidden_states=prompt_embeds\n",
    "            ).sample\n",
    "            \n",
    "            # MSE loss\n",
    "            loss = F.mse_loss(noise_pred, noise, reduction='mean')\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), config.max_grad_norm)\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(unet, vae, text_encoder, tokenizer, val_loader, device, dtype):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    unet.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=\"Validating\", ncols=80)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                input_images = batch['input_image'].to(device, dtype=dtype)\n",
    "                output_images = batch['output_image'].to(device, dtype=dtype)\n",
    "                prompts = batch['prompt']\n",
    "                \n",
    "                # Encode images\n",
    "                input_latents = vae.encode(input_images).latent_dist.sample() * 0.18215\n",
    "                output_latents = vae.encode(output_images).latent_dist.sample() * 0.18215\n",
    "                input_latents = input_latents.to(dtype)\n",
    "                output_latents = output_latents.to(dtype)\n",
    "                \n",
    "                # Encode prompts\n",
    "                prompt_embeds_list = []\n",
    "                for prompt in prompts:\n",
    "                    prompt_embed = encode_text_embedding(tokenizer, text_encoder, prompt, device, dtype)\n",
    "                    prompt_embeds_list.append(prompt_embed)\n",
    "                prompt_embeds = torch.cat(prompt_embeds_list, dim=0)\n",
    "                \n",
    "                # Random timesteps\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    scheduler.config.num_train_timesteps,\n",
    "                    (input_latents.shape[0],),\n",
    "                    device=device\n",
    "                ).long()\n",
    "                \n",
    "                # Noise\n",
    "                noise = torch.randn_like(output_latents)\n",
    "                \n",
    "                noisy_latents = scheduler.add_noise(output_latents, noise, timesteps)\n",
    "                latent_model_input = torch.cat([input_latents, noisy_latents], dim=1)\n",
    "                \n",
    "                # Forward\n",
    "                noise_pred = unet(\n",
    "                    latent_model_input,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=prompt_embeds\n",
    "                ).sample\n",
    "                \n",
    "                loss = F.mse_loss(noise_pred, noise, reduction='mean')\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f7751",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Model: timbrooks/instruct-pix2pix\n",
      "Epochs: 20\n",
      "Training samples: 50000\n",
      "Validation samples: 2784\n",
      "Device: cuda:0\n",
      "\n",
      "Dtype: torch.float32\n",
      "\n",
      "============================================================\n",
      "\n",
      "Epoch 1/20\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                               | 18/25000 [00:01<31:07, 13.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 0: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 1: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 2: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 3: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 4: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 5: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 6: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 7: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 8: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 9: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 10: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 11: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 12: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 13: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 14: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 15: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 16: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 17: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 18: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 19: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 20: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 21: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 22: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 23: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 24: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 25: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 26: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                               | 44/25000 [00:02<11:29, 36.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 27: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 28: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 29: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 30: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 31: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 32: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 33: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 34: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 35: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 36: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 37: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 38: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 39: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 40: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 41: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 42: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 43: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 44: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 45: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 46: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 47: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 48: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 49: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 50: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 51: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 52: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 53: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 54: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                               | 76/25000 [00:02<06:03, 68.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 55: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 56: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 57: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 58: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 59: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 60: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 61: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 62: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 63: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 64: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 65: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 66: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 67: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 68: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 69: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 70: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 71: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 72: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 73: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 74: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 75: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 76: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 77: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 78: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 79: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 80: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 81: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 82: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 83: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 84: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▏                             | 108/25000 [00:02<04:16, 97.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 85: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 86: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 87: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 88: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 89: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 90: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 91: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 92: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 93: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 94: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 95: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 96: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 97: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 98: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 99: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 100: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 101: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 102: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 103: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 104: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 105: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 106: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 107: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 108: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 109: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 110: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 111: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 112: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 113: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 114: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▏                            | 140/25000 [00:02<03:31, 117.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 115: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 116: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 117: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 118: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 119: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 120: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 121: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 122: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 123: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 124: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 125: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 126: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 127: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 128: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 129: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 130: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 131: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 132: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 133: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 134: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 135: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 136: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 137: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 138: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 139: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 140: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 141: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 142: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 143: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 144: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 145: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 146: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▏                            | 172/25000 [00:02<03:11, 129.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 147: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 148: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 149: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 150: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 151: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 152: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 153: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 154: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 155: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 156: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 157: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 158: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 159: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 160: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 161: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 162: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 163: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 164: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 165: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 166: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 167: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 168: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 169: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 170: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 171: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 172: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 173: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 174: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▏                            | 202/25000 [00:03<03:09, 130.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 175: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 176: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 177: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 178: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 179: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 180: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 181: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 182: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 183: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 184: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 185: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 186: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 187: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 188: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 189: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 190: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 191: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 192: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 193: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 194: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 195: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 196: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 197: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 198: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 199: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 200: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 201: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 202: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▎                            | 216/25000 [00:03<03:11, 129.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 203: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 204: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 205: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 206: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 207: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 208: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 209: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 210: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 211: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 212: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 213: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 214: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 215: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 216: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 217: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 218: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 219: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 220: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 221: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 222: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 223: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 224: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 225: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 226: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 227: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 228: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 229: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 230: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▎                            | 248/25000 [00:03<03:02, 135.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 231: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 232: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 233: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 234: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 235: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 236: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 237: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 238: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 239: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 240: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 241: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 242: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 243: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 244: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 245: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 246: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 247: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 248: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 249: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 250: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 251: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 252: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 253: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 254: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 255: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 256: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 257: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 258: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 259: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 260: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 261: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 262: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▎                            | 280/25000 [00:03<02:58, 138.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 263: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 264: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 265: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 266: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 267: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 268: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 269: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 270: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 271: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 272: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 273: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 274: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 275: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 276: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 277: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 278: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 279: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 280: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 281: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 282: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 283: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 284: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 285: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 286: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 287: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 288: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 289: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 290: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▎                             | 307/25000 [00:03<05:18, 77.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in batch 291: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 292: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 293: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 294: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 295: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 296: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 297: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 298: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 299: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 300: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 301: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 302: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 303: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 304: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 305: Input type (float) and bias type (c10::Half) should be the same\n",
      "\n",
      "Error in batch 306: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m train_losses.append(train_loss)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(epoch, unet, vae, text_encoder, tokenizer, train_loader, optimizer, lr_scheduler, criterion, device, scaler, dtype)\u001b[39m\n\u001b[32m     23\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     25\u001b[39m progress_bar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, ncols=\u001b[32m80\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Load batch to device with correct dtype\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_images\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_image\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1434\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1436\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/eoi/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  # Aggressive early stopping\n",
    "patience_counter = 0\n",
    "no_improve_threshold = 0.001  # Stop if loss doesn't improve by at least 0.1%\n",
    "\n",
    "# Use float32 for stable training (regardless of GPU)\n",
    "dtype = torch.float32\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(f\"Model: {config.model_id}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Device: {DEVICE}\\n\")\n",
    "print(f\"Dtype: {dtype}\\n\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        epoch,\n",
    "        unet,\n",
    "        vae,\n",
    "        text_encoder,\n",
    "        tokenizer,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        F.mse_loss,\n",
    "        DEVICE,\n",
    "        None,  # scaler - not used with mixed_precision=False\n",
    "        dtype\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(unet, vae, text_encoder, tokenizer, val_loader, DEVICE, dtype)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping with improvement check\n",
    "    if val_loss < best_val_loss - no_improve_threshold:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_path = os.path.join(config.checkpoints_dir, \"best_unet\")\n",
    "        unet.save_pretrained(checkpoint_path)\n",
    "        print(f\"✓ Best model saved (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(config.checkpoints_dir, f\"unet_epoch_{epoch + 1}\")\n",
    "        unet.save_pretrained(checkpoint_path)\n",
    "        print(f\"✓ Checkpoint saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0799e6a",
   "metadata": {},
   "source": [
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(train_losses, label='Train Loss', marker='o', linewidth=2, markersize=6)\n",
    "ax.plot(val_losses, label='Val Loss', marker='s', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('InstructPix2Pix Fine-tuning - Training History', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.results_dir, 'training_loss.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss plot saved to: {os.path.join(config.results_dir, 'training_loss.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94d25f",
   "metadata": {},
   "source": [
    "## 11. Load Fine-tuned Model and Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned UNet\n",
    "best_unet_path = os.path.join(config.checkpoints_dir, \"best_unet\")\n",
    "\n",
    "print(f\"Best UNet path: {best_unet_path}\")\n",
    "print(f\"Exists: {os.path.exists(best_unet_path)}\")\n",
    "\n",
    "if os.path.exists(best_unet_path):\n",
    "    print(f\"Loading fine-tuned UNet from: {best_unet_path}\")\n",
    "    \n",
    "    # Create new pipeline with fine-tuned UNet\n",
    "    fine_tuned_unet = UNet2DConditionModel.from_pretrained(best_unet_path)\n",
    "    print(f\"Loaded UNet parameters: {sum(p.numel() for p in fine_tuned_unet.parameters()) / 1e6:.1f}M\")\n",
    "    print(f\"Loaded UNet dtype: {next(fine_tuned_unet.parameters()).dtype}\")\n",
    "    \n",
    "    pipe_finetuned = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        unet=fine_tuned_unet,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"✓ Fine-tuned pipeline created successfully\")\n",
    "    print(f\"Pipeline UNet dtype: {next(pipe_finetuned.unet.parameters()).dtype}\")\n",
    "else:\n",
    "    print(f\"Best model not found at {best_unet_path}\")\n",
    "    print(\"Using original pretrained model\")\n",
    "    print(\"\\nAvailable checkpoints:\")\n",
    "    if os.path.exists(config.checkpoints_dir):\n",
    "        for item in os.listdir(config.checkpoints_dir):\n",
    "            print(f\"  - {item}\")\n",
    "    \n",
    "    pipe_finetuned = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5015a",
   "metadata": {},
   "source": [
    "## 12. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431aec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check training and validation data\n",
    "print(\"=\"*60)\n",
    "print(\"DATA VALIDATION CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTraining pairs: {len(train_pairs)}\")\n",
    "print(f\"Validation pairs: {len(val_pairs)}\")\n",
    "\n",
    "if len(train_pairs) == 0:\n",
    "    print(\"\\n⚠️  WARNING: No training pairs found!\")\n",
    "    print(\"Checking data loading...\")\n",
    "    \n",
    "    # Check if images directory exists\n",
    "    print(f\"\\nImages directory: {images_dir}\")\n",
    "    print(f\"Exists: {os.path.exists(images_dir)}\")\n",
    "    \n",
    "    if os.path.exists(images_dir):\n",
    "        files = os.listdir(images_dir)\n",
    "        print(f\"Files in directory: {len(files)}\")\n",
    "        if files:\n",
    "            print(f\"Sample files: {files[:10]}\")\n",
    "    \n",
    "    print(f\"\\nTrain annotations: {len(df_train)}\")\n",
    "    print(f\"Val annotations: {len(df_val)}\")\n",
    "    \n",
    "    if len(df_train) > 0:\n",
    "        print(f\"\\nSample annotation structure:\")\n",
    "        print(df_train.iloc[0])\n",
    "else:\n",
    "    print(\"\\n✓ Training pairs found\")\n",
    "    \n",
    "    # Show sample pair\n",
    "    sample_pair = train_pairs[0]\n",
    "    print(f\"\\nSample training pair:\")\n",
    "    print(f\"  Input exists: {os.path.exists(sample_pair['input_image'])}\")\n",
    "    print(f\"  Output exists: {os.path.exists(sample_pair['output_image'])}\")\n",
    "    print(f\"  Prompt: {sample_pair['edit_prompt'][:60]}...\")\n",
    "    \n",
    "    if os.path.exists(sample_pair['input_image']):\n",
    "        img = Image.open(sample_pair['input_image'])\n",
    "        print(f\"  Input image size: {img.size}\")\n",
    "\n",
    "if len(val_pairs) > 0:\n",
    "    sample_val = val_pairs[0]\n",
    "    print(f\"\\nSample validation pair:\")\n",
    "    print(f\"  Input exists: {os.path.exists(sample_val['input_image'])}\")\n",
    "    print(f\"  Output exists: {os.path.exists(sample_val['output_image'])}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b674362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VAE encoding/decoding directly\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VAE ENCODING/DECODING TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(val_pairs) > 0:\n",
    "    try:\n",
    "        # Load a test image\n",
    "        test_img_path = val_pairs[0]['input_image']\n",
    "        test_img = Image.open(test_img_path).convert('RGB')\n",
    "        \n",
    "        # Transform it\n",
    "        from torchvision import transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        img_tensor = transform(test_img).unsqueeze(0).to(DEVICE, dtype=torch.float32)\n",
    "        print(f\"Input tensor shape: {img_tensor.shape}\")\n",
    "        print(f\"Input tensor dtype: {img_tensor.dtype}\")\n",
    "        print(f\"Input tensor range: [{img_tensor.min():.2f}, {img_tensor.max():.2f}]\")\n",
    "        \n",
    "        # Encode\n",
    "        with torch.no_grad():\n",
    "            vae.eval()\n",
    "            latents = vae.encode(img_tensor).latent_dist.sample() * 0.18215\n",
    "        \n",
    "        print(f\"\\nLatent shape: {latents.shape}\")\n",
    "        print(f\"Latent dtype: {latents.dtype}\")\n",
    "        print(f\"Latent range: [{latents.min():.2f}, {latents.max():.2f}]\")\n",
    "        \n",
    "        # Decode\n",
    "        with torch.no_grad():\n",
    "            reconstructed = vae.decode(latents / 0.18215).sample\n",
    "        \n",
    "        print(f\"\\nReconstructed shape: {reconstructed.shape}\")\n",
    "        print(f\"Reconstructed dtype: {reconstructed.dtype}\")\n",
    "        print(f\"Reconstructed range: [{reconstructed.min():.2f}, {reconstructed.max():.2f}]\")\n",
    "        \n",
    "        # Convert to image\n",
    "        reconstructed = (reconstructed / 2 + 0.5).clamp(0, 1)\n",
    "        reconstructed_img = transforms.ToPILImage()(reconstructed.squeeze(0))\n",
    "        \n",
    "        # Display\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes[0].imshow(test_img)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title('VAE Reconstruction')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n✓ VAE is working correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ VAE Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Test UNet output directly\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UNET OUTPUT TEST - DIAGNOSING BLACK PIXELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(val_pairs) > 0:\n",
    "    try:\n",
    "        val_pair = val_pairs[0]\n",
    "        input_img = Image.open(val_pair['input_image']).convert('RGB')\n",
    "        \n",
    "        # Prepare input\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        input_tensor = transform(input_img).unsqueeze(0).to(DEVICE, dtype=torch.float32)\n",
    "        \n",
    "        # Encode to latents\n",
    "        with torch.no_grad():\n",
    "            vae.eval()\n",
    "            input_latents = vae.encode(input_tensor).latent_dist.sample() * 0.18215\n",
    "            input_latents = input_latents.to(dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Input latents shape: {input_latents.shape}\")\n",
    "        print(f\"Input latents range: [{input_latents.min():.4f}, {input_latents.max():.4f}]\")\n",
    "        \n",
    "        # Create dummy output latents\n",
    "        with torch.no_grad():\n",
    "            output_img = Image.open(val_pair['output_image']).convert('RGB')\n",
    "            output_tensor = transform(output_img).unsqueeze(0).to(DEVICE, dtype=torch.float32)\n",
    "            output_latents = vae.encode(output_tensor).latent_dist.sample() * 0.18215\n",
    "            output_latents = output_latents.to(dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Output latents range: [{output_latents.min():.4f}, {output_latents.max():.4f}]\")\n",
    "        \n",
    "        # Create noisy output\n",
    "        timestep = torch.tensor([500]).to(DEVICE)\n",
    "        noise = torch.randn_like(output_latents)\n",
    "        noisy_latents = scheduler.add_noise(output_latents, noise, timestep)\n",
    "        \n",
    "        print(f\"Noisy latents range: [{noisy_latents.min():.4f}, {noisy_latents.max():.4f}]\")\n",
    "        \n",
    "        # Concatenate\n",
    "        latent_model_input = torch.cat([input_latents, noisy_latents], dim=1)\n",
    "        print(f\"Model input shape: {latent_model_input.shape}\")\n",
    "        print(f\"Model input range: [{latent_model_input.min():.4f}, {latent_model_input.max():.4f}]\")\n",
    "        \n",
    "        # Encode prompt\n",
    "        prompt_embed = encode_text_embedding(tokenizer, text_encoder, val_pair['edit_prompt'], DEVICE, torch.float32)\n",
    "        print(f\"Prompt embed shape: {prompt_embed.shape}\")\n",
    "        \n",
    "        # Run PRETRAINED UNet (original)\n",
    "        print(\"\\n--- Testing PRETRAINED UNet ---\")\n",
    "        with torch.no_grad():\n",
    "            unet.eval()\n",
    "            noise_pred_original = unet(\n",
    "                latent_model_input,\n",
    "                timestep,\n",
    "                encoder_hidden_states=prompt_embed\n",
    "            ).sample\n",
    "        \n",
    "        print(f\"Original UNet output shape: {noise_pred_original.shape}\")\n",
    "        print(f\"Original UNet output range: [{noise_pred_original.min():.4f}, {noise_pred_original.max():.4f}]\")\n",
    "        print(f\"Original UNet output std: {noise_pred_original.std():.4f}\")\n",
    "        \n",
    "        # Now load and test FINE-TUNED UNet\n",
    "        print(\"\\n--- Testing FINE-TUNED UNet ---\")\n",
    "        best_unet_path = os.path.join(config.checkpoints_dir, \"best_unet\")\n",
    "        \n",
    "        if os.path.exists(best_unet_path):\n",
    "            ft_unet = UNet2DConditionModel.from_pretrained(best_unet_path)\n",
    "            ft_unet = ft_unet.to(DEVICE).float()\n",
    "            ft_unet.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                noise_pred_ft = ft_unet(\n",
    "                    latent_model_input,\n",
    "                    timestep,\n",
    "                    encoder_hidden_states=prompt_embed\n",
    "                ).sample\n",
    "            \n",
    "            print(f\"Fine-tuned UNet output shape: {noise_pred_ft.shape}\")\n",
    "            print(f\"Fine-tuned UNet output range: [{noise_pred_ft.min():.4f}, {noise_pred_ft.max():.4f}]\")\n",
    "            print(f\"Fine-tuned UNet output std: {noise_pred_ft.std():.4f}\")\n",
    "            \n",
    "            # Calculate difference\n",
    "            diff = (noise_pred_ft - noise_pred_original).abs().mean()\n",
    "            print(f\"\\nDifference between original and fine-tuned: {diff:.6f}\")\n",
    "            \n",
    "            if noise_pred_ft.std() < 0.001:\n",
    "                print(\"\\n⚠️  PROBLEM: Fine-tuned UNet output has very low variance!\")\n",
    "                print(\"This suggests the model collapsed during training.\")\n",
    "                print(\"\\nRECOMMENDATION:\")\n",
    "                print(\"1. Check training data - ensure images are not all black\")\n",
    "                print(\"2. Reduce learning rate (try 1e-5 instead of 5e-5)\")\n",
    "                print(\"3. Use fewer epochs to prevent overfitting\")\n",
    "                print(\"4. Check if gradient clipping is too aggressive\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Fine-tuned model not found at {best_unet_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afac92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing inference on validation samples...\\n\")\n",
    "\n",
    "for idx in range(min(3, len(val_pairs))):\n",
    "    pair = val_pairs[idx]\n",
    "    \n",
    "    print(f\"\\nSample {idx + 1}:\")\n",
    "    print(f\"  Edit prompt: {pair['edit_prompt'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load input image\n",
    "        input_img = Image.open(pair['input_image']).convert('RGB')\n",
    "        print(f\"  Input image shape: {input_img.size}\")\n",
    "        \n",
    "        # Generate next image with consistent dtype and enable_attention_slicing for stability\n",
    "        with torch.no_grad():\n",
    "            # Ensure pipeline is in eval mode\n",
    "            pipe_finetuned.unet.eval()\n",
    "            pipe_finetuned.vae.eval()\n",
    "            pipe_finetuned.text_encoder.eval()\n",
    "            \n",
    "            generated = pipe_finetuned(\n",
    "                prompt=pair['edit_prompt'],\n",
    "                image=input_img,\n",
    "                guidance_scale=7.5,\n",
    "                num_inference_steps=30,\n",
    "                height=512,\n",
    "                width=512\n",
    "            ).images[0]\n",
    "        \n",
    "        print(f\"  Generated image shape: {generated.size}\")\n",
    "        \n",
    "        # Load ground truth\n",
    "        ground_truth = Image.open(pair['output_image']).convert('RGB')\n",
    "        \n",
    "        # Display\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(input_img)\n",
    "        axes[0].set_title('Input Image (Previous Frame)', fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(generated)\n",
    "        axes[1].set_title('Generated (Fine-tuned)', fontsize=10, color='green')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(ground_truth)\n",
    "        axes[2].set_title('Ground Truth (Next Frame)', fontsize=10, color='blue')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(config.results_dir, f'inference_sample_{idx + 1}.png'),\n",
    "            dpi=100,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        print(f\"  ✓ Sample saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nInference samples saved to: {config.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72126d78",
   "metadata": {},
   "source": [
    "## 13. Generate Full Story Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_with_instructpix2pix(story_id, df, pipe, images_dir):\n",
    "    \"\"\"\n",
    "    Generate a full story sequence using InstructPix2Pix.\n",
    "    \"\"\"\n",
    "    story_data = df[df['story_id'] == story_id].sort_values('image_order')\n",
    "    generated_imgs = []\n",
    "    \n",
    "    if len(story_data) < 2:\n",
    "        print(f\"Story {story_id} has less than 2 images\")\n",
    "        return None\n",
    "    \n",
    "    # First image (use real)\n",
    "    first_img_path = os.path.join(images_dir, f\"{story_data.iloc[0]['youtube_image_id']}.jpg\")\n",
    "    first_img = Image.open(first_img_path).convert('RGB')\n",
    "    generated_imgs.append(first_img)\n",
    "    current_img = first_img\n",
    "    \n",
    "    # Generate remaining frames\n",
    "    print(f\"\\nGenerating story sequence (story_id: {story_id})...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(1, len(story_data)):\n",
    "            prompt = story_data.iloc[i]['storytext']\n",
    "            print(f\"  Frame {i + 1}/{len(story_data)}: {prompt[:40]}...\")\n",
    "            \n",
    "            try:\n",
    "                generated = pipe(\n",
    "                    prompt=prompt,\n",
    "                    image=current_img,\n",
    "                    guidance_scale=7.5,\n",
    "                    num_inference_steps=30\n",
    "                ).images[0]\n",
    "                \n",
    "                generated_imgs.append(generated)\n",
    "                current_img = generated  # Use generated as input for next\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {e}\")\n",
    "                break\n",
    "    \n",
    "    return generated_imgs, story_data\n",
    "\n",
    "\n",
    "# Generate a story\n",
    "if len(val_pairs) > 0:\n",
    "    story_id = val_pairs[0]['story_id']\n",
    "    \n",
    "    result = generate_story_with_instructpix2pix(\n",
    "        story_id,\n",
    "        df_val,\n",
    "        pipe_finetuned,\n",
    "        images_dir\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        generated_imgs, story_data = result\n",
    "        \n",
    "        # Display sequence\n",
    "        num_imgs = len(generated_imgs)\n",
    "        fig, axes = plt.subplots(2, (num_imgs + 1) // 2, figsize=(4*(num_imgs), 8))\n",
    "        axes = axes.flatten() if num_imgs > 1 else [axes]\n",
    "        \n",
    "        for idx, img in enumerate(generated_imgs):\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].set_title(f\"Frame {idx + 1}\", fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(num_imgs, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(config.results_dir, 'full_story_sequence.png'),\n",
    "            dpi=100,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nGenerated {num_imgs} frames for story {story_id}\")\n",
    "        print(\"\\nStory narrative:\")\n",
    "        for idx, row in story_data.iterrows():\n",
    "            print(f\"{row['image_order']}. {row['storytext']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045e2b6",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d79c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 14. CLIP Alignment Evaluation\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLIP ALIGNMENT EVALUATION - Pretrained vs Fine-tuned\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Install clip if needed\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        import clip\n",
    "    except ImportError:\n",
    "        print(\"Installing CLIP...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai-clip\", \"-q\"])\n",
    "        import clip\n",
    "    \n",
    "    from PIL import Image as PILImage\n",
    "    import torch.nn.functional as F_nn\n",
    "    \n",
    "    # Load CLIP model\n",
    "    device_clip = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device_clip)\n",
    "    clip_model.eval()\n",
    "    \n",
    "    print(f\"✓ CLIP model loaded on {device_clip}\")\n",
    "    \n",
    "    def calculate_clip_score(image, text, model, preprocess, device):\n",
    "        \"\"\"Calculate CLIP alignment score between image and text (0-100)\"\"\"\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            if isinstance(image, PILImage.Image):\n",
    "                image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "            else:\n",
    "                image_input = image.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Tokenize text\n",
    "            text_input = clip.tokenize([text]).to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(image_input)\n",
    "                text_features = model.encode_text(text_input)\n",
    "                \n",
    "                # Normalize\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity = (image_features @ text_features.t()).squeeze()\n",
    "                score = float(similarity.cpu().numpy()) * 100  # Scale to 0-100\n",
    "            \n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating CLIP score: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Evaluate on validation samples\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Evaluating on validation samples (Pretrained vs Fine-tuned)\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    pretrained_scores = []\n",
    "    finetuned_scores = []\n",
    "    \n",
    "    for idx in range(min(5, len(val_pairs))):\n",
    "        pair = val_pairs[idx]\n",
    "        prompt = pair['edit_prompt']\n",
    "        \n",
    "        print(f\"\\n📊 Sample {idx + 1}:\")\n",
    "        print(f\"  Prompt: {prompt[:70]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load input image\n",
    "            input_img = PILImage.open(pair['input_image']).convert('RGB')\n",
    "            \n",
    "            # Generate with PRETRAINED model\n",
    "            with torch.no_grad():\n",
    "                pipe.unet.eval()\n",
    "                pipe.vae.eval()\n",
    "                pipe.text_encoder.eval()\n",
    "                \n",
    "                generated_pretrained = pipe(\n",
    "                    prompt=prompt,\n",
    "                    image=input_img,\n",
    "                    guidance_scale=7.5,\n",
    "                    num_inference_steps=20,\n",
    "                    height=512,\n",
    "                    width=512\n",
    "                ).images[0]\n",
    "            \n",
    "            # Generate with FINE-TUNED model\n",
    "            with torch.no_grad():\n",
    "                pipe_finetuned.unet.eval()\n",
    "                pipe_finetuned.vae.eval()\n",
    "                pipe_finetuned.text_encoder.eval()\n",
    "                \n",
    "                generated_finetuned = pipe_finetuned(\n",
    "                    prompt=prompt,\n",
    "                    image=input_img,\n",
    "                    guidance_scale=7.5,\n",
    "                    num_inference_steps=20,\n",
    "                    height=512,\n",
    "                    width=512\n",
    "                ).images[0]\n",
    "            \n",
    "            # Calculate CLIP scores\n",
    "            score_pretrained = calculate_clip_score(generated_pretrained, prompt, clip_model, clip_preprocess, device_clip)\n",
    "            score_finetuned = calculate_clip_score(generated_finetuned, prompt, clip_model, clip_preprocess, device_clip)\n",
    "            \n",
    "            if score_pretrained and score_finetuned:\n",
    "                pretrained_scores.append(score_pretrained)\n",
    "                finetuned_scores.append(score_finetuned)\n",
    "                \n",
    "                improvement = score_finetuned - score_pretrained\n",
    "                improvement_pct = (improvement / score_pretrained * 100) if score_pretrained > 0 else 0\n",
    "                \n",
    "                print(f\"  Pretrained CLIP Score: {score_pretrained:.2f}/100\")\n",
    "                print(f\"  Fine-tuned CLIP Score: {score_finetuned:.2f}/100\")\n",
    "                print(f\"  Improvement: {improvement:+.2f} ({improvement_pct:+.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary statistics\n",
    "    if pretrained_scores and finetuned_scores:\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"CLIP ALIGNMENT SUMMARY\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        avg_pretrained = np.mean(pretrained_scores)\n",
    "        avg_finetuned = np.mean(finetuned_scores)\n",
    "        avg_improvement = avg_finetuned - avg_pretrained\n",
    "        avg_improvement_pct = (avg_improvement / avg_pretrained * 100) if avg_pretrained > 0 else 0\n",
    "        \n",
    "        print(f\"\\n📈 Average CLIP Scores (5 samples):\")\n",
    "        print(f\"  Pretrained: {avg_pretrained:.2f}/100\")\n",
    "        print(f\"  Fine-tuned: {avg_finetuned:.2f}/100\")\n",
    "        print(f\"  Average Improvement: {avg_improvement:+.2f} ({avg_improvement_pct:+.1f}%)\")\n",
    "        \n",
    "        if avg_improvement > 0:\n",
    "            print(f\"\\n✅ Fine-tuned model shows BETTER text-image alignment!\")\n",
    "        elif avg_improvement < 0:\n",
    "            print(f\"\\n⚠️  Fine-tuned model shows LOWER text-image alignment\")\n",
    "        else:\n",
    "            print(f\"\\n➖ No significant difference in alignment\")\n",
    "        \n",
    "        # Visualize comparison\n",
    "        fig, axes = plt.subplots(min(3, len(val_pairs)), 3, figsize=(15, 5*min(3, len(val_pairs))))\n",
    "        if min(3, len(val_pairs)) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx in range(min(3, len(val_pairs))):\n",
    "            pair = val_pairs[idx]\n",
    "            prompt = pair['edit_prompt']\n",
    "            input_img = PILImage.open(pair['input_image']).convert('RGB')\n",
    "            \n",
    "            # Generate images\n",
    "            with torch.no_grad():\n",
    "                pipe.unet.eval()\n",
    "                gen_pretrained = pipe(prompt=prompt, image=input_img, guidance_scale=7.5, \n",
    "                                      num_inference_steps=20, height=512, width=512).images[0]\n",
    "                \n",
    "                pipe_finetuned.unet.eval()\n",
    "                gen_finetuned = pipe_finetuned(prompt=prompt, image=input_img, guidance_scale=7.5, \n",
    "                                               num_inference_steps=20, height=512, width=512).images[0]\n",
    "            \n",
    "            # Plot\n",
    "            axes[idx, 0].imshow(gen_pretrained)\n",
    "            axes[idx, 0].set_title(f\"Pretrained\\n({pretrained_scores[idx]:.1f}/100)\", fontsize=10, color='blue')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            axes[idx, 1].imshow(gen_finetuned)\n",
    "            axes[idx, 1].set_title(f\"Fine-tuned\\n({finetuned_scores[idx]:.1f}/100)\", fontsize=10, color='green')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            axes[idx, 2].text(0.5, 0.5, f'Prompt:\\n{prompt[:40]}...', \n",
    "                            ha='center', va='center', fontsize=9, wrap=True)\n",
    "            axes[idx, 2].axis('off')\n",
    "        \n",
    "        plt.suptitle('CLIP Alignment Comparison: Pretrained vs Fine-tuned', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.results_dir, 'clip_alignment_comparison.png'), dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"\\n✓ Comparison visualization saved to: {os.path.join(config.results_dir, 'clip_alignment_comparison.png')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  CLIP evaluation skipped: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INSTRUCTPIX2PIX FINE-TUNING - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: InstructPix2Pix (Pretrained)\")\n",
    "print(f\"  Base model: {config.model_id}\")\n",
    "print(f\"  Architecture: Stable Diffusion + instruction tuning\")\n",
    "print(f\"  Input: (Previous image, Text instruction)\")\n",
    "print(f\"  Output: Next image in sequence\")\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training pairs: {len(train_pairs)}\")\n",
    "print(f\"  Validation pairs: {len(val_pairs)}\")\n",
    "print(f\"  Stories (train): {df_train['story_id'].nunique()}\")\n",
    "print(f\"  Stories (val): {df_val['story_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Batch size: {config.batch_size} (effective: {config.batch_size * config.gradient_accumulation_steps})\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Mixed precision: {config.use_mixed_precision}\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "if train_losses:\n",
    "    print(f\"  Initial loss: {train_losses[0]:.4f}\")\n",
    "    print(f\"  Final loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Loss reduction: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nCheckpoints: {config.checkpoints_dir}\")\n",
    "print(f\"Results: {config.results_dir}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"KEY ADVANTAGES:\")\n",
    "print(\"✓ Pretrained on large-scale image-text datasets\")\n",
    "print(\"✓ Optimized for image editing with text instructions\")\n",
    "print(\"✓ Easy to fine-tune on custom dataset\")\n",
    "print(\"✓ High-quality image generation (512×512)\")\n",
    "print(\"✓ Contextually aware (uses previous image as reference)\")\n",
    "print(\"✓ Autoregressive inference for full story generation\")\n",
    "print(\"✓ Fast inference (~5-10 seconds per image)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eoi_project",
   "language": "python",
   "name": "eoi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
