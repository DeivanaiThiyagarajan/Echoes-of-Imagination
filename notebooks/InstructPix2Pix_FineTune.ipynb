{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc9a080",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78acee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and Diffusers\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import (\n",
    "    StableDiffusionInstructPix2PixPipeline,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    DDPMScheduler,\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel\n",
    ")\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from torchvision import transforms\n",
    "\n",
    "# Check GPU\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0' if USE_GPU else 'cpu')\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPU Available: {USE_GPU}\")\n",
    "if USE_GPU:\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392f4bf",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model selection\n",
    "    model_id: str = \"timbrooks/instruct-pix2pix\"  # Pretrained InstructPix2Pix\n",
    "    \n",
    "    # Image configuration\n",
    "    image_size: int = 512\n",
    "    \n",
    "    # Training configuration\n",
    "    batch_size: int = 2  # Small batch for 8GB+ GPU\n",
    "    gradient_accumulation_steps: int = 2  # Effective batch = 4\n",
    "    num_epochs: int = 20\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 1e-2\n",
    "    warmup_steps: int = 500\n",
    "    max_grad_norm: float = 1.0\n",
    "    use_mixed_precision: bool = True\n",
    "    \n",
    "    # Fine-tuning strategy\n",
    "    fine_tune_unet: bool = True  # Fine-tune UNet\n",
    "    fine_tune_text_encoder: bool = False  # Keep text encoder frozen\n",
    "    fine_tune_vae: bool = False  # Keep VAE frozen\n",
    "    \n",
    "    # Data\n",
    "    num_workers: int = 4\n",
    "    \n",
    "    # Paths\n",
    "    checkpoints_dir: str = \"./models/instructpix2pix_checkpoints\"\n",
    "    results_dir: str = \"./results/instructpix2pix\"\n",
    "    log_dir: str = \"./logs/instructpix2pix\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_id}\")\n",
    "print(f\"  Image size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"  Batch size: {config.batch_size} (effective: {config.batch_size * config.gradient_accumulation_steps})\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Fine-tune UNet: {config.fine_tune_unet}\")\n",
    "print(f\"  Fine-tune Text Encoder: {config.fine_tune_text_encoder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3157d",
   "metadata": {},
   "source": [
    "## 3. Load SSID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SSID annotations\n",
    "def load_annotations(json_path, split_name):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    flat_data = [storylet for story in data['annotations'] for storylet in story]\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    df['split'] = split_name\n",
    "    return df\n",
    "\n",
    "# Paths\n",
    "annotations_dir = '../data/SSID_Annotations/SSID_Annotations'\n",
    "images_dir = '../data/SSID_Images/SSID_Images'\n",
    "\n",
    "train_json = os.path.join(annotations_dir, \"SSID_Train.json\")\n",
    "val_json = os.path.join(annotations_dir, \"SSID_Validation.json\")\n",
    "\n",
    "# Load splits\n",
    "df_train = load_annotations(train_json, 'train')\n",
    "df_val = load_annotations(val_json, 'val')\n",
    "\n",
    "print(f\"Train storylets: {len(df_train)}\")\n",
    "print(f\"Validation storylets: {len(df_val)}\")\n",
    "print(f\"Unique stories (train): {df_train['story_id'].nunique()}\")\n",
    "print(f\"Unique stories (val): {df_val['story_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba1b8a",
   "metadata": {},
   "source": [
    "## 4. Create Training Pairs (Previous Image + Text → Next Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e998f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(df, images_dir, split='train'):\n",
    "    \"\"\"\n",
    "    Create (input_image, edit_prompt, output_image) triplets.\n",
    "    input_image: previous frame in story\n",
    "    edit_prompt: text description of what changes to next frame\n",
    "    output_image: actual next frame\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for story_id in df['story_id'].unique():\n",
    "        story_data = df[df['story_id'] == story_id].sort_values('image_order').reset_index(drop=True)\n",
    "        \n",
    "        # Need at least 2 images per story\n",
    "        if len(story_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Create pairs: (image_t, text_t+1, image_t+1)\n",
    "        for i in range(len(story_data) - 1):\n",
    "            prev_row = story_data.iloc[i]\n",
    "            next_row = story_data.iloc[i + 1]\n",
    "            \n",
    "            prev_img_path = os.path.join(images_dir, f\"{prev_row['youtube_image_id']}.jpg\")\n",
    "            next_img_path = os.path.join(images_dir, f\"{next_row['youtube_image_id']}.jpg\")\n",
    "            \n",
    "            # Verify both images exist\n",
    "            if os.path.exists(prev_img_path) and os.path.exists(next_img_path):\n",
    "                pairs.append({\n",
    "                    'input_image': prev_img_path,  # Previous image\n",
    "                    'edit_prompt': next_row['storytext'],  # Text describing next scene\n",
    "                    'output_image': next_img_path,  # Target next image\n",
    "                    'story_id': story_id,\n",
    "                    'split': split\n",
    "                })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create pairs\n",
    "train_pairs = create_training_pairs(df_train, images_dir, 'train')\n",
    "val_pairs = create_training_pairs(df_val, images_dir, 'val')\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)}\")\n",
    "print(f\"Validation pairs: {len(val_pairs)}\")\n",
    "\n",
    "if train_pairs:\n",
    "    print(f\"\\nExample pair:\")\n",
    "    pair = train_pairs[0]\n",
    "    print(f\"  Input image: {os.path.basename(pair['input_image'])}\")\n",
    "    print(f\"  Edit prompt: {pair['edit_prompt'][:60]}...\")\n",
    "    print(f\"  Output image: {os.path.basename(pair['output_image'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30bfd7",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructPix2PixDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for InstructPix2Pix fine-tuning.\n",
    "    Input: (input_image, edit_prompt)\n",
    "    Target: output_image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pairs, tokenizer, image_size=512):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.CenterCrop((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Load and transform images\n",
    "        try:\n",
    "            input_img = Image.open(pair['input_image']).convert('RGB')\n",
    "            output_img = Image.open(pair['output_image']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images: {e}\")\n",
    "            # Return blank images as fallback\n",
    "            input_img = Image.new('RGB', (self.image_size, self.image_size), color='gray')\n",
    "            output_img = Image.new('RGB', (self.image_size, self.image_size), color='gray')\n",
    "        \n",
    "        input_tensor = self.image_transform(input_img)\n",
    "        output_tensor = self.image_transform(output_img)\n",
    "        \n",
    "        # Tokenize edit prompt\n",
    "        prompt = pair['edit_prompt']\n",
    "        tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_image': input_tensor,\n",
    "            'prompt_input_ids': tokens['input_ids'].squeeze(),\n",
    "            'prompt_attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'output_image': output_tensor,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InstructPix2PixDataset(train_pairs, tokenizer, config.image_size)\n",
    "val_dataset = InstructPix2PixDataset(val_pairs, tokenizer, config.image_size)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "\n",
    "print(f\"Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Validation loader batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4c25e",
   "metadata": {},
   "source": [
    "## 6. Load Pretrained InstructPix2Pix Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading pretrained InstructPix2Pix model...\")\n",
    "print(f\"Model: {config.model_id}\\n\")\n",
    "\n",
    "try:\n",
    "    # Load the full pipeline first\n",
    "    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32\n",
    "    )\n",
    "    \n",
    "    # Extract individual components\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    vae = pipe.vae\n",
    "    unet = pipe.unet\n",
    "    scheduler = pipe.scheduler\n",
    "    \n",
    "    # Move to device\n",
    "    text_encoder = text_encoder.to(DEVICE)\n",
    "    vae = vae.to(DEVICE)\n",
    "    unet = unet.to(DEVICE)\n",
    "    \n",
    "    print(\"✓ Model components loaded successfully!\")\n",
    "    print(f\"\\nModel Architecture:\")\n",
    "    print(f\"  Text Encoder: {sum(p.numel() for p in text_encoder.parameters()) / 1e6:.1f}M parameters\")\n",
    "    print(f\"  VAE: {sum(p.numel() for p in vae.parameters()) / 1e6:.1f}M parameters\")\n",
    "    print(f\"  UNet: {sum(p.numel() for p in unet.parameters()) / 1e6:.1f}M parameters\")\n",
    "    print(f\"  Total: {(sum(p.numel() for p in text_encoder.parameters()) + sum(p.numel() for p in vae.parameters()) + sum(p.numel() for p in unet.parameters())) / 1e6:.1f}M parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model: {e}\")\n",
    "    print(f\"Make sure you have internet connection to download the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb9b0c",
   "metadata": {},
   "source": [
    "## 7. Setup for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze components we're not fine-tuning\n",
    "if not config.fine_tune_text_encoder:\n",
    "    text_encoder.requires_grad_(False)\n",
    "    print(\"Text encoder frozen\")\n",
    "\n",
    "if not config.fine_tune_vae:\n",
    "    vae.requires_grad_(False)\n",
    "    print(\"VAE frozen\")\n",
    "\n",
    "# Only fine-tune UNet\n",
    "if config.fine_tune_unet:\n",
    "    unet.requires_grad_(True)\n",
    "    print(\"UNet unfrozen for fine-tuning\")\n",
    "else:\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "print(f\"\\nTrainable parameters: {trainable_params / 1e6:.1f}M\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = AdamW(\n",
    "    unet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_update_steps_per_epoch = len(train_loader) // config.gradient_accumulation_steps\n",
    "max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=max_train_steps\n",
    ")\n",
    "\n",
    "# Gradient scaler for mixed precision\n",
    "scaler = GradScaler() if config.use_mixed_precision and USE_GPU else None\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Total training steps: {max_train_steps}\")\n",
    "print(f\"Mixed precision: {config.use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4165f",
   "metadata": {},
   "source": [
    "## 8. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_embedding(tokenizer, text_encoder, prompt, device):\n",
    "    \"\"\"Encode text prompt to embedding\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        prompt,\n",
    "        padding='max_length',\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embedding = text_encoder(\n",
    "            input_ids=tokens['input_ids'].to(device),\n",
    "            attention_mask=tokens['attention_mask'].to(device)\n",
    "        )[0]\n",
    "    \n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def train_epoch(epoch, unet, vae, text_encoder, tokenizer, train_loader, optimizer, lr_scheduler, criterion, device, scaler):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    unet.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", ncols=80)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Load batch to device\n",
    "            input_images = batch['input_image'].to(device)\n",
    "            output_images = batch['output_image'].to(device)\n",
    "            prompts = batch['prompt']\n",
    "            \n",
    "            # Encode images to latent space\n",
    "            with torch.no_grad():\n",
    "                input_latents = vae.encode(input_images).latent_dist.sample() * 0.18215\n",
    "                output_latents = vae.encode(output_images).latent_dist.sample() * 0.18215\n",
    "            \n",
    "            # Encode prompts\n",
    "            prompt_embeds_list = []\n",
    "            for prompt in prompts:\n",
    "                prompt_embed = encode_text_embedding(tokenizer, text_encoder, prompt, device)\n",
    "                prompt_embeds_list.append(prompt_embed)\n",
    "            prompt_embeds = torch.cat(prompt_embeds_list, dim=0)\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                scheduler.config.num_train_timesteps,\n",
    "                (input_latents.shape[0],),\n",
    "                device=device\n",
    "            ).long()\n",
    "            \n",
    "            # Sample noise\n",
    "            noise = torch.randn_like(output_latents)\n",
    "            \n",
    "            # Add noise to output latents (forward process)\n",
    "            noisy_latents = scheduler.add_noise(output_latents, noise, timesteps)\n",
    "            \n",
    "            # Concatenate input and noisy latents\n",
    "            latent_model_input = torch.cat([input_latents, noisy_latents], dim=1)\n",
    "            \n",
    "            with autocast(enabled=config.use_mixed_precision and USE_GPU):\n",
    "                # Predict noise\n",
    "                noise_pred = unet(\n",
    "                    latent_model_input,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=prompt_embeds\n",
    "                ).sample\n",
    "                \n",
    "                # MSE loss\n",
    "                loss = F.mse_loss(noise_pred, noise, reduction='mean')\n",
    "            \n",
    "            # Backward pass with gradient accumulation\n",
    "            if config.use_mixed_precision and USE_GPU:\n",
    "                scaler.scale(loss / config.gradient_accumulation_steps).backward()\n",
    "            else:\n",
    "                (loss / config.gradient_accumulation_steps).backward()\n",
    "            \n",
    "            # Gradient accumulation step\n",
    "            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
    "                if config.use_mixed_precision and USE_GPU:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(unet.parameters(), config.max_grad_norm)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(unet.parameters(), config.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item():.4f})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(unet, vae, text_encoder, tokenizer, val_loader, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    unet.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=\"Validating\", ncols=80)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                input_images = batch['input_image'].to(device)\n",
    "                output_images = batch['output_image'].to(device)\n",
    "                prompts = batch['prompt']\n",
    "                \n",
    "                # Encode images\n",
    "                input_latents = vae.encode(input_images).latent_dist.sample() * 0.18215\n",
    "                output_latents = vae.encode(output_images).latent_dist.sample() * 0.18215\n",
    "                \n",
    "                # Encode prompts\n",
    "                prompt_embeds_list = []\n",
    "                for prompt in prompts:\n",
    "                    prompt_embed = encode_text_embedding(tokenizer, text_encoder, prompt, device)\n",
    "                    prompt_embeds_list.append(prompt_embed)\n",
    "                prompt_embeds = torch.cat(prompt_embeds_list, dim=0)\n",
    "                \n",
    "                # Random timesteps\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    scheduler.config.num_train_timesteps,\n",
    "                    (input_latents.shape[0],),\n",
    "                    device=device\n",
    "                ).long()\n",
    "                \n",
    "                # Noise\n",
    "                noise = torch.randn_like(output_latents)\n",
    "                \n",
    "                noisy_latents = scheduler.add_noise(output_latents, noise, timesteps)\n",
    "                latent_model_input = torch.cat([input_latents, noisy_latents], dim=1)\n",
    "                \n",
    "                # Forward\n",
    "                noise_pred = unet(\n",
    "                    latent_model_input,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=prompt_embeds\n",
    "                ).sample\n",
    "                \n",
    "                loss = F.mse_loss(noise_pred, noise, reduction='mean')\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": loss.item():.4f})\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f7751",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(f\"Model: {config.model_id}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Device: {DEVICE}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        epoch,\n",
    "        unet,\n",
    "        vae,\n",
    "        text_encoder,\n",
    "        tokenizer,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        F.mse_loss,\n",
    "        DEVICE,\n",
    "        scaler\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(unet, vae, text_encoder, tokenizer, val_loader, DEVICE)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_path = os.path.join(config.checkpoints_dir, \"best_unet\")\n",
    "        unet.save_pretrained(checkpoint_path)\n",
    "        print(f\"✓ Best model saved (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(config.checkpoints_dir, f\"unet_epoch_{epoch + 1}\")\n",
    "        unet.save_pretrained(checkpoint_path)\n",
    "        print(f\"✓ Checkpoint saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0799e6a",
   "metadata": {},
   "source": [
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(train_losses, label='Train Loss', marker='o', linewidth=2, markersize=6)\n",
    "ax.plot(val_losses, label='Val Loss', marker='s', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('InstructPix2Pix Fine-tuning - Training History', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.results_dir, 'training_loss.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss plot saved to: {os.path.join(config.results_dir, 'training_loss.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94d25f",
   "metadata": {},
   "source": [
    "## 11. Load Fine-tuned Model and Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned UNet\n",
    "best_unet_path = os.path.join(config.checkpoints_dir, \"best_unet\")\n",
    "\n",
    "if os.path.exists(best_unet_path):\n",
    "    print(f\"Loading fine-tuned UNet from: {best_unet_path}\")\n",
    "    \n",
    "    # Create new pipeline with fine-tuned UNet\n",
    "    pipe_finetuned = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        unet=UNet2DConditionModel.from_pretrained(best_unet_path),\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"✓ Fine-tuned pipeline created successfully\")\n",
    "else:\n",
    "    print(f\"Best model not found at {best_unet_path}\")\n",
    "    print(\"Using original pretrained model\")\n",
    "    pipe_finetuned = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5015a",
   "metadata": {},
   "source": [
    "## 12. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afac92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing inference on validation samples...\\n\")\n",
    "\n",
    "for idx in range(min(3, len(val_pairs))):\n",
    "    pair = val_pairs[idx]\n",
    "    \n",
    "    print(f\"\\nSample {idx + 1}:\")\n",
    "    print(f\"  Edit prompt: {pair['edit_prompt'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load input image\n",
    "        input_img = Image.open(pair['input_image']).convert('RGB')\n",
    "        \n",
    "        # Generate next image\n",
    "        with torch.no_grad():\n",
    "            generated = pipe_finetuned(\n",
    "                prompt=pair['edit_prompt'],\n",
    "                image=input_img,\n",
    "                guidance_scale=7.5,\n",
    "                num_inference_steps=30\n",
    "            ).images[0]\n",
    "        \n",
    "        # Load ground truth\n",
    "        ground_truth = Image.open(pair['output_image']).convert('RGB')\n",
    "        \n",
    "        # Display\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(input_img)\n",
    "        axes[0].set_title('Input Image (Previous Frame)', fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(generated)\n",
    "        axes[1].set_title('Generated (Fine-tuned)', fontsize=10, color='green')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(ground_truth)\n",
    "        axes[2].set_title('Ground Truth (Next Frame)', fontsize=10, color='blue')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(config.results_dir, f'inference_sample_{idx + 1}.png'),\n",
    "            dpi=100,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        print(f\"  ✓ Sample saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "\n",
    "print(f\"\\nInference samples saved to: {config.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72126d78",
   "metadata": {},
   "source": [
    "## 13. Generate Full Story Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_with_instructpix2pix(story_id, df, pipe, images_dir):\n",
    "    \"\"\"\n",
    "    Generate a full story sequence using InstructPix2Pix.\n",
    "    \"\"\"\n",
    "    story_data = df[df['story_id'] == story_id].sort_values('image_order')\n",
    "    generated_imgs = []\n",
    "    \n",
    "    if len(story_data) < 2:\n",
    "        print(f\"Story {story_id} has less than 2 images\")\n",
    "        return None\n",
    "    \n",
    "    # First image (use real)\n",
    "    first_img_path = os.path.join(images_dir, f\"{story_data.iloc[0]['youtube_image_id']}.jpg\")\n",
    "    first_img = Image.open(first_img_path).convert('RGB')\n",
    "    generated_imgs.append(first_img)\n",
    "    current_img = first_img\n",
    "    \n",
    "    # Generate remaining frames\n",
    "    print(f\"\\nGenerating story sequence (story_id: {story_id})...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(1, len(story_data)):\n",
    "            prompt = story_data.iloc[i]['storytext']\n",
    "            print(f\"  Frame {i + 1}/{len(story_data)}: {prompt[:40]}...\")\n",
    "            \n",
    "            try:\n",
    "                generated = pipe(\n",
    "                    prompt=prompt,\n",
    "                    image=current_img,\n",
    "                    guidance_scale=7.5,\n",
    "                    num_inference_steps=30\n",
    "                ).images[0]\n",
    "                \n",
    "                generated_imgs.append(generated)\n",
    "                current_img = generated  # Use generated as input for next\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {e}\")\n",
    "                break\n",
    "    \n",
    "    return generated_imgs, story_data\n",
    "\n",
    "\n",
    "# Generate a story\n",
    "if len(val_pairs) > 0:\n",
    "    story_id = val_pairs[0]['story_id']\n",
    "    \n",
    "    result = generate_story_with_instructpix2pix(\n",
    "        story_id,\n",
    "        df_val,\n",
    "        pipe_finetuned,\n",
    "        images_dir\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        generated_imgs, story_data = result\n",
    "        \n",
    "        # Display sequence\n",
    "        num_imgs = len(generated_imgs)\n",
    "        fig, axes = plt.subplots(2, (num_imgs + 1) // 2, figsize=(4*(num_imgs), 8))\n",
    "        axes = axes.flatten() if num_imgs > 1 else [axes]\n",
    "        \n",
    "        for idx, img in enumerate(generated_imgs):\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].set_title(f\"Frame {idx + 1}\", fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(num_imgs, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(config.results_dir, 'full_story_sequence.png'),\n",
    "            dpi=100,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nGenerated {num_imgs} frames for story {story_id}\")\n",
    "        print(\"\\nStory narrative:\")\n",
    "        for idx, row in story_data.iterrows():\n",
    "            print(f\"{row['image_order']}. {row['storytext']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045e2b6",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INSTRUCTPIX2PIX FINE-TUNING - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: InstructPix2Pix (Pretrained)\")\n",
    "print(f\"  Base model: {config.model_id}\")\n",
    "print(f\"  Architecture: Stable Diffusion + instruction tuning\")\n",
    "print(f\"  Input: (Previous image, Text instruction)\")\n",
    "print(f\"  Output: Next image in sequence\")\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training pairs: {len(train_pairs)}\")\n",
    "print(f\"  Validation pairs: {len(val_pairs)}\")\n",
    "print(f\"  Stories (train): {df_train['story_id'].nunique()}\")\n",
    "print(f\"  Stories (val): {df_val['story_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Batch size: {config.batch_size} (effective: {config.batch_size * config.gradient_accumulation_steps})\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Mixed precision: {config.use_mixed_precision}\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "if train_losses:\n",
    "    print(f\"  Initial loss: {train_losses[0]:.4f}\")\n",
    "    print(f\"  Final loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Loss reduction: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nCheckpoints: {config.checkpoints_dir}\")\n",
    "print(f\"Results: {config.results_dir}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"KEY ADVANTAGES:\")\n",
    "print(\"✓ Pretrained on large-scale image-text datasets\")\n",
    "print(\"✓ Optimized for image editing with text instructions\")\n",
    "print(\"✓ Easy to fine-tune on custom dataset\")\n",
    "print(\"✓ High-quality image generation (512×512)\")\n",
    "print(\"✓ Contextually aware (uses previous image as reference)\")\n",
    "print(\"✓ Autoregressive inference for full story generation\")\n",
    "print(\"✓ Fast inference (~5-10 seconds per image)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
