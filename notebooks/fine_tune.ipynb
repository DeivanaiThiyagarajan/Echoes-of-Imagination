{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0f70ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aishu\\anaconda3\\envs\\eoi\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPTextModel  # <-- from transformers, not diffusers\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8703a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from typing import List\n",
    "import random, math, os, sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from timm.utils import ModelEmaV3\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc83fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2516026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_encoders import clip_model, bert_model\n",
    "from dataloaders_text import caption_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977a69df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Aishu\\anaconda3\\envs\\eoi\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aishu\\.cache\\huggingface\\hub\\models--stabilityai--stable-diffusion-xl-base-1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 19 files: 100%|██████████| 19/19 [09:45<00:00, 30.82s/it]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bea4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2f098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    \"learning_rate\": 1e-5,  # A smaller learning rate works best for fine-tuning\n",
    "    \"batch_size\": 8,        # Adjust based on your GPU's VRAM\n",
    "    \"gradient_accumulation_steps\": 4,  # Helps manage larger batch sizes with limited VRAM\n",
    "    \"num_train_epochs\": 5,  # Enough to converge without overfitting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6bbaeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Flickr CSV: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Aishu\\\\Downloads\\\\AML-2\\\\Echoes-of-Imagination\\\\data\\\\datas\\\\extracted_files\\\\flickr30k_images/results.csv'. Using a small dummy set for Flickr.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Aishu\\\\Downloads\\\\AML-2\\\\Echoes-of-Imagination\\\\data\\\\datas\\\\extracted_files\\\\annotations_trainval2014/annotations/captions_train2014.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m caption_dataset()\n\u001b[1;32m----> 2\u001b[0m train_dataloader,_ \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagination\\src\\dataloaders_text.py:162\u001b[0m, in \u001b[0;36mcaption_dataset.get_dataloader\u001b[1;34m(self, partition, batch_size, distributed)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, partition, batch_size, distributed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 162\u001b[0m     train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(partition \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    164\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m train_dataset\n",
      "File \u001b[1;32mc:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagination\\src\\dataloaders_text.py:145\u001b[0m, in \u001b[0;36mcaption_dataset.get_datasets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_datasets\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 145\u001b[0m     df_combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_full_caption_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     train_df, temp_df \u001b[38;5;241m=\u001b[39m train_test_split(df_combined, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    148\u001b[0m     val_df, test_df \u001b[38;5;241m=\u001b[39m train_test_split(temp_df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagination\\src\\dataloaders_text.py:137\u001b[0m, in \u001b[0;36mcaption_dataset.load_full_caption_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_full_caption_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    136\u001b[0m     df_flickr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_flickr_dataset()\n\u001b[1;32m--> 137\u001b[0m     df_coco_unified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_coco_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_coco_unified, df_flickr], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_combined\n",
      "File \u001b[1;32mc:\\Users\\Aishu\\Downloads\\AML-2\\Echoes-of-Imagination\\src\\dataloaders_text.py:77\u001b[0m, in \u001b[0;36mcaption_dataset.load_coco_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_coco_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# Load JSON\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoco_captions_2014_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     78\u001b[0m         captions_2014 \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco_captions_2017_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Aishu\\\\Downloads\\\\AML-2\\\\Echoes-of-Imagination\\\\data\\\\datas\\\\extracted_files\\\\annotations_trainval2014/annotations/captions_train2014.json'"
     ]
    }
   ],
   "source": [
    "dataloader = caption_dataset()\n",
    "train_dataloader,_ = dataloader.get_dataloader(partition=\"train\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42048d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
